{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ecce1ac2-f2f3-4dda-9c18-344393964b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import logging\n",
    "import os\n",
    "from pathlib import Path\n",
    "import ray\n",
    "from graphenv.examples.tsp.graph_utils import make_complete_planar_graph\n",
    "from graphenv.examples.tsp.tsp_model import TSPModel, TSPQModel\n",
    "from graphenv.examples.tsp.tsp_nfp_model import TSPGNNModel\n",
    "from graphenv.examples.tsp.tsp_nfp_state import TSPNFPState\n",
    "from graphenv.examples.tsp.tsp_state import TSPState\n",
    "from graphenv.graph_env import GraphEnv\n",
    "from networkx.algorithms.approximation.traveling_salesman import greedy_tsp\n",
    "from ray import tune\n",
    "from ray.rllib.algorithms.a3c import A3CConfig\n",
    "from ray.rllib.algorithms.dqn import DQNConfig\n",
    "from ray.rllib.algorithms.marwil import MARWILConfig\n",
    "from ray.rllib.algorithms.ppo import PPOConfig\n",
    "from ray.rllib.models import ModelCatalog\n",
    "from ray.rllib.utils.framework import try_import_tf\n",
    "from ray.tune.registry import register_env\n",
    "import networkx as nx\n",
    "from ray.rllib.algorithms.algorithm import Algorithm\n",
    "from ray.tune import ExperimentAnalysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf5c0c65-0fe9-4515-a05a-f769f828f36a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_StoreAction(option_strings=['--log-level'], dest='log_level', nargs=None, const=None, default='INFO', type=<class 'str'>, choices=None, required=False, help=None, metavar=None)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf1, tf, tfv = try_import_tf()\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\n",
    "    \"--run\",\n",
    "    type=str,\n",
    "    default=\"PPO\",\n",
    "    choices=[\"PPO\", \"DQN\", \"A3C\", \"MARWIL\"],\n",
    "    help=\"The RLlib-registered algorithm to use.\",\n",
    ")\n",
    "parser.add_argument(\"--N\", type=int, default=5, help=\"Number of nodes in TSP network\")\n",
    "parser.add_argument(\n",
    "    \"--use-gnn\", action=\"store_true\", help=\"use the nfp state and gnn model\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--max-num-neighbors\",\n",
    "    type=int,\n",
    "    default=5,\n",
    "    help=\"Number of nearest neighbors for the gnn model\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--seed\", type=int, default=0, help=\"Random seed used to generate networkx graph\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--num-workers\", type=int, default=1, help=\"Number of rllib workers\"\n",
    ")\n",
    "parser.add_argument(\"--num-gpus\", type=int, default=0, help=\"Number of GPUs\")\n",
    "parser.add_argument(\"--lr\", type=float, default=1e-4, help=\"learning rate\")\n",
    "parser.add_argument(\n",
    "    \"--entropy-coeff\", type=float, default=0.0, help=\"entropy coefficient\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--rollouts-per-worker\",\n",
    "    type=int,\n",
    "    default=1,\n",
    "    help=\"Number of rollouts for each worker to collect\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--stop-iters\", type=int, default=100, help=\"Number of iterations to train.\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--stop-timesteps\", type=int, default=100000, help=\"Number of timesteps to train.\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--stop-reward\", type=float, default=0.0, help=\"Reward at which we stop training.\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--local-mode\",\n",
    "    action=\"store_true\",\n",
    "    help=\"Init Ray in local mode for easier debugging.\",\n",
    ")\n",
    "parser.add_argument(\"--log-level\", type=str, default=\"INFO\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3a8c3210-76d3-4fd4-b291-d629a5fb818c",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = parser.parse_args(args=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "296a6bf2-92f8-4061-8447-12f29cb46b08",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running with following CLI options: Namespace(run='PPO', N=5, use_gnn=False, max_num_neighbors=5, seed=0, num_workers=1, num_gpus=0, lr=0.0001, entropy_coeff=0.0, rollouts_per_worker=1, stop_iters=100, stop_timesteps=100000, stop_reward=0.0, local_mode=False, log_level='INFO')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-01 18:13:09,601\tINFO worker.py:1553 -- Started a local Ray instance.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "    <div style=\"margin-left: 50px;display: flex;flex-direction: row;align-items: center\">\n",
       "        <h3 style=\"color: var(--jp-ui-font-color0)\">Ray</h3>\n",
       "        <svg version=\"1.1\" id=\"ray\" width=\"3em\" viewBox=\"0 0 144.5 144.6\" style=\"margin-left: 3em;margin-right: 3em\">\n",
       "            <g id=\"layer-1\">\n",
       "                <path fill=\"#00a2e9\" class=\"st0\" d=\"M97.3,77.2c-3.8-1.1-6.2,0.9-8.3,5.1c-3.5,6.8-9.9,9.9-17.4,9.6S58,88.1,54.8,81.2c-1.4-3-3-4-6.3-4.1\n",
       "                    c-5.6-0.1-9.9,0.1-13.1,6.4c-3.8,7.6-13.6,10.2-21.8,7.6C5.2,88.4-0.4,80.5,0,71.7c0.1-8.4,5.7-15.8,13.8-18.2\n",
       "                    c8.4-2.6,17.5,0.7,22.3,8c1.3,1.9,1.3,5.2,3.6,5.6c3.9,0.6,8,0.2,12,0.2c1.8,0,1.9-1.6,2.4-2.8c3.5-7.8,9.7-11.8,18-11.9\n",
       "                    c8.2-0.1,14.4,3.9,17.8,11.4c1.3,2.8,2.9,3.6,5.7,3.3c1-0.1,2,0.1,3,0c2.8-0.5,6.4,1.7,8.1-2.7s-2.3-5.5-4.1-7.5\n",
       "                    c-5.1-5.7-10.9-10.8-16.1-16.3C84,38,81.9,37.1,78,38.3C66.7,42,56.2,35.7,53,24.1C50.3,14,57.3,2.8,67.7,0.5\n",
       "                    C78.4-2,89,4.7,91.5,15.3c0.1,0.3,0.1,0.5,0.2,0.8c0.7,3.4,0.7,6.9-0.8,9.8c-1.7,3.2-0.8,5,1.5,7.2c6.7,6.5,13.3,13,19.8,19.7\n",
       "                    c1.8,1.8,3,2.1,5.5,1.2c9.1-3.4,17.9-0.6,23.4,7c4.8,6.9,4.6,16.1-0.4,22.9c-5.4,7.2-14.2,9.9-23.1,6.5c-2.3-0.9-3.5-0.6-5.1,1.1\n",
       "                    c-6.7,6.9-13.6,13.7-20.5,20.4c-1.8,1.8-2.5,3.2-1.4,5.9c3.5,8.7,0.3,18.6-7.7,23.6c-7.9,5-18.2,3.8-24.8-2.9\n",
       "                    c-6.4-6.4-7.4-16.2-2.5-24.3c4.9-7.8,14.5-11,23.1-7.8c3,1.1,4.7,0.5,6.9-1.7C91.7,98.4,98,92.3,104.2,86c1.6-1.6,4.1-2.7,2.6-6.2\n",
       "                    c-1.4-3.3-3.8-2.5-6.2-2.6C99.8,77.2,98.9,77.2,97.3,77.2z M72.1,29.7c5.5,0.1,9.9-4.3,10-9.8c0-0.1,0-0.2,0-0.3\n",
       "                    C81.8,14,77,9.8,71.5,10.2c-5,0.3-9,4.2-9.3,9.2c-0.2,5.5,4,10.1,9.5,10.3C71.8,29.7,72,29.7,72.1,29.7z M72.3,62.3\n",
       "                    c-5.4-0.1-9.9,4.2-10.1,9.7c0,0.2,0,0.3,0,0.5c0.2,5.4,4.5,9.7,9.9,10c5.1,0.1,9.9-4.7,10.1-9.8c0.2-5.5-4-10-9.5-10.3\n",
       "                    C72.6,62.3,72.4,62.3,72.3,62.3z M115,72.5c0.1,5.4,4.5,9.7,9.8,9.9c5.6-0.2,10-4.8,10-10.4c-0.2-5.4-4.6-9.7-10-9.7\n",
       "                    c-5.3-0.1-9.8,4.2-9.9,9.5C115,72.1,115,72.3,115,72.5z M19.5,62.3c-5.4,0.1-9.8,4.4-10,9.8c-0.1,5.1,5.2,10.4,10.2,10.3\n",
       "                    c5.6-0.2,10-4.9,9.8-10.5c-0.1-5.4-4.5-9.7-9.9-9.6C19.6,62.3,19.5,62.3,19.5,62.3z M71.8,134.6c5.9,0.2,10.3-3.9,10.4-9.6\n",
       "                    c0.5-5.5-3.6-10.4-9.1-10.8c-5.5-0.5-10.4,3.6-10.8,9.1c0,0.5,0,0.9,0,1.4c-0.2,5.3,4,9.8,9.3,10\n",
       "                    C71.6,134.6,71.7,134.6,71.8,134.6z\"/>\n",
       "            </g>\n",
       "        </svg>\n",
       "        <table>\n",
       "            <tr>\n",
       "                <td style=\"text-align: left\"><b>Python version:</b></td>\n",
       "                <td style=\"text-align: left\"><b>3.9.21</b></td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                <td style=\"text-align: left\"><b>Ray version:</b></td>\n",
       "                <td style=\"text-align: left\"><b> 2.3.1</b></td>\n",
       "            </tr>\n",
       "            \n",
       "        </table>\n",
       "    </div>\n",
       "</div>\n"
      ],
      "text/plain": [
       "RayContext(dashboard_url='', python_version='3.9.21', ray_version='2.3.1', ray_commit='5f14cee8dfc6d61ec4fd3bc2c440f9944e92b33a', address_info={'node_ip_address': '192.168.0.126', 'raylet_ip_address': '192.168.0.126', 'redis_address': None, 'object_store_address': '/tmp/ray/session_2025-05-01_18-13-07_901369_17357/sockets/plasma_store', 'raylet_socket_name': '/tmp/ray/session_2025-05-01_18-13-07_901369_17357/sockets/raylet', 'webui_url': '', 'session_dir': '/tmp/ray/session_2025-05-01_18-13-07_901369_17357', 'metrics_export_port': 59247, 'gcs_address': '192.168.0.126:59605', 'address': '192.168.0.126:59605', 'dashboard_agent_listen_port': 52365, 'node_id': 'd112e23a9e24ac7ac278b65aa52985938df5d06eaa4d41018bf7b436'})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=19144)\u001b[0m WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "\u001b[2m\u001b[36m(pid=19144)\u001b[0m I0000 00:00:1746123285.100181   19144 fork_posix.cc:75] Other threads are currently calling into gRPC, skipping fork() handlers\n",
      "\u001b[2m\u001b[36m(PPO pid=19144)\u001b[0m 2025-05-01 18:14:46,748\tWARNING algorithm_config.py:596 -- Cannot create PPOConfig from given `config_dict`! Property __stdout_file__ not supported.\n",
      "\u001b[2m\u001b[36m(PPO pid=19144)\u001b[0m 2025-05-01 18:14:46,748\tINFO algorithm_config.py:2888 -- Executing eagerly (framework='tf2'), with eager_tracing=tf2. For production workloads, make sure to set eager_tracing=True  in order to match the speed of tf-static-graph (framework='tf'). For debugging purposes, `eager_tracing=False` is the best choice.\n",
      "\u001b[2m\u001b[36m(pid=19277)\u001b[0m WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "\u001b[2m\u001b[36m(pid=19277)\u001b[0m I0000 00:00:1746123288.468627   19277 fork_posix.cc:75] Other threads are currently calling into gRPC, skipping fork() handlers\n",
      "\u001b[2m\u001b[36m(pid=19279)\u001b[0m WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "\u001b[2m\u001b[36m(pid=19279)\u001b[0m I0000 00:00:1746123288.532544   19279 fork_posix.cc:75] Other threads are currently calling into gRPC, skipping fork() handlers\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19277)\u001b[0m 2025-05-01 18:14:50,140\tWARNING env.py:156 -- Your env doesn't have a .spec.max_episode_steps attribute. Your horizon will default to infinity, and your environment will not be reset.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19277)\u001b[0m 2025-05-01 18:14:51,201\tINFO eager_tf_policy_v2.py:75 -- Creating TF-eager policy running on CPU.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19277)\u001b[0m 2025-05-01 18:14:51,217\tINFO policy.py:1214 -- Policy (worker=1) running on CPU.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19277)\u001b[0m 2025-05-01 18:14:51,226\tINFO catalog.py:477 -- Wrapping <class 'graphenv.examples.tsp.tsp_model.TSPModel'> as None\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19280)\u001b[0m 2025-05-01 18:14:51,280\tINFO eager_tf_policy_v2.py:75 -- Creating TF-eager policy running on CPU.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19280)\u001b[0m 2025-05-01 18:14:51,295\tINFO policy.py:1214 -- Policy (worker=4) running on CPU.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19280)\u001b[0m 2025-05-01 18:14:51,304\tINFO catalog.py:477 -- Wrapping <class 'graphenv.examples.tsp.tsp_model.TSPModel'> as None\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19279)\u001b[0m 2025-05-01 18:14:51,280\tINFO eager_tf_policy_v2.py:75 -- Creating TF-eager policy running on CPU.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19279)\u001b[0m 2025-05-01 18:14:51,295\tINFO policy.py:1214 -- Policy (worker=3) running on CPU.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19279)\u001b[0m 2025-05-01 18:14:51,304\tINFO catalog.py:477 -- Wrapping <class 'graphenv.examples.tsp.tsp_model.TSPModel'> as None\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19277)\u001b[0m 2025-05-01 18:14:51,409\tINFO util.py:122 -- Using connectors:\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19277)\u001b[0m 2025-05-01 18:14:51,409\tINFO util.py:123 --     AgentConnectorPipeline\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19277)\u001b[0m         ObsPreprocessorConnector\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19277)\u001b[0m         StateBufferConnector\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19277)\u001b[0m         ViewRequirementAgentConnector\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19277)\u001b[0m 2025-05-01 18:14:51,409\tINFO util.py:124 --     ActionConnectorPipeline\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19277)\u001b[0m         ConvertToNumpyConnector\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19277)\u001b[0m         NormalizeActionsConnector\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19277)\u001b[0m         ImmutableActionsConnector\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19278)\u001b[0m 2025-05-01 18:14:51,358\tINFO eager_tf_policy_v2.py:75 -- Creating TF-eager policy running on CPU.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19278)\u001b[0m 2025-05-01 18:14:51,373\tINFO policy.py:1214 -- Policy (worker=2) running on CPU.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19278)\u001b[0m 2025-05-01 18:14:51,383\tINFO catalog.py:477 -- Wrapping <class 'graphenv.examples.tsp.tsp_model.TSPModel'> as None\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19280)\u001b[0m 2025-05-01 18:14:51,498\tINFO util.py:122 -- Using connectors:\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19280)\u001b[0m 2025-05-01 18:14:51,498\tINFO util.py:123 --     AgentConnectorPipeline\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19280)\u001b[0m         ObsPreprocessorConnector\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19280)\u001b[0m         StateBufferConnector\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19280)\u001b[0m         ViewRequirementAgentConnector\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19280)\u001b[0m 2025-05-01 18:14:51,498\tINFO util.py:124 --     ActionConnectorPipeline\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19280)\u001b[0m         ConvertToNumpyConnector\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19280)\u001b[0m         NormalizeActionsConnector\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19280)\u001b[0m         ImmutableActionsConnector\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19279)\u001b[0m 2025-05-01 18:14:51,493\tINFO util.py:122 -- Using connectors:\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19279)\u001b[0m 2025-05-01 18:14:51,493\tINFO util.py:123 --     AgentConnectorPipeline\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19279)\u001b[0m         ObsPreprocessorConnector\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19279)\u001b[0m         StateBufferConnector\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19279)\u001b[0m         ViewRequirementAgentConnector\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19279)\u001b[0m 2025-05-01 18:14:51,493\tINFO util.py:124 --     ActionConnectorPipeline\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19279)\u001b[0m         ConvertToNumpyConnector\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19279)\u001b[0m         NormalizeActionsConnector\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19279)\u001b[0m         ImmutableActionsConnector\n",
      "\u001b[2m\u001b[36m(PPO pid=19144)\u001b[0m 2025-05-01 18:14:51,571\tINFO worker_set.py:310 -- Inferred observation/action spaces from remote worker (local worker has no env): {'default_policy': (Repeated(Dict('nbr_dist': Box(0.0, 1.4142135623730951, (1,), float64), 'node_idx': Box(0, 5, (1,), int64), 'node_obs': Box(0.0, 1.0, (2,), float64), 'parent_dist': Box(0.0, 1.4142135623730951, (1,), float64)), 6), Discrete(5)), '__env__': (Repeated(Dict('nbr_dist': Box(0.0, 1.4142135623730951, (1,), float64), 'node_idx': Box(0, 5, (1,), int64), 'node_obs': Box(0.0, 1.0, (2,), float64), 'parent_dist': Box(0.0, 1.4142135623730951, (1,), float64)), 6), Discrete(5))}\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19278)\u001b[0m 2025-05-01 18:14:51,565\tINFO util.py:122 -- Using connectors:\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19278)\u001b[0m 2025-05-01 18:14:51,566\tINFO util.py:123 --     AgentConnectorPipeline\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19278)\u001b[0m         ObsPreprocessorConnector\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19278)\u001b[0m         StateBufferConnector\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19278)\u001b[0m         ViewRequirementAgentConnector\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19278)\u001b[0m 2025-05-01 18:14:51,566\tINFO util.py:124 --     ActionConnectorPipeline\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19278)\u001b[0m         ConvertToNumpyConnector\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19278)\u001b[0m         NormalizeActionsConnector\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19278)\u001b[0m         ImmutableActionsConnector\n",
      "\u001b[2m\u001b[36m(PPO pid=19144)\u001b[0m 2025-05-01 18:14:52,773\tINFO eager_tf_policy_v2.py:75 -- Creating TF-eager policy running on CPU.\n",
      "\u001b[2m\u001b[36m(PPO pid=19144)\u001b[0m 2025-05-01 18:14:52,788\tINFO policy.py:1214 -- Policy (worker=local) running on CPU.\n",
      "\u001b[2m\u001b[36m(PPO pid=19144)\u001b[0m 2025-05-01 18:14:52,796\tINFO catalog.py:477 -- Wrapping <class 'graphenv.examples.tsp.tsp_model.TSPModel'> as None\n",
      "\u001b[2m\u001b[36m(PPO pid=19144)\u001b[0m 2025-05-01 18:14:52,981\tINFO util.py:122 -- Using connectors:\n",
      "\u001b[2m\u001b[36m(PPO pid=19144)\u001b[0m 2025-05-01 18:14:52,981\tINFO util.py:123 --     AgentConnectorPipeline\n",
      "\u001b[2m\u001b[36m(PPO pid=19144)\u001b[0m         ObsPreprocessorConnector\n",
      "\u001b[2m\u001b[36m(PPO pid=19144)\u001b[0m         StateBufferConnector\n",
      "\u001b[2m\u001b[36m(PPO pid=19144)\u001b[0m         ViewRequirementAgentConnector\n",
      "\u001b[2m\u001b[36m(PPO pid=19144)\u001b[0m 2025-05-01 18:14:52,981\tINFO util.py:124 --     ActionConnectorPipeline\n",
      "\u001b[2m\u001b[36m(PPO pid=19144)\u001b[0m         ConvertToNumpyConnector\n",
      "\u001b[2m\u001b[36m(PPO pid=19144)\u001b[0m         NormalizeActionsConnector\n",
      "\u001b[2m\u001b[36m(PPO pid=19144)\u001b[0m         ImmutableActionsConnector\n",
      "\u001b[2m\u001b[36m(PPO pid=19144)\u001b[0m 2025-05-01 18:14:52,981\tINFO rollout_worker.py:2043 -- Built policy map: <PolicyMap lru-caching-capacity=100 policy-IDs=['default_policy']>\n",
      "\u001b[2m\u001b[36m(PPO pid=19144)\u001b[0m 2025-05-01 18:14:52,981\tINFO rollout_worker.py:2044 -- Built preprocessor map: {'default_policy': None}\n",
      "\u001b[2m\u001b[36m(PPO pid=19144)\u001b[0m 2025-05-01 18:14:52,981\tINFO rollout_worker.py:760 -- Built filter map: defaultdict(<class 'ray.rllib.utils.filter.NoFilter'>, {})\n",
      "\u001b[2m\u001b[36m(PPO pid=19144)\u001b[0m 2025-05-01 18:14:52,986\tINFO algorithm_config.py:2888 -- Executing eagerly (framework='tf2'), with eager_tracing=tf2. For production workloads, make sure to set eager_tracing=True  in order to match the speed of tf-static-graph (framework='tf'). For debugging purposes, `eager_tracing=False` is the best choice.\n",
      "\u001b[2m\u001b[36m(PPO pid=19144)\u001b[0m 2025-05-01 18:14:52,990\tWARNING env.py:156 -- Your env doesn't have a .spec.max_episode_steps attribute. Your horizon will default to infinity, and your environment will not be reset.\n",
      "\u001b[2m\u001b[36m(PPO pid=19144)\u001b[0m 2025-05-01 18:14:52,995\tINFO eager_tf_policy_v2.py:75 -- Creating TF-eager policy running on CPU.\n",
      "\u001b[2m\u001b[36m(PPO pid=19144)\u001b[0m 2025-05-01 18:14:52,996\tINFO policy.py:1214 -- Policy (worker=local) running on CPU.\n",
      "\u001b[2m\u001b[36m(PPO pid=19144)\u001b[0m 2025-05-01 18:14:52,997\tINFO catalog.py:477 -- Wrapping <class 'graphenv.examples.tsp.tsp_model.TSPModel'> as None\n",
      "\u001b[2m\u001b[36m(PPO pid=19144)\u001b[0m 2025-05-01 18:14:53,072\tINFO util.py:122 -- Using connectors:\n",
      "\u001b[2m\u001b[36m(PPO pid=19144)\u001b[0m 2025-05-01 18:14:53,072\tINFO util.py:123 --     AgentConnectorPipeline\n",
      "\u001b[2m\u001b[36m(PPO pid=19144)\u001b[0m         ObsPreprocessorConnector\n",
      "\u001b[2m\u001b[36m(PPO pid=19144)\u001b[0m         StateBufferConnector\n",
      "\u001b[2m\u001b[36m(PPO pid=19144)\u001b[0m         ViewRequirementAgentConnector\n",
      "\u001b[2m\u001b[36m(PPO pid=19144)\u001b[0m 2025-05-01 18:14:53,072\tINFO util.py:124 --     ActionConnectorPipeline\n",
      "\u001b[2m\u001b[36m(PPO pid=19144)\u001b[0m         ConvertToNumpyConnector\n",
      "\u001b[2m\u001b[36m(PPO pid=19144)\u001b[0m         NormalizeActionsConnector\n",
      "\u001b[2m\u001b[36m(PPO pid=19144)\u001b[0m         ImmutableActionsConnector\n",
      "\u001b[2m\u001b[36m(PPO pid=19144)\u001b[0m 2025-05-01 18:14:53,072\tINFO rollout_worker.py:2043 -- Built policy map: <PolicyMap lru-caching-capacity=100 policy-IDs=['default_policy']>\n",
      "\u001b[2m\u001b[36m(PPO pid=19144)\u001b[0m 2025-05-01 18:14:53,072\tINFO rollout_worker.py:2044 -- Built preprocessor map: {'default_policy': None}\n",
      "\u001b[2m\u001b[36m(PPO pid=19144)\u001b[0m 2025-05-01 18:14:53,072\tINFO rollout_worker.py:760 -- Built filter map: defaultdict(<class 'ray.rllib.utils.filter.NoFilter'>, {})\n",
      "\u001b[2m\u001b[36m(PPO pid=19144)\u001b[0m 2025-05-01 18:14:53,073\tWARNING util.py:67 -- Install gputil for GPU system monitoring.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19277)\u001b[0m 2025-05-01 18:14:53,076\tINFO rollout_worker.py:908 -- Generating sample batch of size 5\n",
      "\u001b[2m\u001b[36m(PPO pid=19144)\u001b[0m 2025-05-01 18:14:53,342\tWARNING deprecation.py:50 -- DeprecationWarning: `_get_slice_indices` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=19144)\u001b[0m /home/vladimir/.pyenv/versions/3.9.21/lib/python3.9/site-packages/numpy/core/_methods.py:180: RuntimeWarning: overflow encountered in reduce\n",
      "\u001b[2m\u001b[36m(PPO pid=19144)\u001b[0m   ret = umr_sum(arr, axis, dtype, out, keepdims, where=where)\n",
      "\u001b[2m\u001b[36m(PPO pid=19144)\u001b[0m 2025-05-01 18:14:53,343\tINFO rollout_worker.py:1004 -- Training on concatenated sample batches:\n",
      "\u001b[2m\u001b[36m(PPO pid=19144)\u001b[0m \n",
      "\u001b[2m\u001b[36m(PPO pid=19144)\u001b[0m { 'count': 2,\n",
      "\u001b[2m\u001b[36m(PPO pid=19144)\u001b[0m   'policy_batches': { 'default_policy': { 'action_dist_inputs': np.ndarray((2, 5), dtype=float32, min=-3.4028234663852886e+38, max=1.168, mean=-inf),\n",
      "\u001b[2m\u001b[36m(PPO pid=19144)\u001b[0m                                           'action_logp': np.ndarray((2,), dtype=float32, min=-1.374, max=-1.127, mean=-1.251),\n",
      "\u001b[2m\u001b[36m(PPO pid=19144)\u001b[0m                                           'actions': np.ndarray((2,), dtype=int32, min=0.0, max=2.0, mean=1.0),\n",
      "\u001b[2m\u001b[36m(PPO pid=19144)\u001b[0m                                           'advantages': np.ndarray((2,), dtype=float32, min=-0.317, max=0.062, mean=-0.127),\n",
      "\u001b[2m\u001b[36m(PPO pid=19144)\u001b[0m                                           'agent_index': np.ndarray((2,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(PPO pid=19144)\u001b[0m                                           'eps_id': np.ndarray((2,), dtype=int64, min=2.9195359036481464e+16, max=2.9195359036481464e+16, mean=2.9195359036481464e+16),\n",
      "\u001b[2m\u001b[36m(PPO pid=19144)\u001b[0m                                           'new_obs': np.ndarray((2, 31), dtype=float32, min=0.0, max=4.0, mean=0.57),\n",
      "\u001b[2m\u001b[36m(PPO pid=19144)\u001b[0m                                           'obs': np.ndarray((2, 31), dtype=float32, min=0.0, max=5.0, mean=0.681),\n",
      "\u001b[2m\u001b[36m(PPO pid=19144)\u001b[0m                                           'rewards': np.ndarray((2,), dtype=float32, min=-0.586, max=-0.362, mean=-0.474),\n",
      "\u001b[2m\u001b[36m(PPO pid=19144)\u001b[0m                                           't': np.ndarray((2,), dtype=int64, min=0.0, max=1.0, mean=0.5),\n",
      "\u001b[2m\u001b[36m(PPO pid=19144)\u001b[0m                                           'terminateds': np.ndarray((2,), dtype=bool, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(PPO pid=19144)\u001b[0m                                           'truncateds': np.ndarray((2,), dtype=bool, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(PPO pid=19144)\u001b[0m                                           'unroll_id': np.ndarray((2,), dtype=int64, min=1.0, max=1.0, mean=1.0),\n",
      "\u001b[2m\u001b[36m(PPO pid=19144)\u001b[0m                                           'value_targets': np.ndarray((2,), dtype=float32, min=-1.856, max=-1.509, mean=-1.682),\n",
      "\u001b[2m\u001b[36m(PPO pid=19144)\u001b[0m                                           'vf_preds': np.ndarray((2,), dtype=float32, min=0.92, max=0.947, mean=0.933)}},\n",
      "\u001b[2m\u001b[36m(PPO pid=19144)\u001b[0m   'type': 'MultiAgentBatch'}\n",
      "\u001b[2m\u001b[36m(PPO pid=19144)\u001b[0m \n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19277)\u001b[0m /home/vladimir/.pyenv/versions/3.9.21/lib/python3.9/site-packages/numpy/core/_methods.py:180: RuntimeWarning: overflow encountered in reduce\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19277)\u001b[0m   ret = umr_sum(arr, axis, dtype, out, keepdims, where=where)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19277)\u001b[0m 2025-05-01 18:14:53,338\tINFO rollout_worker.py:949 -- Completed sample batch:\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19277)\u001b[0m \n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19277)\u001b[0m { 'count': 5,\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19277)\u001b[0m   'policy_batches': { 'default_policy': { 'action_dist_inputs': np.ndarray((5, 5), dtype=float32, min=-3.4028234663852886e+38, max=1.267, mean=-inf),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19277)\u001b[0m                                           'action_logp': np.ndarray((5,), dtype=float32, min=-1.411, max=-0.0, mean=-0.665),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19277)\u001b[0m                                           'actions': np.ndarray((5,), dtype=int32, min=0.0, max=3.0, mean=0.6),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19277)\u001b[0m                                           'advantages': np.ndarray((5,), dtype=float32, min=-3.449, max=-1.109, mean=-2.284),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19277)\u001b[0m                                           'agent_index': np.ndarray((5,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19277)\u001b[0m                                           'eps_id': np.ndarray((5,), dtype=int64, min=3.6164666963735104e+17, max=3.6164666963735104e+17, mean=3.6164666963735104e+17),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19277)\u001b[0m                                           'new_obs': np.ndarray((5, 31), dtype=float32, min=0.0, max=4.0, mean=0.384),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19277)\u001b[0m                                           'obs': np.ndarray((5, 31), dtype=float32, min=0.0, max=5.0, mean=0.521),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19277)\u001b[0m                                           'rewards': np.ndarray((5,), dtype=float32, min=-0.712, max=-0.208, mean=-0.511),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19277)\u001b[0m                                           't': np.ndarray((5,), dtype=int64, min=0.0, max=4.0, mean=2.0),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19277)\u001b[0m                                           'terminateds': np.ndarray((5,), dtype=bool, min=0.0, max=1.0, mean=0.2),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19277)\u001b[0m                                           'truncateds': np.ndarray((5,), dtype=bool, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19277)\u001b[0m                                           'unroll_id': np.ndarray((5,), dtype=int64, min=1.0, max=1.0, mean=1.0),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19277)\u001b[0m                                           'value_targets': np.ndarray((5,), dtype=float32, min=-2.514, max=-0.208, mean=-1.376),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19277)\u001b[0m                                           'vf_preds': np.ndarray((5,), dtype=float32, min=0.888, max=0.935, mean=0.908)}},\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19277)\u001b[0m   'type': 'MultiAgentBatch'}\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19277)\u001b[0m \n",
      "\u001b[2m\u001b[36m(PPO pid=19144)\u001b[0m 2025-05-01 18:14:53,497\tWARNING tf_utils.py:576 -- KL divergence is non-finite, this will likely destabilize your model and the training process. Action(s) in a specific state have near-zero probability. This can happen naturally in deterministic environments where the optimal policy has zero mass for a specific action. To fix this issue, consider setting the coefficient for the KL loss term to zero or increasing policy entropy.\n",
      "\u001b[2m\u001b[36m(PPO pid=19144)\u001b[0m 2025-05-01 18:14:53,668\tINFO eager_tf_policy_v2.py:930 -- Optimizing variable hidden_layer_1/kernel:0\n",
      "\u001b[2m\u001b[36m(PPO pid=19144)\u001b[0m 2025-05-01 18:14:53,668\tINFO eager_tf_policy_v2.py:930 -- Optimizing variable hidden_layer_1/bias:0\n",
      "\u001b[2m\u001b[36m(PPO pid=19144)\u001b[0m 2025-05-01 18:14:53,669\tINFO eager_tf_policy_v2.py:930 -- Optimizing variable embed_layer/embeddings:0\n",
      "\u001b[2m\u001b[36m(PPO pid=19144)\u001b[0m 2025-05-01 18:14:53,669\tINFO eager_tf_policy_v2.py:930 -- Optimizing variable hidden_layer_2/kernel:0\n",
      "\u001b[2m\u001b[36m(PPO pid=19144)\u001b[0m 2025-05-01 18:14:53,669\tINFO eager_tf_policy_v2.py:930 -- Optimizing variable hidden_layer_2/bias:0\n",
      "\u001b[2m\u001b[36m(PPO pid=19144)\u001b[0m 2025-05-01 18:14:53,669\tINFO eager_tf_policy_v2.py:930 -- Optimizing variable action_value_output/kernel:0\n",
      "\u001b[2m\u001b[36m(PPO pid=19144)\u001b[0m 2025-05-01 18:14:53,669\tINFO eager_tf_policy_v2.py:930 -- Optimizing variable action_value_output/bias:0\n",
      "\u001b[2m\u001b[36m(PPO pid=19144)\u001b[0m 2025-05-01 18:14:53,669\tINFO eager_tf_policy_v2.py:930 -- Optimizing variable action_weight_output/kernel:0\n",
      "\u001b[2m\u001b[36m(PPO pid=19144)\u001b[0m 2025-05-01 18:14:53,669\tINFO eager_tf_policy_v2.py:930 -- Optimizing variable action_weight_output/bias:0\n",
      "\u001b[2m\u001b[36m(PPO pid=19144)\u001b[0m 2025-05-01 18:14:54,014\tWARNING tf_utils.py:576 -- KL divergence is non-finite, this will likely destabilize your model and the training process. Action(s) in a specific state have near-zero probability. This can happen naturally in deterministic environments where the optimal policy has zero mass for a specific action. To fix this issue, consider setting the coefficient for the KL loss term to zero or increasing policy entropy.\n",
      "\u001b[2m\u001b[36m(PPO pid=19144)\u001b[0m 2025-05-01 18:14:54,525\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=19144)\u001b[0m 2025-05-01 18:14:54,525\tINFO rollout_worker.py:908 -- Generating sample batch of size 1\n",
      "\u001b[2m\u001b[36m(PPO pid=19144)\u001b[0m 2025-05-01 18:14:54,777\tINFO rollout_worker.py:949 -- Completed sample batch:\n",
      "\u001b[2m\u001b[36m(PPO pid=19144)\u001b[0m \n",
      "\u001b[2m\u001b[36m(PPO pid=19144)\u001b[0m { 'count': 5,\n",
      "\u001b[2m\u001b[36m(PPO pid=19144)\u001b[0m   'policy_batches': { 'default_policy': { 'advantages': np.ndarray((5,), dtype=float32, min=-2.802, max=0.023, mean=-1.305),\n",
      "\u001b[2m\u001b[36m(PPO pid=19144)\u001b[0m                                           'agent_index': np.ndarray((5,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(PPO pid=19144)\u001b[0m                                           'eps_id': np.ndarray((5,), dtype=int64, min=4.409881484365084e+17, max=4.409881484365084e+17, mean=4.409881484365084e+17),\n",
      "\u001b[2m\u001b[36m(PPO pid=19144)\u001b[0m                                           'new_obs': np.ndarray((5, 31), dtype=float32, min=0.0, max=4.0, mean=0.384),\n",
      "\u001b[2m\u001b[36m(PPO pid=19144)\u001b[0m                                           'obs': np.ndarray((5, 31), dtype=float32, min=0.0, max=5.0, mean=0.53),\n",
      "\u001b[2m\u001b[36m(PPO pid=19144)\u001b[0m                                           'rewards': np.ndarray((5,), dtype=float32, min=-0.979, max=-0.252, mean=-0.547),\n",
      "\u001b[2m\u001b[36m(PPO pid=19144)\u001b[0m                                           't': np.ndarray((5,), dtype=int64, min=0.0, max=4.0, mean=2.0),\n",
      "\u001b[2m\u001b[36m(PPO pid=19144)\u001b[0m                                           'terminateds': np.ndarray((5,), dtype=bool, min=0.0, max=1.0, mean=0.2),\n",
      "\u001b[2m\u001b[36m(PPO pid=19144)\u001b[0m                                           'truncateds': np.ndarray((5,), dtype=bool, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(PPO pid=19144)\u001b[0m                                           'unroll_id': np.ndarray((5,), dtype=int64, min=1.0, max=1.0, mean=1.0),\n",
      "\u001b[2m\u001b[36m(PPO pid=19144)\u001b[0m                                           'value_targets': np.ndarray((5,), dtype=float32, min=-2.681, max=-0.362, mean=-1.594),\n",
      "\u001b[2m\u001b[36m(PPO pid=19144)\u001b[0m                                           'vf_preds': np.ndarray((5,), dtype=float32, min=-0.517, max=0.12, mean=-0.289)}},\n",
      "\u001b[2m\u001b[36m(PPO pid=19144)\u001b[0m   'type': 'MultiAgentBatch'}\n",
      "\u001b[2m\u001b[36m(PPO pid=19144)\u001b[0m \n",
      "\u001b[2m\u001b[36m(PPO pid=19144)\u001b[0m 2025-05-01 18:14:55,634\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=19144)\u001b[0m 2025-05-01 18:14:56,480\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=19144)\u001b[0m 2025-05-01 18:14:57,326\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=19144)\u001b[0m 2025-05-01 18:14:58,168\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=19144)\u001b[0m 2025-05-01 18:14:59,010\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=19144)\u001b[0m 2025-05-01 18:14:59,862\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=19144)\u001b[0m 2025-05-01 18:15:00,713\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=19144)\u001b[0m 2025-05-01 18:15:01,564\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=19144)\u001b[0m 2025-05-01 18:15:02,413\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=19144)\u001b[0m 2025-05-01 18:15:03,268\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=19144)\u001b[0m 2025-05-01 18:15:04,102\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=19144)\u001b[0m 2025-05-01 18:15:04,931\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=19144)\u001b[0m 2025-05-01 18:15:05,802\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=19144)\u001b[0m 2025-05-01 18:15:06,656\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=19144)\u001b[0m 2025-05-01 18:15:07,519\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=19144)\u001b[0m 2025-05-01 18:15:08,383\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=19144)\u001b[0m 2025-05-01 18:15:09,240\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=19144)\u001b[0m 2025-05-01 18:15:10,103\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=19144)\u001b[0m 2025-05-01 18:15:10,952\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=19144)\u001b[0m 2025-05-01 18:15:11,787\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=19144)\u001b[0m 2025-05-01 18:15:12,620\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=19144)\u001b[0m 2025-05-01 18:15:13,450\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=19144)\u001b[0m 2025-05-01 18:15:14,280\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=19144)\u001b[0m 2025-05-01 18:15:15,125\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=19144)\u001b[0m 2025-05-01 18:15:15,973\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=19144)\u001b[0m 2025-05-01 18:15:16,810\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=19144)\u001b[0m 2025-05-01 18:15:17,645\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=19144)\u001b[0m 2025-05-01 18:15:18,483\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=19144)\u001b[0m 2025-05-01 18:15:19,319\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=19144)\u001b[0m 2025-05-01 18:15:20,152\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=19144)\u001b[0m 2025-05-01 18:15:20,993\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=19144)\u001b[0m 2025-05-01 18:15:21,825\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=19144)\u001b[0m 2025-05-01 18:15:22,665\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=19144)\u001b[0m 2025-05-01 18:15:23,498\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=19144)\u001b[0m 2025-05-01 18:15:24,338\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=19144)\u001b[0m 2025-05-01 18:15:25,175\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=19144)\u001b[0m 2025-05-01 18:15:26,024\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=19144)\u001b[0m 2025-05-01 18:15:26,866\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=19144)\u001b[0m 2025-05-01 18:15:27,699\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=19144)\u001b[0m 2025-05-01 18:15:28,542\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=19144)\u001b[0m 2025-05-01 18:15:29,384\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=19144)\u001b[0m 2025-05-01 18:15:30,226\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=19144)\u001b[0m 2025-05-01 18:15:31,078\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=19144)\u001b[0m 2025-05-01 18:15:31,921\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=19144)\u001b[0m 2025-05-01 18:15:32,763\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=19144)\u001b[0m 2025-05-01 18:15:33,612\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=19144)\u001b[0m 2025-05-01 18:15:34,477\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=19144)\u001b[0m 2025-05-01 18:15:35,339\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=19144)\u001b[0m 2025-05-01 18:15:36,208\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=19144)\u001b[0m 2025-05-01 18:15:37,072\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=19144)\u001b[0m 2025-05-01 18:15:37,935\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=19144)\u001b[0m 2025-05-01 18:15:38,798\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=19144)\u001b[0m 2025-05-01 18:15:39,652\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=19144)\u001b[0m 2025-05-01 18:15:40,508\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=19144)\u001b[0m 2025-05-01 18:15:41,366\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=19144)\u001b[0m 2025-05-01 18:15:42,206\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=19144)\u001b[0m 2025-05-01 18:15:43,062\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=19144)\u001b[0m 2025-05-01 18:15:43,911\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=19144)\u001b[0m 2025-05-01 18:15:44,749\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=19144)\u001b[0m 2025-05-01 18:15:45,597\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=19144)\u001b[0m 2025-05-01 18:15:46,435\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=19144)\u001b[0m 2025-05-01 18:15:47,272\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=19144)\u001b[0m 2025-05-01 18:15:48,107\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=19144)\u001b[0m 2025-05-01 18:15:48,947\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=19144)\u001b[0m 2025-05-01 18:15:49,787\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=19144)\u001b[0m 2025-05-01 18:15:50,629\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=19144)\u001b[0m 2025-05-01 18:15:51,470\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=19144)\u001b[0m 2025-05-01 18:15:52,309\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=19144)\u001b[0m 2025-05-01 18:15:53,152\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=19144)\u001b[0m 2025-05-01 18:15:53,987\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=19144)\u001b[0m 2025-05-01 18:15:54,833\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=19144)\u001b[0m 2025-05-01 18:15:55,677\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=19144)\u001b[0m 2025-05-01 18:15:56,522\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=19144)\u001b[0m 2025-05-01 18:15:57,360\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=19144)\u001b[0m 2025-05-01 18:15:58,195\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=19144)\u001b[0m 2025-05-01 18:15:59,044\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=19144)\u001b[0m 2025-05-01 18:15:59,907\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=19144)\u001b[0m 2025-05-01 18:16:00,769\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=19144)\u001b[0m 2025-05-01 18:16:01,637\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=19144)\u001b[0m 2025-05-01 18:16:02,497\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=19144)\u001b[0m 2025-05-01 18:16:03,354\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=19144)\u001b[0m 2025-05-01 18:16:04,220\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=19144)\u001b[0m 2025-05-01 18:16:05,071\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=19144)\u001b[0m 2025-05-01 18:16:05,915\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=19144)\u001b[0m 2025-05-01 18:16:06,762\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=19144)\u001b[0m 2025-05-01 18:16:07,598\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=19144)\u001b[0m 2025-05-01 18:16:08,432\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=19144)\u001b[0m 2025-05-01 18:16:09,272\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=19144)\u001b[0m 2025-05-01 18:16:10,109\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=19144)\u001b[0m 2025-05-01 18:16:10,947\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=19144)\u001b[0m 2025-05-01 18:16:11,794\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=19144)\u001b[0m 2025-05-01 18:16:12,629\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=19144)\u001b[0m 2025-05-01 18:16:13,467\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=19144)\u001b[0m 2025-05-01 18:16:14,307\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=19144)\u001b[0m 2025-05-01 18:16:15,151\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=19144)\u001b[0m 2025-05-01 18:16:15,992\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=19144)\u001b[0m 2025-05-01 18:16:16,834\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=19144)\u001b[0m 2025-05-01 18:16:17,669\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=19144)\u001b[0m 2025-05-01 18:16:18,508\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(pid=20334)\u001b[0m WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "\u001b[2m\u001b[36m(pid=20334)\u001b[0m I0000 00:00:1746123497.701153   20334 fork_posix.cc:75] Other threads are currently calling into gRPC, skipping fork() handlers\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=20334)\u001b[0m 2025-05-01 18:18:19,398\tINFO eager_tf_policy_v2.py:75 -- Creating TF-eager policy running on CPU.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=20334)\u001b[0m 2025-05-01 18:18:19,415\tINFO policy.py:1214 -- Policy (worker=2) running on CPU.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=20334)\u001b[0m 2025-05-01 18:18:19,424\tINFO catalog.py:477 -- Wrapping <class 'graphenv.examples.tsp.tsp_model.TSPModel'> as None\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=20334)\u001b[0m 2025-05-01 18:18:19,611\tINFO util.py:122 -- Using connectors:\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=20334)\u001b[0m 2025-05-01 18:18:19,611\tINFO util.py:123 --     AgentConnectorPipeline\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=20334)\u001b[0m         ObsPreprocessorConnector\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=20334)\u001b[0m         StateBufferConnector\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=20334)\u001b[0m         ViewRequirementAgentConnector\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=20334)\u001b[0m 2025-05-01 18:18:19,611\tINFO util.py:124 --     ActionConnectorPipeline\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=20334)\u001b[0m         ConvertToNumpyConnector\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=20334)\u001b[0m         NormalizeActionsConnector\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=20334)\u001b[0m         ImmutableActionsConnector\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=20336)\u001b[0m 2025-05-01 18:18:19,626\tINFO eager_tf_policy_v2.py:75 -- Creating TF-eager policy running on CPU.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=20333)\u001b[0m 2025-05-01 18:18:19,616\tWARNING env.py:156 -- Your env doesn't have a .spec.max_episode_steps attribute. Your horizon will default to infinity, and your environment will not be reset.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=20336)\u001b[0m 2025-05-01 18:18:19,640\tINFO policy.py:1214 -- Policy (worker=4) running on CPU.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=20336)\u001b[0m 2025-05-01 18:18:19,650\tINFO catalog.py:477 -- Wrapping <class 'graphenv.examples.tsp.tsp_model.TSPModel'> as None\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=20335)\u001b[0m 2025-05-01 18:18:19,639\tINFO eager_tf_policy_v2.py:75 -- Creating TF-eager policy running on CPU.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=20335)\u001b[0m 2025-05-01 18:18:19,653\tINFO policy.py:1214 -- Policy (worker=3) running on CPU.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=20335)\u001b[0m 2025-05-01 18:18:19,663\tINFO catalog.py:477 -- Wrapping <class 'graphenv.examples.tsp.tsp_model.TSPModel'> as None\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=20336)\u001b[0m 2025-05-01 18:18:19,832\tINFO util.py:122 -- Using connectors:\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=20336)\u001b[0m 2025-05-01 18:18:19,832\tINFO util.py:123 --     AgentConnectorPipeline\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=20336)\u001b[0m         ObsPreprocessorConnector\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=20336)\u001b[0m         StateBufferConnector\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=20336)\u001b[0m         ViewRequirementAgentConnector\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=20336)\u001b[0m 2025-05-01 18:18:19,833\tINFO util.py:124 --     ActionConnectorPipeline\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=20336)\u001b[0m         ConvertToNumpyConnector\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=20336)\u001b[0m         NormalizeActionsConnector\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=20336)\u001b[0m         ImmutableActionsConnector\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=20335)\u001b[0m 2025-05-01 18:18:19,850\tINFO util.py:122 -- Using connectors:\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=20335)\u001b[0m 2025-05-01 18:18:19,851\tINFO util.py:123 --     AgentConnectorPipeline\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=20335)\u001b[0m         ObsPreprocessorConnector\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=20335)\u001b[0m         StateBufferConnector\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=20335)\u001b[0m         ViewRequirementAgentConnector\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=20335)\u001b[0m 2025-05-01 18:18:19,851\tINFO util.py:124 --     ActionConnectorPipeline\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=20335)\u001b[0m         ConvertToNumpyConnector\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=20335)\u001b[0m         NormalizeActionsConnector\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=20335)\u001b[0m         ImmutableActionsConnector\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=20333)\u001b[0m 2025-05-01 18:18:19,912\tINFO eager_tf_policy_v2.py:75 -- Creating TF-eager policy running on CPU.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=20333)\u001b[0m 2025-05-01 18:18:19,926\tINFO policy.py:1214 -- Policy (worker=1) running on CPU.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=20333)\u001b[0m 2025-05-01 18:18:19,935\tINFO catalog.py:477 -- Wrapping <class 'graphenv.examples.tsp.tsp_model.TSPModel'> as None\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=20333)\u001b[0m 2025-05-01 18:18:20,116\tINFO util.py:122 -- Using connectors:\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=20333)\u001b[0m 2025-05-01 18:18:20,116\tINFO util.py:123 --     AgentConnectorPipeline\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=20333)\u001b[0m         ObsPreprocessorConnector\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=20333)\u001b[0m         StateBufferConnector\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=20333)\u001b[0m         ViewRequirementAgentConnector\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=20333)\u001b[0m 2025-05-01 18:18:20,116\tINFO util.py:124 --     ActionConnectorPipeline\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=20333)\u001b[0m         ConvertToNumpyConnector\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=20333)\u001b[0m         NormalizeActionsConnector\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=20333)\u001b[0m         ImmutableActionsConnector\n",
      "\u001b[2m\u001b[36m(pid=21126)\u001b[0m WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "\u001b[2m\u001b[36m(pid=21126)\u001b[0m I0000 00:00:1746123868.503263   21126 fork_posix.cc:75] Other threads are currently calling into gRPC, skipping fork() handlers\n",
      "\u001b[2m\u001b[36m(PPO pid=21126)\u001b[0m 2025-05-01 18:24:30,115\tWARNING algorithm_config.py:596 -- Cannot create PPOConfig from given `config_dict`! Property __stdout_file__ not supported.\n",
      "\u001b[2m\u001b[36m(PPO pid=21126)\u001b[0m 2025-05-01 18:24:30,116\tINFO algorithm_config.py:2888 -- Executing eagerly (framework='tf2'), with eager_tracing=tf2. For production workloads, make sure to set eager_tracing=True  in order to match the speed of tf-static-graph (framework='tf'). For debugging purposes, `eager_tracing=False` is the best choice.\n",
      "\u001b[2m\u001b[36m(pid=21251)\u001b[0m WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "\u001b[2m\u001b[36m(pid=21251)\u001b[0m I0000 00:00:1746123871.851033   21251 fork_posix.cc:75] Other threads are currently calling into gRPC, skipping fork() handlers\n",
      "\u001b[2m\u001b[36m(pid=21249)\u001b[0m WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "\u001b[2m\u001b[36m(pid=21249)\u001b[0m I0000 00:00:1746123871.873421   21249 fork_posix.cc:75] Other threads are currently calling into gRPC, skipping fork() handlers\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=21251)\u001b[0m 2025-05-01 18:24:33,569\tINFO eager_tf_policy_v2.py:75 -- Creating TF-eager policy running on CPU.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=21251)\u001b[0m 2025-05-01 18:24:33,584\tINFO policy.py:1214 -- Policy (worker=4) running on CPU.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=21251)\u001b[0m 2025-05-01 18:24:33,593\tINFO catalog.py:477 -- Wrapping <class 'graphenv.examples.tsp.tsp_model.TSPModel'> as None\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=21249)\u001b[0m 2025-05-01 18:24:33,586\tINFO eager_tf_policy_v2.py:75 -- Creating TF-eager policy running on CPU.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=21249)\u001b[0m 2025-05-01 18:24:33,601\tINFO policy.py:1214 -- Policy (worker=2) running on CPU.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=21249)\u001b[0m 2025-05-01 18:24:33,611\tINFO catalog.py:477 -- Wrapping <class 'graphenv.examples.tsp.tsp_model.TSPModel'> as None\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=21250)\u001b[0m 2025-05-01 18:24:33,736\tINFO eager_tf_policy_v2.py:75 -- Creating TF-eager policy running on CPU.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=21248)\u001b[0m 2025-05-01 18:24:33,746\tWARNING env.py:156 -- Your env doesn't have a .spec.max_episode_steps attribute. Your horizon will default to infinity, and your environment will not be reset.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=21248)\u001b[0m 2025-05-01 18:24:33,767\tINFO eager_tf_policy_v2.py:75 -- Creating TF-eager policy running on CPU.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=21248)\u001b[0m 2025-05-01 18:24:33,782\tINFO policy.py:1214 -- Policy (worker=1) running on CPU.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=21248)\u001b[0m 2025-05-01 18:24:33,792\tINFO catalog.py:477 -- Wrapping <class 'graphenv.examples.tsp.tsp_model.TSPModel'> as None\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=21251)\u001b[0m 2025-05-01 18:24:33,787\tINFO util.py:122 -- Using connectors:\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=21251)\u001b[0m 2025-05-01 18:24:33,788\tINFO util.py:123 --     AgentConnectorPipeline\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=21251)\u001b[0m         ObsPreprocessorConnector\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=21251)\u001b[0m         StateBufferConnector\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=21251)\u001b[0m         ViewRequirementAgentConnector\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=21251)\u001b[0m 2025-05-01 18:24:33,788\tINFO util.py:124 --     ActionConnectorPipeline\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=21251)\u001b[0m         ConvertToNumpyConnector\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=21251)\u001b[0m         NormalizeActionsConnector\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=21251)\u001b[0m         ImmutableActionsConnector\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=21250)\u001b[0m 2025-05-01 18:24:33,752\tINFO policy.py:1214 -- Policy (worker=3) running on CPU.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=21250)\u001b[0m 2025-05-01 18:24:33,761\tINFO catalog.py:477 -- Wrapping <class 'graphenv.examples.tsp.tsp_model.TSPModel'> as None\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=21249)\u001b[0m 2025-05-01 18:24:33,801\tINFO util.py:122 -- Using connectors:\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=21249)\u001b[0m 2025-05-01 18:24:33,801\tINFO util.py:123 --     AgentConnectorPipeline\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=21249)\u001b[0m         ObsPreprocessorConnector\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=21249)\u001b[0m         StateBufferConnector\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=21249)\u001b[0m         ViewRequirementAgentConnector\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=21249)\u001b[0m 2025-05-01 18:24:33,801\tINFO util.py:124 --     ActionConnectorPipeline\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=21249)\u001b[0m         ConvertToNumpyConnector\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=21249)\u001b[0m         NormalizeActionsConnector\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=21249)\u001b[0m         ImmutableActionsConnector\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=21250)\u001b[0m 2025-05-01 18:24:33,945\tINFO util.py:122 -- Using connectors:\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=21250)\u001b[0m 2025-05-01 18:24:33,945\tINFO util.py:123 --     AgentConnectorPipeline\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=21250)\u001b[0m         ObsPreprocessorConnector\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=21250)\u001b[0m         StateBufferConnector\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=21250)\u001b[0m         ViewRequirementAgentConnector\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=21250)\u001b[0m 2025-05-01 18:24:33,945\tINFO util.py:124 --     ActionConnectorPipeline\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=21250)\u001b[0m         ConvertToNumpyConnector\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=21250)\u001b[0m         NormalizeActionsConnector\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=21250)\u001b[0m         ImmutableActionsConnector\n",
      "\u001b[2m\u001b[36m(PPO pid=21126)\u001b[0m 2025-05-01 18:24:33,984\tINFO worker_set.py:310 -- Inferred observation/action spaces from remote worker (local worker has no env): {'default_policy': (Repeated(Dict('nbr_dist': Box(0.0, 1.4142135623730951, (1,), float64), 'node_idx': Box(0, 5, (1,), int64), 'node_obs': Box(0.0, 1.0, (2,), float64), 'parent_dist': Box(0.0, 1.4142135623730951, (1,), float64)), 6), Discrete(5)), '__env__': (Repeated(Dict('nbr_dist': Box(0.0, 1.4142135623730951, (1,), float64), 'node_idx': Box(0, 5, (1,), int64), 'node_obs': Box(0.0, 1.0, (2,), float64), 'parent_dist': Box(0.0, 1.4142135623730951, (1,), float64)), 6), Discrete(5))}\n",
      "\u001b[2m\u001b[36m(PPO pid=21126)\u001b[0m 2025-05-01 18:24:34,004\tINFO eager_tf_policy_v2.py:75 -- Creating TF-eager policy running on CPU.\n",
      "\u001b[2m\u001b[36m(PPO pid=21126)\u001b[0m 2025-05-01 18:24:34,018\tINFO policy.py:1214 -- Policy (worker=local) running on CPU.\n",
      "\u001b[2m\u001b[36m(PPO pid=21126)\u001b[0m 2025-05-01 18:24:34,026\tINFO catalog.py:477 -- Wrapping <class 'graphenv.examples.tsp.tsp_model.TSPModel'> as None\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=21248)\u001b[0m 2025-05-01 18:24:33,979\tINFO util.py:122 -- Using connectors:\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=21248)\u001b[0m 2025-05-01 18:24:33,979\tINFO util.py:123 --     AgentConnectorPipeline\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=21248)\u001b[0m         ObsPreprocessorConnector\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=21248)\u001b[0m         StateBufferConnector\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=21248)\u001b[0m         ViewRequirementAgentConnector\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=21248)\u001b[0m 2025-05-01 18:24:33,979\tINFO util.py:124 --     ActionConnectorPipeline\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=21248)\u001b[0m         ConvertToNumpyConnector\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=21248)\u001b[0m         NormalizeActionsConnector\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=21248)\u001b[0m         ImmutableActionsConnector\n",
      "\u001b[2m\u001b[36m(PPO pid=21126)\u001b[0m 2025-05-01 18:24:34,205\tINFO util.py:122 -- Using connectors:\n",
      "\u001b[2m\u001b[36m(PPO pid=21126)\u001b[0m 2025-05-01 18:24:34,205\tINFO util.py:123 --     AgentConnectorPipeline\n",
      "\u001b[2m\u001b[36m(PPO pid=21126)\u001b[0m         ObsPreprocessorConnector\n",
      "\u001b[2m\u001b[36m(PPO pid=21126)\u001b[0m         StateBufferConnector\n",
      "\u001b[2m\u001b[36m(PPO pid=21126)\u001b[0m         ViewRequirementAgentConnector\n",
      "\u001b[2m\u001b[36m(PPO pid=21126)\u001b[0m 2025-05-01 18:24:34,206\tINFO util.py:124 --     ActionConnectorPipeline\n",
      "\u001b[2m\u001b[36m(PPO pid=21126)\u001b[0m         ConvertToNumpyConnector\n",
      "\u001b[2m\u001b[36m(PPO pid=21126)\u001b[0m         NormalizeActionsConnector\n",
      "\u001b[2m\u001b[36m(PPO pid=21126)\u001b[0m         ImmutableActionsConnector\n",
      "\u001b[2m\u001b[36m(PPO pid=21126)\u001b[0m 2025-05-01 18:24:34,206\tINFO rollout_worker.py:2043 -- Built policy map: <PolicyMap lru-caching-capacity=100 policy-IDs=['default_policy']>\n",
      "\u001b[2m\u001b[36m(PPO pid=21126)\u001b[0m 2025-05-01 18:24:34,206\tINFO rollout_worker.py:2044 -- Built preprocessor map: {'default_policy': None}\n",
      "\u001b[2m\u001b[36m(PPO pid=21126)\u001b[0m 2025-05-01 18:24:34,206\tINFO rollout_worker.py:760 -- Built filter map: defaultdict(<class 'ray.rllib.utils.filter.NoFilter'>, {})\n",
      "\u001b[2m\u001b[36m(PPO pid=21126)\u001b[0m 2025-05-01 18:24:34,210\tINFO algorithm_config.py:2888 -- Executing eagerly (framework='tf2'), with eager_tracing=tf2. For production workloads, make sure to set eager_tracing=True  in order to match the speed of tf-static-graph (framework='tf'). For debugging purposes, `eager_tracing=False` is the best choice.\n",
      "\u001b[2m\u001b[36m(PPO pid=21126)\u001b[0m 2025-05-01 18:24:34,214\tWARNING env.py:156 -- Your env doesn't have a .spec.max_episode_steps attribute. Your horizon will default to infinity, and your environment will not be reset.\n",
      "\u001b[2m\u001b[36m(PPO pid=21126)\u001b[0m 2025-05-01 18:24:34,219\tINFO eager_tf_policy_v2.py:75 -- Creating TF-eager policy running on CPU.\n",
      "\u001b[2m\u001b[36m(PPO pid=21126)\u001b[0m 2025-05-01 18:24:34,220\tINFO policy.py:1214 -- Policy (worker=local) running on CPU.\n",
      "\u001b[2m\u001b[36m(PPO pid=21126)\u001b[0m 2025-05-01 18:24:34,221\tINFO catalog.py:477 -- Wrapping <class 'graphenv.examples.tsp.tsp_model.TSPModel'> as None\n",
      "\u001b[2m\u001b[36m(PPO pid=21126)\u001b[0m 2025-05-01 18:24:34,297\tINFO util.py:122 -- Using connectors:\n",
      "\u001b[2m\u001b[36m(PPO pid=21126)\u001b[0m 2025-05-01 18:24:34,297\tINFO util.py:123 --     AgentConnectorPipeline\n",
      "\u001b[2m\u001b[36m(PPO pid=21126)\u001b[0m         ObsPreprocessorConnector\n",
      "\u001b[2m\u001b[36m(PPO pid=21126)\u001b[0m         StateBufferConnector\n",
      "\u001b[2m\u001b[36m(PPO pid=21126)\u001b[0m         ViewRequirementAgentConnector\n",
      "\u001b[2m\u001b[36m(PPO pid=21126)\u001b[0m 2025-05-01 18:24:34,297\tINFO util.py:124 --     ActionConnectorPipeline\n",
      "\u001b[2m\u001b[36m(PPO pid=21126)\u001b[0m         ConvertToNumpyConnector\n",
      "\u001b[2m\u001b[36m(PPO pid=21126)\u001b[0m         NormalizeActionsConnector\n",
      "\u001b[2m\u001b[36m(PPO pid=21126)\u001b[0m         ImmutableActionsConnector\n",
      "\u001b[2m\u001b[36m(PPO pid=21126)\u001b[0m 2025-05-01 18:24:34,297\tINFO rollout_worker.py:2043 -- Built policy map: <PolicyMap lru-caching-capacity=100 policy-IDs=['default_policy']>\n",
      "\u001b[2m\u001b[36m(PPO pid=21126)\u001b[0m 2025-05-01 18:24:34,297\tINFO rollout_worker.py:2044 -- Built preprocessor map: {'default_policy': None}\n",
      "\u001b[2m\u001b[36m(PPO pid=21126)\u001b[0m 2025-05-01 18:24:34,297\tINFO rollout_worker.py:760 -- Built filter map: defaultdict(<class 'ray.rllib.utils.filter.NoFilter'>, {})\n",
      "\u001b[2m\u001b[36m(PPO pid=21126)\u001b[0m 2025-05-01 18:24:34,297\tWARNING util.py:67 -- Install gputil for GPU system monitoring.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=21248)\u001b[0m 2025-05-01 18:24:34,300\tINFO rollout_worker.py:908 -- Generating sample batch of size 5\n",
      "\u001b[2m\u001b[36m(PPO pid=21126)\u001b[0m 2025-05-01 18:24:34,570\tWARNING deprecation.py:50 -- DeprecationWarning: `_get_slice_indices` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=21126)\u001b[0m /home/vladimir/.pyenv/versions/3.9.21/lib/python3.9/site-packages/numpy/core/_methods.py:180: RuntimeWarning: overflow encountered in reduce\n",
      "\u001b[2m\u001b[36m(PPO pid=21126)\u001b[0m   ret = umr_sum(arr, axis, dtype, out, keepdims, where=where)\n",
      "\u001b[2m\u001b[36m(PPO pid=21126)\u001b[0m 2025-05-01 18:24:34,571\tINFO rollout_worker.py:1004 -- Training on concatenated sample batches:\n",
      "\u001b[2m\u001b[36m(PPO pid=21126)\u001b[0m \n",
      "\u001b[2m\u001b[36m(PPO pid=21126)\u001b[0m { 'count': 2,\n",
      "\u001b[2m\u001b[36m(PPO pid=21126)\u001b[0m   'policy_batches': { 'default_policy': { 'action_dist_inputs': np.ndarray((2, 5), dtype=float32, min=-3.4028234663852886e+38, max=1.074, mean=-inf),\n",
      "\u001b[2m\u001b[36m(PPO pid=21126)\u001b[0m                                           'action_logp': np.ndarray((2,), dtype=float32, min=-1.388, max=-0.0, mean=-0.694),\n",
      "\u001b[2m\u001b[36m(PPO pid=21126)\u001b[0m                                           'actions': np.ndarray((2,), dtype=int32, min=0.0, max=3.0, mean=1.5),\n",
      "\u001b[2m\u001b[36m(PPO pid=21126)\u001b[0m                                           'advantages': np.ndarray((2,), dtype=float32, min=-1.205, max=1.061, mean=-0.072),\n",
      "\u001b[2m\u001b[36m(PPO pid=21126)\u001b[0m                                           'agent_index': np.ndarray((2,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(PPO pid=21126)\u001b[0m                                           'eps_id': np.ndarray((2,), dtype=int64, min=6.294587637328873e+17, max=6.294587637328873e+17, mean=6.294587637328873e+17),\n",
      "\u001b[2m\u001b[36m(PPO pid=21126)\u001b[0m                                           'new_obs': np.ndarray((2, 31), dtype=float32, min=0.0, max=4.0, mean=0.38),\n",
      "\u001b[2m\u001b[36m(PPO pid=21126)\u001b[0m                                           'obs': np.ndarray((2, 31), dtype=float32, min=0.0, max=5.0, mean=0.485),\n",
      "\u001b[2m\u001b[36m(PPO pid=21126)\u001b[0m                                           'rewards': np.ndarray((2,), dtype=float32, min=-0.629, max=-0.097, mean=-0.363),\n",
      "\u001b[2m\u001b[36m(PPO pid=21126)\u001b[0m                                           't': np.ndarray((2,), dtype=int64, min=0.0, max=4.0, mean=2.0),\n",
      "\u001b[2m\u001b[36m(PPO pid=21126)\u001b[0m                                           'terminateds': np.ndarray((2,), dtype=bool, min=0.0, max=1.0, mean=0.5),\n",
      "\u001b[2m\u001b[36m(PPO pid=21126)\u001b[0m                                           'truncateds': np.ndarray((2,), dtype=bool, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(PPO pid=21126)\u001b[0m                                           'unroll_id': np.ndarray((2,), dtype=int64, min=1.0, max=1.0, mean=1.0),\n",
      "\u001b[2m\u001b[36m(PPO pid=21126)\u001b[0m                                           'value_targets': np.ndarray((2,), dtype=float32, min=-2.351, max=-0.629, mean=-1.49),\n",
      "\u001b[2m\u001b[36m(PPO pid=21126)\u001b[0m                                           'vf_preds': np.ndarray((2,), dtype=float32, min=0.875, max=0.996, mean=0.935)}},\n",
      "\u001b[2m\u001b[36m(PPO pid=21126)\u001b[0m   'type': 'MultiAgentBatch'}\n",
      "\u001b[2m\u001b[36m(PPO pid=21126)\u001b[0m \n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=21248)\u001b[0m /home/vladimir/.pyenv/versions/3.9.21/lib/python3.9/site-packages/numpy/core/_methods.py:180: RuntimeWarning: overflow encountered in reduce\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=21248)\u001b[0m   ret = umr_sum(arr, axis, dtype, out, keepdims, where=where)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=21248)\u001b[0m 2025-05-01 18:24:34,568\tINFO rollout_worker.py:949 -- Completed sample batch:\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=21248)\u001b[0m \n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=21248)\u001b[0m { 'count': 5,\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=21248)\u001b[0m   'policy_batches': { 'default_policy': { 'action_dist_inputs': np.ndarray((5, 5), dtype=float32, min=-3.4028234663852886e+38, max=1.093, mean=-inf),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=21248)\u001b[0m                                           'action_logp': np.ndarray((5,), dtype=float32, min=-1.31, max=-0.0, mean=-0.613),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=21248)\u001b[0m                                           'actions': np.ndarray((5,), dtype=int32, min=0.0, max=1.0, mean=0.6),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=21248)\u001b[0m                                           'advantages': np.ndarray((5,), dtype=float32, min=-3.391, max=-1.392, mean=-2.252),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=21248)\u001b[0m                                           'agent_index': np.ndarray((5,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=21248)\u001b[0m                                           'eps_id': np.ndarray((5,), dtype=int64, min=8.927060436898299e+17, max=8.927060436898299e+17, mean=8.927060436898299e+17),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=21248)\u001b[0m                                           'new_obs': np.ndarray((5, 31), dtype=float32, min=0.0, max=4.0, mean=0.353),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=21248)\u001b[0m                                           'obs': np.ndarray((5, 31), dtype=float32, min=0.0, max=5.0, mean=0.487),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=21248)\u001b[0m                                           'rewards': np.ndarray((5,), dtype=float32, min=-0.845, max=-0.144, mean=-0.497),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=21248)\u001b[0m                                           't': np.ndarray((5,), dtype=int64, min=0.0, max=4.0, mean=2.0),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=21248)\u001b[0m                                           'terminateds': np.ndarray((5,), dtype=bool, min=0.0, max=1.0, mean=0.2),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=21248)\u001b[0m                                           'truncateds': np.ndarray((5,), dtype=bool, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=21248)\u001b[0m                                           'unroll_id': np.ndarray((5,), dtype=int64, min=1.0, max=1.0, mean=1.0),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=21248)\u001b[0m                                           'value_targets': np.ndarray((5,), dtype=float32, min=-2.445, max=-0.425, mean=-1.32),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=21248)\u001b[0m                                           'vf_preds': np.ndarray((5,), dtype=float32, min=0.889, max=0.967, mean=0.932)}},\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=21248)\u001b[0m   'type': 'MultiAgentBatch'}\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=21248)\u001b[0m \n",
      "\u001b[2m\u001b[36m(PPO pid=21126)\u001b[0m 2025-05-01 18:24:34,725\tWARNING tf_utils.py:576 -- KL divergence is non-finite, this will likely destabilize your model and the training process. Action(s) in a specific state have near-zero probability. This can happen naturally in deterministic environments where the optimal policy has zero mass for a specific action. To fix this issue, consider setting the coefficient for the KL loss term to zero or increasing policy entropy.\n",
      "\u001b[2m\u001b[36m(PPO pid=21126)\u001b[0m 2025-05-01 18:24:34,897\tINFO eager_tf_policy_v2.py:930 -- Optimizing variable hidden_layer_1/kernel:0\n",
      "\u001b[2m\u001b[36m(PPO pid=21126)\u001b[0m 2025-05-01 18:24:34,897\tINFO eager_tf_policy_v2.py:930 -- Optimizing variable hidden_layer_1/bias:0\n",
      "\u001b[2m\u001b[36m(PPO pid=21126)\u001b[0m 2025-05-01 18:24:34,897\tINFO eager_tf_policy_v2.py:930 -- Optimizing variable embed_layer/embeddings:0\n",
      "\u001b[2m\u001b[36m(PPO pid=21126)\u001b[0m 2025-05-01 18:24:34,897\tINFO eager_tf_policy_v2.py:930 -- Optimizing variable hidden_layer_2/kernel:0\n",
      "\u001b[2m\u001b[36m(PPO pid=21126)\u001b[0m 2025-05-01 18:24:34,897\tINFO eager_tf_policy_v2.py:930 -- Optimizing variable hidden_layer_2/bias:0\n",
      "\u001b[2m\u001b[36m(PPO pid=21126)\u001b[0m 2025-05-01 18:24:34,897\tINFO eager_tf_policy_v2.py:930 -- Optimizing variable action_value_output/kernel:0\n",
      "\u001b[2m\u001b[36m(PPO pid=21126)\u001b[0m 2025-05-01 18:24:34,897\tINFO eager_tf_policy_v2.py:930 -- Optimizing variable action_value_output/bias:0\n",
      "\u001b[2m\u001b[36m(PPO pid=21126)\u001b[0m 2025-05-01 18:24:34,897\tINFO eager_tf_policy_v2.py:930 -- Optimizing variable action_weight_output/kernel:0\n",
      "\u001b[2m\u001b[36m(PPO pid=21126)\u001b[0m 2025-05-01 18:24:34,897\tINFO eager_tf_policy_v2.py:930 -- Optimizing variable action_weight_output/bias:0\n",
      "\u001b[2m\u001b[36m(PPO pid=21126)\u001b[0m 2025-05-01 18:24:35,243\tWARNING tf_utils.py:576 -- KL divergence is non-finite, this will likely destabilize your model and the training process. Action(s) in a specific state have near-zero probability. This can happen naturally in deterministic environments where the optimal policy has zero mass for a specific action. To fix this issue, consider setting the coefficient for the KL loss term to zero or increasing policy entropy.\n",
      "\u001b[2m\u001b[36m(PPO pid=21126)\u001b[0m 2025-05-01 18:24:35,755\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=21126)\u001b[0m 2025-05-01 18:24:35,755\tINFO rollout_worker.py:908 -- Generating sample batch of size 1\n",
      "\u001b[2m\u001b[36m(PPO pid=21126)\u001b[0m 2025-05-01 18:24:36,006\tINFO rollout_worker.py:949 -- Completed sample batch:\n",
      "\u001b[2m\u001b[36m(PPO pid=21126)\u001b[0m \n",
      "\u001b[2m\u001b[36m(PPO pid=21126)\u001b[0m { 'count': 5,\n",
      "\u001b[2m\u001b[36m(PPO pid=21126)\u001b[0m   'policy_batches': { 'default_policy': { 'advantages': np.ndarray((5,), dtype=float32, min=-2.101, max=-0.548, mean=-1.23),\n",
      "\u001b[2m\u001b[36m(PPO pid=21126)\u001b[0m                                           'agent_index': np.ndarray((5,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(PPO pid=21126)\u001b[0m                                           'eps_id': np.ndarray((5,), dtype=int64, min=8.193016373313325e+17, max=8.193016373313325e+17, mean=8.193016373313325e+17),\n",
      "\u001b[2m\u001b[36m(PPO pid=21126)\u001b[0m                                           'new_obs': np.ndarray((5, 31), dtype=float32, min=0.0, max=4.0, mean=0.367),\n",
      "\u001b[2m\u001b[36m(PPO pid=21126)\u001b[0m                                           'obs': np.ndarray((5, 31), dtype=float32, min=0.0, max=5.0, mean=0.497),\n",
      "\u001b[2m\u001b[36m(PPO pid=21126)\u001b[0m                                           'rewards': np.ndarray((5,), dtype=float32, min=-0.703, max=-0.074, mean=-0.424),\n",
      "\u001b[2m\u001b[36m(PPO pid=21126)\u001b[0m                                           't': np.ndarray((5,), dtype=int64, min=0.0, max=4.0, mean=2.0),\n",
      "\u001b[2m\u001b[36m(PPO pid=21126)\u001b[0m                                           'terminateds': np.ndarray((5,), dtype=bool, min=0.0, max=1.0, mean=0.2),\n",
      "\u001b[2m\u001b[36m(PPO pid=21126)\u001b[0m                                           'truncateds': np.ndarray((5,), dtype=bool, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(PPO pid=21126)\u001b[0m                                           'unroll_id': np.ndarray((5,), dtype=int64, min=1.0, max=1.0, mean=1.0),\n",
      "\u001b[2m\u001b[36m(PPO pid=21126)\u001b[0m                                           'value_targets': np.ndarray((5,), dtype=float32, min=-2.069, max=-0.533, mean=-1.418),\n",
      "\u001b[2m\u001b[36m(PPO pid=21126)\u001b[0m                                           'vf_preds': np.ndarray((5,), dtype=float32, min=-0.373, max=0.032, mean=-0.188)}},\n",
      "\u001b[2m\u001b[36m(PPO pid=21126)\u001b[0m   'type': 'MultiAgentBatch'}\n",
      "\u001b[2m\u001b[36m(PPO pid=21126)\u001b[0m \n",
      "\u001b[2m\u001b[36m(PPO pid=21126)\u001b[0m 2025-05-01 18:24:36,863\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=21126)\u001b[0m 2025-05-01 18:24:37,703\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=21126)\u001b[0m 2025-05-01 18:24:38,537\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=21126)\u001b[0m 2025-05-01 18:24:39,373\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=21126)\u001b[0m 2025-05-01 18:24:40,207\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=21126)\u001b[0m 2025-05-01 18:24:41,046\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=21126)\u001b[0m 2025-05-01 18:24:41,900\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=21126)\u001b[0m 2025-05-01 18:24:42,745\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=21126)\u001b[0m 2025-05-01 18:24:43,580\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=21126)\u001b[0m 2025-05-01 18:24:44,431\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=21126)\u001b[0m 2025-05-01 18:24:45,279\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=21126)\u001b[0m 2025-05-01 18:24:46,138\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=21126)\u001b[0m 2025-05-01 18:24:47,009\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=21126)\u001b[0m 2025-05-01 18:24:47,871\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=21126)\u001b[0m 2025-05-01 18:24:48,719\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=21126)\u001b[0m 2025-05-01 18:24:49,574\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=21126)\u001b[0m 2025-05-01 18:24:50,417\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=21126)\u001b[0m 2025-05-01 18:24:51,251\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=21126)\u001b[0m 2025-05-01 18:24:52,110\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=21126)\u001b[0m 2025-05-01 18:24:52,964\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=21126)\u001b[0m 2025-05-01 18:24:53,826\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=21126)\u001b[0m 2025-05-01 18:24:54,687\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=21126)\u001b[0m 2025-05-01 18:24:55,550\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=21126)\u001b[0m 2025-05-01 18:24:56,416\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=21126)\u001b[0m 2025-05-01 18:24:57,289\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=21126)\u001b[0m 2025-05-01 18:24:58,151\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=21126)\u001b[0m 2025-05-01 18:24:59,010\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=21126)\u001b[0m 2025-05-01 18:24:59,872\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=21126)\u001b[0m 2025-05-01 18:25:00,740\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=21126)\u001b[0m 2025-05-01 18:25:01,612\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=21126)\u001b[0m 2025-05-01 18:25:02,482\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=21126)\u001b[0m 2025-05-01 18:25:03,346\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=21126)\u001b[0m 2025-05-01 18:25:04,202\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=21126)\u001b[0m 2025-05-01 18:25:05,059\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=21126)\u001b[0m 2025-05-01 18:25:05,911\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=21126)\u001b[0m 2025-05-01 18:25:06,758\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=21126)\u001b[0m 2025-05-01 18:25:07,617\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=21126)\u001b[0m 2025-05-01 18:25:08,459\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=21126)\u001b[0m 2025-05-01 18:25:09,303\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=21126)\u001b[0m 2025-05-01 18:25:10,161\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=21126)\u001b[0m 2025-05-01 18:25:10,998\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=21126)\u001b[0m 2025-05-01 18:25:11,835\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=21126)\u001b[0m 2025-05-01 18:25:12,684\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=21126)\u001b[0m 2025-05-01 18:25:13,521\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=21126)\u001b[0m 2025-05-01 18:25:14,377\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=21126)\u001b[0m 2025-05-01 18:25:15,235\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=21126)\u001b[0m 2025-05-01 18:25:16,094\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=21126)\u001b[0m 2025-05-01 18:25:16,949\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=21126)\u001b[0m 2025-05-01 18:25:17,814\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=21126)\u001b[0m 2025-05-01 18:25:18,705\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=21126)\u001b[0m 2025-05-01 18:25:19,559\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=21126)\u001b[0m 2025-05-01 18:25:20,420\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=21126)\u001b[0m 2025-05-01 18:25:21,280\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=21126)\u001b[0m 2025-05-01 18:25:22,145\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=21126)\u001b[0m 2025-05-01 18:25:23,011\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=21126)\u001b[0m 2025-05-01 18:25:23,871\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=21126)\u001b[0m 2025-05-01 18:25:24,727\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=21126)\u001b[0m 2025-05-01 18:25:25,584\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=21126)\u001b[0m 2025-05-01 18:25:26,447\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=21126)\u001b[0m 2025-05-01 18:25:27,322\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=21126)\u001b[0m 2025-05-01 18:25:28,179\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=21126)\u001b[0m 2025-05-01 18:25:29,029\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=21126)\u001b[0m 2025-05-01 18:25:29,879\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=21126)\u001b[0m 2025-05-01 18:25:30,730\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=21126)\u001b[0m 2025-05-01 18:25:31,583\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=21126)\u001b[0m 2025-05-01 18:25:32,436\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=21126)\u001b[0m 2025-05-01 18:25:33,298\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=21126)\u001b[0m 2025-05-01 18:25:34,149\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=21126)\u001b[0m 2025-05-01 18:25:34,993\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=21126)\u001b[0m 2025-05-01 18:25:35,847\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=21126)\u001b[0m 2025-05-01 18:25:36,699\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=21126)\u001b[0m 2025-05-01 18:25:37,549\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=21126)\u001b[0m 2025-05-01 18:25:38,411\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=21126)\u001b[0m 2025-05-01 18:25:39,263\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=21126)\u001b[0m 2025-05-01 18:25:40,103\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=21126)\u001b[0m 2025-05-01 18:25:40,944\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=21126)\u001b[0m 2025-05-01 18:25:41,787\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=21126)\u001b[0m 2025-05-01 18:25:42,632\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=21126)\u001b[0m 2025-05-01 18:25:43,482\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=21126)\u001b[0m 2025-05-01 18:25:44,362\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=21126)\u001b[0m 2025-05-01 18:25:45,228\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=21126)\u001b[0m 2025-05-01 18:25:46,070\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=21126)\u001b[0m 2025-05-01 18:25:46,910\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=21126)\u001b[0m 2025-05-01 18:25:47,742\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=21126)\u001b[0m 2025-05-01 18:25:48,581\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=21126)\u001b[0m 2025-05-01 18:25:49,423\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=21126)\u001b[0m 2025-05-01 18:25:50,274\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=21126)\u001b[0m 2025-05-01 18:25:51,115\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=21126)\u001b[0m 2025-05-01 18:25:51,958\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=21126)\u001b[0m 2025-05-01 18:25:52,810\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=21126)\u001b[0m 2025-05-01 18:25:53,652\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=21126)\u001b[0m 2025-05-01 18:25:54,487\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=21126)\u001b[0m 2025-05-01 18:25:55,331\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=21126)\u001b[0m 2025-05-01 18:25:56,169\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=21126)\u001b[0m 2025-05-01 18:25:57,014\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=21126)\u001b[0m 2025-05-01 18:25:57,868\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=21126)\u001b[0m 2025-05-01 18:25:58,725\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=21126)\u001b[0m 2025-05-01 18:25:59,577\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=21126)\u001b[0m 2025-05-01 18:26:00,435\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(pid=22196)\u001b[0m WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "\u001b[2m\u001b[36m(pid=22196)\u001b[0m I0000 00:00:1746124033.915143   22196 fork_posix.cc:75] Other threads are currently calling into gRPC, skipping fork() handlers\n",
      "\u001b[2m\u001b[36m(pid=22195)\u001b[0m WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "\u001b[2m\u001b[36m(pid=22195)\u001b[0m I0000 00:00:1746124033.923617   22195 fork_posix.cc:75] Other threads are currently calling into gRPC, skipping fork() handlers\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=22196)\u001b[0m 2025-05-01 18:27:15,625\tINFO eager_tf_policy_v2.py:75 -- Creating TF-eager policy running on CPU.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=22196)\u001b[0m 2025-05-01 18:27:15,640\tINFO policy.py:1214 -- Policy (worker=2) running on CPU.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=22195)\u001b[0m 2025-05-01 18:27:15,642\tWARNING env.py:156 -- Your env doesn't have a .spec.max_episode_steps attribute. Your horizon will default to infinity, and your environment will not be reset.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=22196)\u001b[0m 2025-05-01 18:27:15,649\tINFO catalog.py:477 -- Wrapping <class 'graphenv.examples.tsp.tsp_model.TSPModel'> as None\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=22195)\u001b[0m 2025-05-01 18:27:15,658\tINFO eager_tf_policy_v2.py:75 -- Creating TF-eager policy running on CPU.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=22195)\u001b[0m 2025-05-01 18:27:15,674\tINFO policy.py:1214 -- Policy (worker=1) running on CPU.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=22195)\u001b[0m 2025-05-01 18:27:15,684\tINFO catalog.py:477 -- Wrapping <class 'graphenv.examples.tsp.tsp_model.TSPModel'> as None\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=22196)\u001b[0m 2025-05-01 18:27:15,838\tINFO util.py:122 -- Using connectors:\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=22196)\u001b[0m 2025-05-01 18:27:15,838\tINFO util.py:123 --     AgentConnectorPipeline\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=22196)\u001b[0m         ObsPreprocessorConnector\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=22196)\u001b[0m         StateBufferConnector\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=22196)\u001b[0m         ViewRequirementAgentConnector\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=22196)\u001b[0m 2025-05-01 18:27:15,838\tINFO util.py:124 --     ActionConnectorPipeline\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=22196)\u001b[0m         ConvertToNumpyConnector\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=22196)\u001b[0m         NormalizeActionsConnector\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=22196)\u001b[0m         ImmutableActionsConnector\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=22197)\u001b[0m 2025-05-01 18:27:15,845\tINFO eager_tf_policy_v2.py:75 -- Creating TF-eager policy running on CPU.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=22197)\u001b[0m 2025-05-01 18:27:15,860\tINFO policy.py:1214 -- Policy (worker=3) running on CPU.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=22197)\u001b[0m 2025-05-01 18:27:15,869\tINFO catalog.py:477 -- Wrapping <class 'graphenv.examples.tsp.tsp_model.TSPModel'> as None\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=22195)\u001b[0m 2025-05-01 18:27:15,878\tINFO util.py:122 -- Using connectors:\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=22195)\u001b[0m 2025-05-01 18:27:15,878\tINFO util.py:123 --     AgentConnectorPipeline\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=22195)\u001b[0m         ObsPreprocessorConnector\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=22195)\u001b[0m         StateBufferConnector\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=22195)\u001b[0m         ViewRequirementAgentConnector\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=22195)\u001b[0m 2025-05-01 18:27:15,878\tINFO util.py:124 --     ActionConnectorPipeline\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=22195)\u001b[0m         ConvertToNumpyConnector\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=22195)\u001b[0m         NormalizeActionsConnector\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=22195)\u001b[0m         ImmutableActionsConnector\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=22198)\u001b[0m 2025-05-01 18:27:15,926\tINFO eager_tf_policy_v2.py:75 -- Creating TF-eager policy running on CPU.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=22198)\u001b[0m 2025-05-01 18:27:15,940\tINFO policy.py:1214 -- Policy (worker=4) running on CPU.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=22198)\u001b[0m 2025-05-01 18:27:15,949\tINFO catalog.py:477 -- Wrapping <class 'graphenv.examples.tsp.tsp_model.TSPModel'> as None\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=22197)\u001b[0m 2025-05-01 18:27:16,058\tINFO util.py:122 -- Using connectors:\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=22197)\u001b[0m 2025-05-01 18:27:16,058\tINFO util.py:123 --     AgentConnectorPipeline\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=22197)\u001b[0m         ObsPreprocessorConnector\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=22197)\u001b[0m         StateBufferConnector\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=22197)\u001b[0m         ViewRequirementAgentConnector\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=22197)\u001b[0m 2025-05-01 18:27:16,058\tINFO util.py:124 --     ActionConnectorPipeline\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=22197)\u001b[0m         ConvertToNumpyConnector\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=22197)\u001b[0m         NormalizeActionsConnector\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=22197)\u001b[0m         ImmutableActionsConnector\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=22198)\u001b[0m 2025-05-01 18:27:16,131\tINFO util.py:122 -- Using connectors:\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=22198)\u001b[0m 2025-05-01 18:27:16,131\tINFO util.py:123 --     AgentConnectorPipeline\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=22198)\u001b[0m         ObsPreprocessorConnector\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=22198)\u001b[0m         StateBufferConnector\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=22198)\u001b[0m         ViewRequirementAgentConnector\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=22198)\u001b[0m 2025-05-01 18:27:16,131\tINFO util.py:124 --     ActionConnectorPipeline\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=22198)\u001b[0m         ConvertToNumpyConnector\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=22198)\u001b[0m         NormalizeActionsConnector\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=22198)\u001b[0m         ImmutableActionsConnector\n"
     ]
    }
   ],
   "source": [
    "print(f\"Running with following CLI options: {args}\")\n",
    "logging.basicConfig(level=args.log_level.upper())\n",
    "ray.init(local_mode=args.local_mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7ac9d4e5-2477-461b-a467-4f0f457826fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.1786471 , 0.14306129, 0.20869372, 0.53118409],\n",
       "       [0.1786471 , 0.        , 0.20562852, 0.3842079 , 0.39536284],\n",
       "       [0.14306129, 0.20562852, 0.        , 0.2462733 , 0.60040816],\n",
       "       [0.20869372, 0.3842079 , 0.2462733 , 0.        , 0.73154383],\n",
       "       [0.53118409, 0.39536284, 0.60040816, 0.73154383, 0.        ]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N = args.N\n",
    "G = make_complete_planar_graph(N=N, seed=args.seed)\n",
    "nx.to_numpy_array(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a9a56466-059f-45a7-ac36-aa4a93e27ea4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Networkx heuristic reward: -1.700\n",
      "[0, 4, 1, 3, 2, 0]\n",
      "Networkx greedy reward: -1.996\n",
      "[0, 2, 1, 3, 4, 0]\n"
     ]
    }
   ],
   "source": [
    "tsp_approx = nx.approximation.traveling_salesman_problem\n",
    "path = tsp_approx(G, cycle=True)\n",
    "reward_baseline = -sum([G[path[i]][path[i + 1]][\"weight\"] for i in range(0, N)])\n",
    "print(f\"Networkx heuristic reward: {reward_baseline:1.3f}\")\n",
    "print(path)\n",
    "path = tsp_approx(G, cycle=True, method=greedy_tsp)\n",
    "reward_baseline = -sum([G[path[i]][path[i + 1]][\"weight\"] for i in range(0, N)])\n",
    "print(f\"Networkx greedy reward: {reward_baseline:1.3f}\")\n",
    "print(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c508cb95-5a0b-49c9-bc4a-1aa49ab413ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Algorithm-specific config, common ones are in the main config dict below\n",
    "if args.run == \"PPO\":\n",
    "    run_config = PPOConfig()\n",
    "    train_batch_size = args.rollouts_per_worker * N * args.num_workers\n",
    "    sgd_minibatch_size = 16 if train_batch_size > 16 else 2\n",
    "    run_config.training(entropy_coeff=args.entropy_coeff,\n",
    "                        sgd_minibatch_size=sgd_minibatch_size,\n",
    "                        num_sgd_iter=5,\n",
    "    )\n",
    "elif args.run in [\"DQN\"]:\n",
    "    run_config = DQNConfig()\n",
    "    # Update here with custom config\n",
    "    run_config.training(hiddens=False,\n",
    "                    dueling=False\n",
    "    )\n",
    "    run_config.exploration(exploration_config={\"epsilon_timesteps\": 250000})\n",
    "elif args.run == \"A3C\":\n",
    "    run_config = A3CConfig()\n",
    "elif args.run == \"MARWIL\":\n",
    "    run_config = MARWILConfig()\n",
    "else:\n",
    "    raise ValueError(f\"Import agent {args.run} and try again\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "27e446a3-976f-46f0-899a-da10ab18e3a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define custom_model, config, and state based on GNN yes/no\n",
    "if args.use_gnn:\n",
    "    custom_model = \"TSPGNNModel\"\n",
    "    custom_model_config = {\"num_messages\": 3, \"embed_dim\": 32}\n",
    "    ModelCatalog.register_custom_model(custom_model, TSPGNNModel)\n",
    "    _tag = \"gnn\"\n",
    "    state = TSPNFPState(\n",
    "        lambda: make_complete_planar_graph(N=N),\n",
    "        max_num_neighbors=args.max_num_neighbors,\n",
    "    )\n",
    "else:\n",
    "    custom_model_config = {\"hidden_dim\": 256, \"embed_dim\": 256, \"num_nodes\": N}\n",
    "    custom_model = \"TSPModel\"\n",
    "    Model = TSPQModel if args.run in [\"DQN\", \"R2D2\"] else TSPModel\n",
    "    ModelCatalog.register_custom_model(custom_model, Model)\n",
    "    _tag = f\"basic{args.run}\"\n",
    "    state = TSPState(lambda: make_complete_planar_graph(N=N))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "505c95ca-d686-4c5a-890f-cbd9a64531de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register env name with hyperparams that will help tracking experiments\n",
    "# via tensorboard\n",
    "env_name = f\"mygraphenv-v0\" #_{N}_{_tag}_lr={args.lr}\n",
    "register_env(env_name, lambda config: GraphEnv(config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e0d687e6-fe6f-498e-880e-fee875f2fd0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "args.num_gpus = 0\n",
    "args.num_workers = 4\n",
    "run_config = (\n",
    "    run_config\n",
    "    .resources(num_gpus=args.num_gpus) \n",
    "    .framework(\"tf2\") #tf ?\n",
    "    .rollouts(num_rollout_workers=args.num_workers, \n",
    "              # a multiple of N (collect whole episodes)\n",
    "              rollout_fragment_length=N\n",
    "             )\n",
    "    .environment(env=env_name,\n",
    "                 env_config={\"state\": state, \n",
    "                             \"max_num_children\": G.number_of_nodes()}\n",
    "              )\n",
    "    .training(lr=args.lr,\n",
    "              train_batch_size=args.rollouts_per_worker * N * args.num_workers,\n",
    "              model={\"custom_model\": custom_model, \n",
    "                     \"custom_model_config\": custom_model_config}\n",
    "              )\n",
    "    .evaluation(evaluation_config={\"explore\": False},\n",
    "                evaluation_interval=1, \n",
    "                evaluation_duration=100,\n",
    "              )\n",
    "    .debugging(log_level=args.log_level)\n",
    "    .framework(eager_tracing=True)\n",
    ")\n",
    "\n",
    "stop = {\n",
    "    \"training_iteration\": args.stop_iters,\n",
    "    \"timesteps_total\": args.stop_timesteps,\n",
    "    \"episode_reward_mean\": args.stop_reward,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d29b8901-6bec-42a3-a0f6-97358ab62ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_path = Path(\"/home/vladimir/work/graph_test/scratch/ray_results\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e7557961-396a-49cf-839c-6251fc2f7d07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2025-05-01 18:26:01</td></tr>\n",
       "<tr><td>Running for: </td><td>00:01:34.35        </td></tr>\n",
       "<tr><td>Memory:      </td><td>8.0/125.7 GiB      </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      Using FIFO scheduling algorithm.<br>Resources requested: 0/20 CPUs, 0/2 GPUs, 0.0/77.96 GiB heap, 0.0/37.4 GiB objects (0.0/1.0 accelerator_type:G)\n",
       "    </div>\n",
       "    \n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name                   </th><th>status    </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_mygraphenv-v0_83b0c_00000</td><td>TERMINATED</td><td>192.168.0.126:21126</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">         84.8138</td><td style=\"text-align: right;\">2000</td><td style=\"text-align: right;\">-2.32887</td><td style=\"text-align: right;\">            -1.14646</td><td style=\"text-align: right;\">            -3.20093</td><td style=\"text-align: right;\">                 5</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-01 18:24:26,900\tINFO algorithm_config.py:2888 -- Executing eagerly (framework='tf2'), with eager_tracing=tf2. For production workloads, make sure to set eager_tracing=True  in order to match the speed of tf-static-graph (framework='tf'). For debugging purposes, `eager_tracing=False` is the best choice.\n",
      "2025-05-01 18:24:26,906\tINFO algorithm_config.py:2888 -- Executing eagerly (framework='tf2'), with eager_tracing=tf2. For production workloads, make sure to set eager_tracing=True  in order to match the speed of tf-static-graph (framework='tf'). For debugging purposes, `eager_tracing=False` is the best choice.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"trialProgress\">\n",
       "  <h3>Trial Progress</h3>\n",
       "  <table>\n",
       "<thead>\n",
       "<tr><th>Trial name                   </th><th style=\"text-align: right;\">  agent_timesteps_total</th><th>connector_metrics                                                                                                                                             </th><th>counters                                                                                                                        </th><th>custom_metrics  </th><th>date               </th><th>done  </th><th style=\"text-align: right;\">  episode_len_mean</th><th>episode_media  </th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_mean</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episodes_this_iter</th><th style=\"text-align: right;\">  episodes_total</th><th>evaluation                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    </th><th>experiment_id                   </th><th>hostname  </th><th>info                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   </th><th style=\"text-align: right;\">  iterations_since_restore</th><th>node_ip      </th><th style=\"text-align: right;\">  num_agent_steps_sampled</th><th style=\"text-align: right;\">  num_agent_steps_trained</th><th style=\"text-align: right;\">  num_env_steps_sampled</th><th style=\"text-align: right;\">  num_env_steps_sampled_this_iter</th><th style=\"text-align: right;\">  num_env_steps_trained</th><th style=\"text-align: right;\">  num_env_steps_trained_this_iter</th><th style=\"text-align: right;\">  num_faulty_episodes</th><th style=\"text-align: right;\">  num_healthy_workers</th><th style=\"text-align: right;\">  num_in_flight_async_reqs</th><th style=\"text-align: right;\">  num_remote_worker_restarts</th><th style=\"text-align: right;\">  num_steps_trained_this_iter</th><th>perf                                              </th><th style=\"text-align: right;\">  pid</th><th>policy_reward_max  </th><th>policy_reward_mean  </th><th>policy_reward_min  </th><th>sampler_perf                                                                                                                                                                                                     </th><th>sampler_results                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        </th><th style=\"text-align: right;\">  time_since_restore</th><th style=\"text-align: right;\">  time_this_iter_s</th><th style=\"text-align: right;\">  time_total_s</th><th>timers                                                                                                                       </th><th style=\"text-align: right;\">  timestamp</th><th style=\"text-align: right;\">  timesteps_since_restore</th><th style=\"text-align: right;\">  timesteps_total</th><th style=\"text-align: right;\">  training_iteration</th><th>trial_id   </th><th style=\"text-align: right;\">  warmup_time</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_mygraphenv-v0_83b0c_00000</td><td style=\"text-align: right;\">                   2000</td><td>{&#x27;ObsPreprocessorConnector_ms&#x27;: 0.03676104545593262, &#x27;StateBufferConnector_ms&#x27;: 0.007767200469970703, &#x27;ViewRequirementAgentConnector_ms&#x27;: 0.10059976577758789}</td><td>{&#x27;num_env_steps_sampled&#x27;: 2000, &#x27;num_env_steps_trained&#x27;: 2000, &#x27;num_agent_steps_sampled&#x27;: 2000, &#x27;num_agent_steps_trained&#x27;: 2000}</td><td>{}              </td><td>2025-05-01_18-26-01</td><td>True  </td><td style=\"text-align: right;\">                 5</td><td>{}             </td><td style=\"text-align: right;\">            -1.14646</td><td style=\"text-align: right;\">             -2.32887</td><td style=\"text-align: right;\">            -3.20093</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">             400</td><td>{&#x27;episode_reward_max&#x27;: -1.0382531494150822, &#x27;episode_reward_min&#x27;: -3.741914818388316, &#x27;episode_reward_mean&#x27;: -2.2476783066819817, &#x27;episode_len_mean&#x27;: 5.0, &#x27;episode_media&#x27;: {}, &#x27;episodes_this_iter&#x27;: 100, &#x27;policy_reward_min&#x27;: {}, &#x27;policy_reward_max&#x27;: {}, &#x27;policy_reward_mean&#x27;: {}, &#x27;custom_metrics&#x27;: {}, &#x27;hist_stats&#x27;: {&#x27;episode_reward&#x27;: [-2.1254908512281365, -1.2917786344157192, -2.7794127442422445, -2.3737772654242146, -2.096694781312544, -1.998214312998111, -2.5031531048397353, -1.915698498585109, -2.492899001981852, -2.7103124590930316, -2.3879840768367107, -2.9381158688073308, -1.9883952161542804, -2.3683534235219295, -2.827053691594947, -2.4857539936880473, -2.1648105267079463, -1.9267901337690134, -1.943836528223068, -1.575827524040641, -2.419939636433561, -2.817876793314558, -2.865244315516788, -2.3417808895906087, -2.064721903229385, -2.691461603935197, -3.3073526065781946, -1.3276743579954244, -3.15256238668833, -2.587069990529321, -2.543093937930438, -2.516395615074977, -2.49882346959784, -2.2483607726886032, -2.481960482672434, -1.7122295100717926, -2.6059026230999534, -3.449432646215185, -2.053056284368886, -2.6166380872115487, -2.681262272944611, -1.1027037534154638, -2.457549285413978, -1.8922633421590538, -1.9041564216870932, -2.446745009318397, -2.270821133236576, -2.656279150808281, -2.4138248192742107, -2.0067714038070976, -1.5486310102993088, -2.2365595581046227, -2.416607170455187, -2.070211251324009, -1.7638408062713098, -2.186394603007687, -2.367315016414315, -1.9641248247851797, -2.330072955996229, -1.935339928115452, -2.1108316426959535, -2.243735216693254, -2.08342640986155, -2.2640335570799333, -1.9770719114226822, -2.4504604926538853, -1.822147607052899, -2.5927899303820747, -1.9921400692703162, -1.4294876508309025, -1.7488732134226983, -2.8486038253220842, -1.8252003529119836, -2.011604199985138, -1.6943493959966518, -2.2511309430721624, -2.4694389292117567, -2.508040027721897, -2.402603738873854, -3.741914818388316, -2.383153718361382, -2.6293828271289494, -1.9496939720364428, -2.1746490763015425, -2.3377224658771896, -1.618912174975453, -1.8332610480950264, -2.940507453537559, -1.657488937451832, -2.4303967489875085, -2.4270049804930887, -2.45808027110558, -1.745605392215356, -1.9439206874240376, -1.0382531494150822, -2.6206897621060303, -1.5084642019451242, -2.24917657690128, -2.594247917273677, -1.9119311066703504], &#x27;episode_lengths&#x27;: [5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5]}, &#x27;sampler_perf&#x27;: {&#x27;mean_raw_obs_processing_ms&#x27;: 0.43354376595329425, &#x27;mean_inference_ms&#x27;: 0.8059981555877301, &#x27;mean_action_processing_ms&#x27;: 0.08907841862693998, &#x27;mean_env_wait_ms&#x27;: 0.06784589974246658, &#x27;mean_env_render_ms&#x27;: 0.0}, &#x27;num_faulty_episodes&#x27;: 0, &#x27;connector_metrics&#x27;: {&#x27;ObsPreprocessorConnector_ms&#x27;: 0.03167915344238281, &#x27;StateBufferConnector_ms&#x27;: 0.0060787200927734375, &#x27;ViewRequirementAgentConnector_ms&#x27;: 0.08473324775695801}, &#x27;num_agent_steps_sampled_this_iter&#x27;: 500, &#x27;num_env_steps_sampled_this_iter&#x27;: 500, &#x27;timesteps_this_iter&#x27;: 500, &#x27;num_healthy_workers&#x27;: 0, &#x27;num_in_flight_async_reqs&#x27;: 0, &#x27;num_remote_worker_restarts&#x27;: 0}</td><td>d5b151283ff14efb98ca5dda411140ae</td><td>srv5      </td><td>{&#x27;learner&#x27;: {&#x27;default_policy&#x27;: {&#x27;learner_stats&#x27;: {&#x27;cur_kl_coeff&#x27;: 3.1554436679038213e-31, &#x27;cur_lr&#x27;: 9.999999747378752e-05, &#x27;total_loss&#x27;: 0.21769394, &#x27;policy_loss&#x27;: -0.010065695, &#x27;vf_loss&#x27;: 0.22775963, &#x27;vf_explained_var&#x27;: 0.2493176, &#x27;kl&#x27;: 0.0006022395, &#x27;entropy&#x27;: 0.38405335, &#x27;entropy_coeff&#x27;: 0.0}, &#x27;custom_metrics&#x27;: {}, &#x27;num_agent_steps_trained&#x27;: 2.0, &#x27;num_grad_updates_lifetime&#x27;: 4975.5, &#x27;diff_num_grad_updates_vs_sampler_policy&#x27;: 24.5}}, &#x27;num_env_steps_sampled&#x27;: 2000, &#x27;num_env_steps_trained&#x27;: 2000, &#x27;num_agent_steps_sampled&#x27;: 2000, &#x27;num_agent_steps_trained&#x27;: 2000}</td><td style=\"text-align: right;\">                       100</td><td>192.168.0.126</td><td style=\"text-align: right;\">                     2000</td><td style=\"text-align: right;\">                     2000</td><td style=\"text-align: right;\">                   2000</td><td style=\"text-align: right;\">                               20</td><td style=\"text-align: right;\">                   2000</td><td style=\"text-align: right;\">                               20</td><td style=\"text-align: right;\">                    0</td><td style=\"text-align: right;\">                    4</td><td style=\"text-align: right;\">                         0</td><td style=\"text-align: right;\">                           0</td><td style=\"text-align: right;\">                           20</td><td>{&#x27;cpu_util_percent&#x27;: 6.4, &#x27;ram_util_percent&#x27;: 6.4}</td><td style=\"text-align: right;\">21126</td><td>{}                 </td><td>{}                  </td><td>{}                 </td><td>{&#x27;mean_raw_obs_processing_ms&#x27;: 0.5756774617410609, &#x27;mean_inference_ms&#x27;: 1.6284009398993817, &#x27;mean_action_processing_ms&#x27;: 0.10941484692081475, &#x27;mean_env_wait_ms&#x27;: 0.08529641176247782, &#x27;mean_env_render_ms&#x27;: 0.0}</td><td>{&#x27;episode_reward_max&#x27;: -1.1464611892376801, &#x27;episode_reward_min&#x27;: -3.200930589558242, &#x27;episode_reward_mean&#x27;: -2.3288708628100623, &#x27;episode_len_mean&#x27;: 5.0, &#x27;episode_media&#x27;: {}, &#x27;episodes_this_iter&#x27;: 4, &#x27;policy_reward_min&#x27;: {}, &#x27;policy_reward_max&#x27;: {}, &#x27;policy_reward_mean&#x27;: {}, &#x27;custom_metrics&#x27;: {}, &#x27;hist_stats&#x27;: {&#x27;episode_reward&#x27;: [-2.388984540232118, -1.2956705627551097, -3.0269021765014408, -2.871336713078322, -1.8946789544887843, -2.6087670941878898, -2.3238982518758657, -2.421402961723609, -2.6534302124778737, -2.170424256420173, -1.850432168817969, -2.872134814653089, -1.619673165283086, -1.884573291573757, -1.2478918736153988, -2.4630825977324444, -2.639806891409734, -2.631024335774482, -2.0551456059717945, -2.572691997123049, -3.0948187063354244, -2.589696414182098, -2.645068841072532, -2.7708974674344455, -1.7945784544626255, -2.6847378306814784, -2.758691704691069, -2.858921751355586, -2.8270625664793, -2.419992161290342, -3.0120276115173055, -2.0664348305856906, -1.7347696573832805, -1.518139172591099, -1.9618898065292618, -2.3408693989287443, -2.057995338647973, -1.8091140151459562, -2.171533539484203, -2.784003195736911, -2.1446224222476364, -2.325353927212247, -2.482228180553632, -1.8544583243307287, -2.8477146182633266, -2.6855138608530713, -2.551306000398193, -2.663511801666885, -2.0547490319469803, -2.607279097006721, -2.140749199445432, -1.844831633056356, -2.36156222869029, -2.8578718450433707, -2.195219611959812, -2.1840371228333746, -2.7532372986882994, -2.826235550772615, -1.1464611892376801, -2.4569316649473745, -2.5244708987019373, -1.220607916161287, -2.6590120320878188, -1.941129238033662, -2.4453914320090386, -1.844006983571282, -2.0412584849088224, -1.8138912879173614, -2.4725418200347002, -2.4529466664723256, -2.8270457775119224, -2.006987611698472, -2.9196046306312278, -2.3941057671609722, -3.200930589558242, -1.8312028062300985, -2.3424384910953187, -1.852359658782794, -2.9454913675595726, -1.8426851468945133, -2.7121143482345733, -2.2455369947903345, -2.2842135804408317, -2.7461724579675977, -2.5958119408773785, -2.20368351686906, -2.765672967055498, -1.8816303769746123, -2.258179347184148, -2.0186826363423784, -2.1278566607935687, -1.8720596347276826, -2.7538756225761896, -2.7231737799677105, -2.31942062890263, -2.1189018381162237, -2.3435131970577623, -1.8571882253364007, -2.3198228195391737, -2.7843975608457794], &#x27;episode_lengths&#x27;: [5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5]}, &#x27;sampler_perf&#x27;: {&#x27;mean_raw_obs_processing_ms&#x27;: 0.5756774617410609, &#x27;mean_inference_ms&#x27;: 1.6284009398993817, &#x27;mean_action_processing_ms&#x27;: 0.10941484692081475, &#x27;mean_env_wait_ms&#x27;: 0.08529641176247782, &#x27;mean_env_render_ms&#x27;: 0.0}, &#x27;num_faulty_episodes&#x27;: 0, &#x27;connector_metrics&#x27;: {&#x27;ObsPreprocessorConnector_ms&#x27;: 0.03676104545593262, &#x27;StateBufferConnector_ms&#x27;: 0.007767200469970703, &#x27;ViewRequirementAgentConnector_ms&#x27;: 0.10059976577758789}}</td><td style=\"text-align: right;\">             84.8138</td><td style=\"text-align: right;\">          0.834555</td><td style=\"text-align: right;\">       84.8138</td><td>{&#x27;training_iteration_time_ms&#x27;: 107.769, &#x27;learn_time_ms&#x27;: 92.838, &#x27;learn_throughput&#x27;: 215.428, &#x27;synch_weights_time_ms&#x27;: 3.094}</td><td style=\"text-align: right;\"> 1746123961</td><td style=\"text-align: right;\">                        0</td><td style=\"text-align: right;\">             2000</td><td style=\"text-align: right;\">                 100</td><td>83b0c_00000</td><td style=\"text-align: right;\">      4.04257</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "</div>\n",
       "<style>\n",
       ".trialProgress {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".trialProgress h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".trialProgress td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-01 18:26:01,611\tINFO tune.py:798 -- Total run time: 94.75 seconds (94.31 seconds for the tuning loop).\n"
     ]
    }
   ],
   "source": [
    "res = tune.run(\n",
    "    args.run,\n",
    "    config=run_config.to_dict(),\n",
    "    stop=stop,\n",
    "    local_dir=my_path,\n",
    "    checkpoint_freq = 10,\n",
    "    checkpoint_at_end=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fc6ec20-14b4-4068-9f95-c98e5e790b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4eae4fe9-c52d-4451-884b-7bed2662d361",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-01 18:27:09,104\tINFO experiment_analysis.py:789 -- No `self.trials`. Drawing logdirs from checkpoint file. This may result in some information that is out of sync, as checkpointing is periodic.\n"
     ]
    }
   ],
   "source": [
    "# Загружаем результаты\n",
    "analysis = ExperimentAnalysis(my_path)\n",
    "best_trial = analysis.get_best_trial(metric=\"episode_reward_max\", mode=\"max\")\n",
    "best_checkpoint = analysis.get_best_checkpoint(best_trial, mode = \"max\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f642ad15-8584-4449-a466-7bbbedec047e",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = GraphEnv({\"state\": state, \n",
    "          \"max_num_children\": G.number_of_nodes()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4f6df3ee-a3a5-42e8-a2c5-cc619b021e77",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-01 18:27:12,799\tINFO algorithm_config.py:2888 -- Executing eagerly (framework='tf2'), with eager_tracing=tf2. For production workloads, make sure to set eager_tracing=True  in order to match the speed of tf-static-graph (framework='tf'). For debugging purposes, `eager_tracing=False` is the best choice.\n",
      "2025-05-01 18:27:16,136\tINFO worker_set.py:310 -- Inferred observation/action spaces from remote worker (local worker has no env): {'default_policy': (Repeated(Dict('nbr_dist': Box(0.0, 1.4142135623730951, (1,), float64), 'node_idx': Box(0, 5, (1,), int64), 'node_obs': Box(0.0, 1.0, (2,), float64), 'parent_dist': Box(0.0, 1.4142135623730951, (1,), float64)), 6), Discrete(5)), '__env__': (Repeated(Dict('nbr_dist': Box(0.0, 1.4142135623730951, (1,), float64), 'node_idx': Box(0, 5, (1,), int64), 'node_obs': Box(0.0, 1.0, (2,), float64), 'parent_dist': Box(0.0, 1.4142135623730951, (1,), float64)), 6), Discrete(5))}\n",
      "2025-05-01 18:27:16,140\tINFO eager_tf_policy_v2.py:75 -- Creating TF-eager policy running on GPU.\n",
      "2025-05-01 18:27:16,142\tINFO policy.py:1214 -- Policy (worker=local) running on CPU.\n",
      "2025-05-01 18:27:16,142\tINFO catalog.py:477 -- Wrapping <class 'graphenv.examples.tsp.tsp_model.TSPModel'> as None\n",
      "2025-05-01 18:27:16,251\tINFO util.py:122 -- Using connectors:\n",
      "2025-05-01 18:27:16,251\tINFO util.py:123 --     AgentConnectorPipeline\n",
      "        ObsPreprocessorConnector\n",
      "        StateBufferConnector\n",
      "        ViewRequirementAgentConnector\n",
      "2025-05-01 18:27:16,252\tINFO util.py:124 --     ActionConnectorPipeline\n",
      "        ConvertToNumpyConnector\n",
      "        NormalizeActionsConnector\n",
      "        ImmutableActionsConnector\n",
      "2025-05-01 18:27:16,252\tINFO rollout_worker.py:2043 -- Built policy map: <PolicyMap lru-caching-capacity=100 policy-IDs=['default_policy']>\n",
      "2025-05-01 18:27:16,252\tINFO rollout_worker.py:2044 -- Built preprocessor map: {'default_policy': None}\n",
      "2025-05-01 18:27:16,253\tINFO rollout_worker.py:760 -- Built filter map: defaultdict(<class 'ray.rllib.utils.filter.NoFilter'>, {})\n",
      "2025-05-01 18:27:16,257\tINFO algorithm_config.py:2888 -- Executing eagerly (framework='tf2'), with eager_tracing=tf2. For production workloads, make sure to set eager_tracing=True  in order to match the speed of tf-static-graph (framework='tf'). For debugging purposes, `eager_tracing=False` is the best choice.\n",
      "2025-05-01 18:27:16,266\tINFO eager_tf_policy_v2.py:75 -- Creating TF-eager policy running on GPU.\n",
      "2025-05-01 18:27:16,268\tINFO policy.py:1214 -- Policy (worker=local) running on CPU.\n",
      "2025-05-01 18:27:16,268\tINFO catalog.py:477 -- Wrapping <class 'graphenv.examples.tsp.tsp_model.TSPModel'> as None\n",
      "2025-05-01 18:27:16,357\tINFO util.py:122 -- Using connectors:\n",
      "2025-05-01 18:27:16,358\tINFO util.py:123 --     AgentConnectorPipeline\n",
      "        ObsPreprocessorConnector\n",
      "        StateBufferConnector\n",
      "        ViewRequirementAgentConnector\n",
      "2025-05-01 18:27:16,358\tINFO util.py:124 --     ActionConnectorPipeline\n",
      "        ConvertToNumpyConnector\n",
      "        NormalizeActionsConnector\n",
      "        ImmutableActionsConnector\n",
      "2025-05-01 18:27:16,358\tINFO rollout_worker.py:2043 -- Built policy map: <PolicyMap lru-caching-capacity=100 policy-IDs=['default_policy']>\n",
      "2025-05-01 18:27:16,359\tINFO rollout_worker.py:2044 -- Built preprocessor map: {'default_policy': None}\n",
      "2025-05-01 18:27:16,359\tINFO rollout_worker.py:760 -- Built filter map: defaultdict(<class 'ray.rllib.utils.filter.NoFilter'>, {})\n",
      "2025-05-01 18:27:16,360\tWARNING util.py:67 -- Install gputil for GPU system monitoring.\n"
     ]
    }
   ],
   "source": [
    "algo = Algorithm.from_checkpoint(best_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "dd17c22a-08e2-46d0-b34b-731bf9245e93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 3 -0.4066376678985208 {}\n",
      "1 2 -0.2750684250004176 {}\n",
      "2 0 -0.1859166479415223 {}\n",
      "3 0 -0.6496229391255659 {}\n",
      "4 0 -0.24954859274920732 {}\n",
      "[0, 4, 3, 1, 2, 0] -1.766794272715234\n"
     ]
    }
   ],
   "source": [
    "episode_reward = 0\n",
    "terminated = truncated = False\n",
    "obs, info = env.reset()\n",
    "i = 0\n",
    "path = [obs[0]['node_idx'][0]]\n",
    "while not terminated and not truncated and i < 20: \n",
    "    action = algo.compute_single_action(obs, explore = False)\n",
    "    obs, reward, terminated, truncated, info = env.step(action)\n",
    "    print(i, action, reward, info)\n",
    "    episode_reward += reward\n",
    "    path.append(obs[0]['node_idx'][0])\n",
    "    i += 1\n",
    "print(path, episode_reward )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23cb0992-d953-4eb1-9b59-fc87283234fe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

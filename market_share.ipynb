{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "-o2TCW9MIRk-"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import ray.rllib.agents.ppo as ppo\n",
    "import ray.rllib.agents.dqn as dqn\n",
    "import ray.rllib.agents.a3c as a3c\n",
    "from ray.tune.logger import pretty_print\n",
    "import gym\n",
    "from gym.utils.env_checker import check_env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3L0pin0j372Y",
    "outputId": "ab82b1e5-35d9-4a68-adc4-ca7d385bbd9b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RayContext(dashboard_url='', python_version='3.9.11', ray_version='1.12.0', ray_commit='f18fc31c7562990955556899090f8e8656b48d2d', address_info={'node_ip_address': '127.0.0.1', 'raylet_ip_address': '127.0.0.1', 'redis_address': None, 'object_store_address': '/tmp/ray/session_2023-09-05_18-28-34_482700_52968/sockets/plasma_store', 'raylet_socket_name': '/tmp/ray/session_2023-09-05_18-28-34_482700_52968/sockets/raylet', 'webui_url': '', 'session_dir': '/tmp/ray/session_2023-09-05_18-28-34_482700_52968', 'metrics_export_port': 62490, 'gcs_address': '127.0.0.1:63008', 'address': '127.0.0.1:63008', 'node_id': 'f9175ab2d6eaec976fbee1c156b6a76835fc6e08c77b81805ec30c95'})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ray\n",
    "ray.shutdown()\n",
    "ray.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "id": "cVoFKTbl4Oy7"
   },
   "outputs": [],
   "source": [
    "config = dqn.DEFAULT_CONFIG.copy()\n",
    "config[\"num_gpus\"] = 0\n",
    "config[\"num_workers\"] = 1\n",
    "config[\"framework\"] = \"torch\"\n",
    "config[\"env_config\"] = {}\n",
    "# config['kl_coeff'] = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "B = np.array([786,759,888,649]).astype(np.float64)\n",
    "A = np.array([[300,400,10,50,900],\n",
    "              [200,400,10,50,900],\n",
    "              [100,400,10,50,900],\n",
    "              [ 50,400,10,50,900]]).astype(np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "id": "VUVDOmMALlCy"
   },
   "outputs": [],
   "source": [
    "class MyEnv(gym.Env):\n",
    "    def __init__(self, env_config):\n",
    "        super().__init__()\n",
    "        self.action_space = gym.spaces.Discrete(5)\n",
    "        self.observation_space = gym.spaces.Box(low=-1000, high=1000, shape=(4,5), dtype=np.float64)\n",
    "        self.aa = np.array(A)\n",
    "        self.bb = np.array(B)\n",
    "        self.state = self.bb[:,np.newaxis] - self.aa\n",
    "        self.done = False\n",
    "\n",
    "    def reset(self):\n",
    "        self.aa = np.array(A)\n",
    "        self.bb = np.array(B)\n",
    "        self.state = self.bb[:,np.newaxis] - self.aa\n",
    "        self.done = False\n",
    "        return self.state\n",
    "\n",
    "    def step(self, action):\n",
    "        if np.any(self.state < 0, axis = 0)[action]:\n",
    "            self.reward = -50.0\n",
    "            self.done = False\n",
    "        else:\n",
    "            self.bb = self.bb - self.aa[:,action]\n",
    "            self.aa[:,action] = self.bb + 1\n",
    "            self.reward = np.sum(A[:,action])/100.0\n",
    "            self.state = self.bb[:,np.newaxis] - self.aa\n",
    "            if np.all(self.state < 0):\n",
    "                self.done = True\n",
    "            else:\n",
    "                self.done = False\n",
    "        return self.state, self.reward, self.done, {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g8FAxdt_yfhA",
    "outputId": "6a922119-3e20-4914-b22d-176c237d792a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RolloutWorker pid=57603)\u001b[0m 2023-09-05 19:43:44,503\tWARNING rollout_worker.py:498 -- We've added a module for checking environments that are used in experiments. It will cause your environment to fail if your environment is not set upcorrectly. You can disable check env by setting `disable_env_checking` to True in your experiment config dictionary. You can run the environment checking module standalone by calling ray.rllib.utils.check_env(env).\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=57603)\u001b[0m 2023-09-05 19:43:44,503\tWARNING env.py:120 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
      "2023-09-05 19:43:44,535\tWARNING util.py:60 -- Install gputil for GPU system monitoring.\n"
     ]
    }
   ],
   "source": [
    "# agent3 = ppo.PPOTrainer(config=config, env=MyEnv)\n",
    "agent3 = dqn.DQNTrainer(config=config, env=MyEnv)\n",
    "# agent3 = a3c.A3CTrainer(config=config, env=MyEnv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rxrIbbM9yWiz",
    "outputId": "fc8ba23e-ecfa-4543-b987-bd5b0c8a895f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i:  0\n",
      "mean episode length: 10.505263157894737\n",
      "max episode reward: 24.9\n",
      "mean episode reward: -300.3631578947367\n",
      "min episode reward: -1125.1\n",
      "total episodes: 95\n",
      "\n",
      "i:  2\n",
      "mean episode length: 11.97\n",
      "max episode reward: 24.9\n",
      "mean episode reward: -373.6\n",
      "min episode reward: -1825.1\n",
      "total episodes: 266\n",
      "\n",
      "i:  4\n",
      "mean episode length: 12.42\n",
      "max episode reward: 24.9\n",
      "mean episode reward: -396.0999999999999\n",
      "min episode reward: -1825.1\n",
      "total episodes: 431\n",
      "\n",
      "i:  6\n",
      "mean episode length: 16.07\n",
      "max episode reward: 24.9\n",
      "mean episode reward: -578.6\n",
      "min episode reward: -3875.1\n",
      "total episodes: 553\n",
      "\n",
      "i:  8\n",
      "mean episode length: 26.81\n",
      "max episode reward: 24.9\n",
      "mean episode reward: -1115.6000000000004\n",
      "min episode reward: -11225.1\n",
      "total episodes: 616\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "   # Perform one iteration of training the policy with PPO\n",
    "   result = agent3.train()\n",
    "   if i % 2 == 0 or i == 49:\n",
    "       # print(pretty_print(result))\n",
    "       print('i: ', i)\n",
    "       print('mean episode length:', result['episode_len_mean'])\n",
    "       print('max episode reward:', result['episode_reward_max'])\n",
    "       print('mean episode reward:', result['episode_reward_mean'])\n",
    "       print('min episode reward:', result['episode_reward_min'])\n",
    "       print('total episodes:', result['episodes_total'])\n",
    "       print()\n",
    "       # checkpoint = agent3.save()\n",
    "       # print(\"checkpoint saved at\", checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "MIkjj6AV1u2u",
    "outputId": "cf869f1f-6905-43fe-f6e5-67f8cf3fcb7f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state = [[ 436.  336.  726.   -1. -164.]\n",
      " [ 509.  309.  699.   -1. -191.]\n",
      " [ 738.  438.  828.   -1.  -62.]\n",
      " [ 549.  199.  589.   -1. -301.]] action = 3 reward = 2.0 done = False\n",
      "state = [[  36.   -1.  326. -401. -564.]\n",
      " [ 109.   -1.  299. -401. -591.]\n",
      " [ 338.   -1.  428. -401. -462.]\n",
      " [ 149.   -1.  189. -401. -701.]] action = 1 reward = 16.0 done = False\n",
      "state = [[  -1. -301.   26. -701. -864.]\n",
      " [  -1. -201.   99. -601. -791.]\n",
      " [  -1. -101.  328. -501. -562.]\n",
      " [  -1.  -51.  139. -451. -751.]] action = 0 reward = 6.5 done = False\n",
      "state = [[ -11. -311.   -1. -711. -874.]\n",
      " [ -11. -211.   -1. -611. -801.]\n",
      " [ -11. -111.   -1. -511. -572.]\n",
      " [ -11.  -61.   -1. -461. -761.]] action = 2 reward = 0.4 done = True\n",
      "24.9\n"
     ]
    }
   ],
   "source": [
    "env = MyEnv(config)\n",
    "state = env.reset()\n",
    "g = 0\n",
    "done = False\n",
    "reward = 0\n",
    "i = 0\n",
    "while (not done) and i < 50:\n",
    "  action = agent3.compute_action(state, explore = False)\n",
    "  state, reward, done, info = env.step(action)\n",
    "  print(f\"state = {state} action = {action} reward = {reward} done = {done}\")\n",
    "  g += reward\n",
    "  i += 1\n",
    "print(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "Optimization.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

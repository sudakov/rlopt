{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e0b76303-0641-468a-94b2-2b1414759fce",
   "metadata": {},
   "source": [
    "# Обучение с учителем для решения задач коммивояжера\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3ae68ec8-1737-4490-ae49-1139d3c815e6",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-10 06:17:06.361022: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'  # Скрывает INFO-логи (оставляет WARNING и ERROR)\n",
    "import logging\n",
    "logging.getLogger('tensorflow').setLevel(logging.ERROR)\n",
    "import absl.logging\n",
    "absl.logging.set_verbosity(absl.logging.ERROR)\n",
    "import tensorflow as tf\n",
    "tf.get_logger().setLevel('ERROR')  # Скрывает большинство логов TensorFlow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "58cef161-e45d-43f9-8a3e-8b407265b3f7",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.layers import Input, Dense, LayerNormalization\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import backend as K\n",
    "from tqdm import tqdm\n",
    "#from python_tsp.exact import solve_tsp_dynamic_programming, solve_tsp_branch_and_bound\n",
    "#from python_tsp.heuristics import solve_tsp_local_search, solve_tsp_simulated_annealing\n",
    "#from python_tsp.heuristics import solve_tsp_lin_kernighan, solve_tsp_record_to_record\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "374ecaac-3813-49bb-9ce1-cbae15403042",
   "metadata": {},
   "source": [
    "### Модель предсказывает какие дуги входят в маршрут, но не их последовательность"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2431ebc5-3df5-48c8-b68c-713e2eebb9e5",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class TSPSolver:\n",
    "    def __init__(self, num_cities, hidden_dim=256):\n",
    "        self.num_cities = num_cities\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.model = self._build_model()\n",
    "    \n",
    "    def _build_model(self):\n",
    "        # Вход: матрица расстояний (batch, cities, cities)\n",
    "        inputs = Input(shape=(self.num_cities, self.num_cities))\n",
    "        \n",
    "        # Кодировщик на основе полносвязных слоев\n",
    "        x = Dense(self.hidden_dim, activation='relu')(inputs)\n",
    "        x = LayerNormalization()(x)\n",
    "        x = Dense(self.hidden_dim, activation='relu')(x)\n",
    "        x = LayerNormalization()(x)\n",
    "        \n",
    "        # Выходной слой - вероятности переходов\n",
    "        logits = Dense(self.num_cities)(x)\n",
    "        outputs = tf.keras.activations.softmax(logits)\n",
    "        \n",
    "        model = Model(inputs=inputs, outputs=outputs)\n",
    "        model.compile(optimizer=Adam(0.001), loss=self._custom_loss)\n",
    "        return model\n",
    "    \n",
    "    def _custom_loss(self, y_true, y_pred):\n",
    "        # y_true: маска посещенных городов (batch, cities, cities)\n",
    "        # y_pred: вероятности переходов (batch, cities, cities)\n",
    "        \n",
    "        # Применяем маску к предсказаниям\n",
    "        masked_pred = y_pred * y_true\n",
    "        \n",
    "        # Нормализуем вероятности\n",
    "        masked_pred = masked_pred / (K.sum(masked_pred, axis=-1, keepdims=True) + K.epsilon())\n",
    "        \n",
    "        # Вычисляем кросс-энтропию\n",
    "        loss = -K.sum(y_true * K.log(masked_pred + K.epsilon()), axis=-1)\n",
    "        return K.mean(loss)\n",
    "    \n",
    "    def train(self, X_train, routes, epochs=50, batch_size=128):\n",
    "        \"\"\"\n",
    "        X_train: матрицы расстояний (samples, cities, cities)\n",
    "        routes: оптимальные маршруты (samples, cities)\n",
    "        \"\"\"\n",
    "        # Создаем маски переходов для обучения\n",
    "        y_masks = np.zeros_like(X_train)\n",
    "        \n",
    "        for i, route in enumerate(routes):\n",
    "            for j in range(len(route)-1):\n",
    "                from_city = route[j]\n",
    "                to_city = route[j+1]\n",
    "                y_masks[i, from_city, to_city] = 1\n",
    "        \n",
    "        self.model.fit(\n",
    "            X_train,\n",
    "            y_masks,\n",
    "            epochs=epochs,\n",
    "            batch_size=batch_size,\n",
    "            validation_split=0.1\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a65f130e-800f-4d72-87da-b3100843220a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:1', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "print(tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e55d57fd-4da0-4a2b-ae67-8b99ecd3a419",
   "metadata": {},
   "source": [
    "### Подготовка исходных данных "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d69cb6ae-d699-4036-942e-5abee4a31725",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "X = np.load('X_20x20_fixed.npy')\n",
    "Y = np.load('Y_20x20_fixed.npy')\n",
    "border = 60000\n",
    "X_train = X[:border]\n",
    "Y_train = Y[:border]\n",
    "X_test = X[border:]\n",
    "Y_test = Y[border:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2d54ab64-1ceb-41d2-8d10-85f4c7ac797c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize the dataset\n",
    "#X_train = X_train/X_train.max(axis=(1,2))[:,np.newaxis,np.newaxis]\n",
    "#X_test = X_test/X_test.max(axis=(1,2))[:,np.newaxis,np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4df6d68d-af3b-4bb4-ae30-f03fe064820a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 60000)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N = X_train.shape[1]\n",
    "nlen = N*N\n",
    "cnt = X_train.shape[0]\n",
    "rand = np.random.RandomState(1)\n",
    "N, cnt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8819257c-10db-429e-8256-d6a0f0a12f14",
   "metadata": {},
   "source": [
    "### Инициализируем и обучаем модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "203e797d-0656-447f-8232-51506e904c6b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "18/18 [==============================] - 1s 35ms/step - loss: 3.7918e-06 - val_loss: 2.1203e-06\n",
      "Epoch 2/200\n",
      "18/18 [==============================] - 0s 25ms/step - loss: 1.9559e-06 - val_loss: 1.7618e-06\n",
      "Epoch 3/200\n",
      "18/18 [==============================] - 0s 25ms/step - loss: 1.7136e-06 - val_loss: 1.6494e-06\n",
      "Epoch 4/200\n",
      "18/18 [==============================] - 0s 25ms/step - loss: 1.6129e-06 - val_loss: 1.5726e-06\n",
      "Epoch 5/200\n",
      "18/18 [==============================] - 0s 25ms/step - loss: 1.5427e-06 - val_loss: 1.5100e-06\n",
      "Epoch 6/200\n",
      "18/18 [==============================] - 0s 25ms/step - loss: 1.4840e-06 - val_loss: 1.4558e-06\n",
      "Epoch 7/200\n",
      "18/18 [==============================] - 0s 25ms/step - loss: 1.4323e-06 - val_loss: 1.4070e-06\n",
      "Epoch 8/200\n",
      "18/18 [==============================] - 0s 25ms/step - loss: 1.3858e-06 - val_loss: 1.3632e-06\n",
      "Epoch 9/200\n",
      "18/18 [==============================] - 0s 25ms/step - loss: 1.3435e-06 - val_loss: 1.3234e-06\n",
      "Epoch 10/200\n",
      "18/18 [==============================] - 0s 25ms/step - loss: 1.3053e-06 - val_loss: 1.2874e-06\n",
      "Epoch 11/200\n",
      "18/18 [==============================] - 0s 25ms/step - loss: 1.2709e-06 - val_loss: 1.2552e-06\n",
      "Epoch 12/200\n",
      "18/18 [==============================] - 0s 25ms/step - loss: 1.2402e-06 - val_loss: 1.2265e-06\n",
      "Epoch 13/200\n",
      "18/18 [==============================] - 0s 25ms/step - loss: 1.2131e-06 - val_loss: 1.2015e-06\n",
      "Epoch 14/200\n",
      "18/18 [==============================] - 0s 26ms/step - loss: 1.1896e-06 - val_loss: 1.1801e-06\n",
      "Epoch 15/200\n",
      "18/18 [==============================] - 0s 25ms/step - loss: 1.1697e-06 - val_loss: 1.1620e-06\n",
      "Epoch 16/200\n",
      "18/18 [==============================] - 0s 25ms/step - loss: 1.1530e-06 - val_loss: 1.1467e-06\n",
      "Epoch 17/200\n",
      "18/18 [==============================] - 0s 25ms/step - loss: 1.1392e-06 - val_loss: 1.1343e-06\n",
      "Epoch 18/200\n",
      "18/18 [==============================] - 0s 25ms/step - loss: 1.1277e-06 - val_loss: 1.1240e-06\n",
      "Epoch 19/200\n",
      "18/18 [==============================] - 0s 25ms/step - loss: 1.1182e-06 - val_loss: 1.1153e-06\n",
      "Epoch 20/200\n",
      "18/18 [==============================] - 0s 25ms/step - loss: 1.1101e-06 - val_loss: 1.1077e-06\n",
      "Epoch 21/200\n",
      "18/18 [==============================] - 0s 26ms/step - loss: 1.1031e-06 - val_loss: 1.1012e-06\n",
      "Epoch 22/200\n",
      "18/18 [==============================] - 0s 25ms/step - loss: 1.0969e-06 - val_loss: 1.0953e-06\n",
      "Epoch 23/200\n",
      "18/18 [==============================] - 0s 25ms/step - loss: 1.0914e-06 - val_loss: 1.0899e-06\n",
      "Epoch 24/200\n",
      "18/18 [==============================] - 0s 25ms/step - loss: 1.0864e-06 - val_loss: 1.0851e-06\n",
      "Epoch 25/200\n",
      "18/18 [==============================] - 0s 25ms/step - loss: 1.0817e-06 - val_loss: 1.0806e-06\n",
      "Epoch 26/200\n",
      "18/18 [==============================] - 0s 25ms/step - loss: 1.0774e-06 - val_loss: 1.0764e-06\n",
      "Epoch 27/200\n",
      "18/18 [==============================] - 0s 25ms/step - loss: 1.0733e-06 - val_loss: 1.0725e-06\n",
      "Epoch 28/200\n",
      "18/18 [==============================] - 0s 25ms/step - loss: 1.0696e-06 - val_loss: 1.0688e-06\n",
      "Epoch 29/200\n",
      "18/18 [==============================] - 0s 25ms/step - loss: 1.0660e-06 - val_loss: 1.0651e-06\n",
      "Epoch 30/200\n",
      "18/18 [==============================] - 0s 25ms/step - loss: 1.0626e-06 - val_loss: 1.0618e-06\n",
      "Epoch 31/200\n",
      "18/18 [==============================] - 0s 25ms/step - loss: 1.0592e-06 - val_loss: 1.0585e-06\n",
      "Epoch 32/200\n",
      "18/18 [==============================] - 0s 25ms/step - loss: 1.0560e-06 - val_loss: 1.0553e-06\n",
      "Epoch 33/200\n",
      "18/18 [==============================] - 0s 25ms/step - loss: 1.0527e-06 - val_loss: 1.0521e-06\n",
      "Epoch 34/200\n",
      "18/18 [==============================] - 0s 25ms/step - loss: 1.0496e-06 - val_loss: 1.0490e-06\n",
      "Epoch 35/200\n",
      "18/18 [==============================] - 0s 25ms/step - loss: 1.0464e-06 - val_loss: 1.0459e-06\n",
      "Epoch 36/200\n",
      "18/18 [==============================] - 0s 25ms/step - loss: 1.0434e-06 - val_loss: 1.0428e-06\n",
      "Epoch 37/200\n",
      "18/18 [==============================] - 0s 25ms/step - loss: 1.0403e-06 - val_loss: 1.0398e-06\n",
      "Epoch 38/200\n",
      "18/18 [==============================] - 0s 25ms/step - loss: 1.0373e-06 - val_loss: 1.0369e-06\n",
      "Epoch 39/200\n",
      "18/18 [==============================] - 0s 25ms/step - loss: 1.0343e-06 - val_loss: 1.0339e-06\n",
      "Epoch 40/200\n",
      "18/18 [==============================] - 0s 25ms/step - loss: 1.0313e-06 - val_loss: 1.0308e-06\n",
      "Epoch 41/200\n",
      "18/18 [==============================] - 0s 25ms/step - loss: 1.0282e-06 - val_loss: 1.0277e-06\n",
      "Epoch 42/200\n",
      "18/18 [==============================] - 0s 25ms/step - loss: 1.0251e-06 - val_loss: 1.0246e-06\n",
      "Epoch 43/200\n",
      "18/18 [==============================] - 0s 25ms/step - loss: 1.0219e-06 - val_loss: 1.0213e-06\n",
      "Epoch 44/200\n",
      "18/18 [==============================] - 0s 25ms/step - loss: 1.0187e-06 - val_loss: 1.0180e-06\n",
      "Epoch 45/200\n",
      "18/18 [==============================] - 0s 25ms/step - loss: 1.0154e-06 - val_loss: 1.0147e-06\n",
      "Epoch 46/200\n",
      "18/18 [==============================] - 0s 25ms/step - loss: 1.0121e-06 - val_loss: 1.0112e-06\n",
      "Epoch 47/200\n",
      "18/18 [==============================] - 0s 25ms/step - loss: 1.0086e-06 - val_loss: 1.0077e-06\n",
      "Epoch 48/200\n",
      "18/18 [==============================] - 0s 25ms/step - loss: 1.0050e-06 - val_loss: 1.0041e-06\n",
      "Epoch 49/200\n",
      "18/18 [==============================] - 0s 25ms/step - loss: 1.0014e-06 - val_loss: 1.0003e-06\n",
      "Epoch 50/200\n",
      "18/18 [==============================] - 0s 25ms/step - loss: 9.9764e-07 - val_loss: 9.9650e-07\n",
      "Epoch 51/200\n",
      "18/18 [==============================] - 0s 25ms/step - loss: 9.9373e-07 - val_loss: 9.9246e-07\n",
      "Epoch 52/200\n",
      "18/18 [==============================] - 0s 25ms/step - loss: 9.8973e-07 - val_loss: 9.8832e-07\n",
      "Epoch 53/200\n",
      "18/18 [==============================] - 0s 25ms/step - loss: 9.8548e-07 - val_loss: 9.8412e-07\n",
      "Epoch 54/200\n",
      "18/18 [==============================] - 0s 25ms/step - loss: 9.8117e-07 - val_loss: 9.7973e-07\n",
      "Epoch 55/200\n",
      "18/18 [==============================] - 0s 25ms/step - loss: 9.7674e-07 - val_loss: 9.7522e-07\n",
      "Epoch 56/200\n",
      "18/18 [==============================] - 0s 25ms/step - loss: 9.7218e-07 - val_loss: 9.7050e-07\n",
      "Epoch 57/200\n",
      "18/18 [==============================] - 0s 25ms/step - loss: 9.6739e-07 - val_loss: 9.6566e-07\n",
      "Epoch 58/200\n",
      "18/18 [==============================] - 0s 25ms/step - loss: 9.6242e-07 - val_loss: 9.6061e-07\n",
      "Epoch 59/200\n",
      "18/18 [==============================] - 0s 25ms/step - loss: 9.5732e-07 - val_loss: 9.5535e-07\n",
      "Epoch 60/200\n",
      "18/18 [==============================] - 0s 25ms/step - loss: 9.5196e-07 - val_loss: 9.4988e-07\n",
      "Epoch 61/200\n",
      "18/18 [==============================] - 0s 25ms/step - loss: 9.4641e-07 - val_loss: 9.4418e-07\n",
      "Epoch 62/200\n",
      "18/18 [==============================] - 0s 25ms/step - loss: 9.4063e-07 - val_loss: 9.3830e-07\n",
      "Epoch 63/200\n",
      "18/18 [==============================] - 0s 25ms/step - loss: 9.3464e-07 - val_loss: 9.3222e-07\n",
      "Epoch 64/200\n",
      "18/18 [==============================] - 0s 25ms/step - loss: 9.2841e-07 - val_loss: 9.2594e-07\n",
      "Epoch 65/200\n",
      "18/18 [==============================] - 0s 25ms/step - loss: 9.2199e-07 - val_loss: 9.1942e-07\n",
      "Epoch 66/200\n",
      "18/18 [==============================] - 0s 25ms/step - loss: 9.1529e-07 - val_loss: 9.1264e-07\n",
      "Epoch 67/200\n",
      "18/18 [==============================] - 0s 25ms/step - loss: 9.0846e-07 - val_loss: 9.0549e-07\n",
      "Epoch 68/200\n",
      "18/18 [==============================] - 0s 25ms/step - loss: 9.0132e-07 - val_loss: 8.9825e-07\n",
      "Epoch 69/200\n",
      "18/18 [==============================] - 0s 25ms/step - loss: 8.9395e-07 - val_loss: 8.9077e-07\n",
      "Epoch 70/200\n",
      "18/18 [==============================] - 0s 25ms/step - loss: 8.8645e-07 - val_loss: 8.8316e-07\n",
      "Epoch 71/200\n",
      "18/18 [==============================] - 0s 25ms/step - loss: 8.7883e-07 - val_loss: 8.7554e-07\n",
      "Epoch 72/200\n",
      "18/18 [==============================] - 0s 25ms/step - loss: 8.7115e-07 - val_loss: 8.6786e-07\n",
      "Epoch 73/200\n",
      "18/18 [==============================] - 0s 25ms/step - loss: 8.6338e-07 - val_loss: 8.5999e-07\n",
      "Epoch 74/200\n",
      "18/18 [==============================] - 0s 25ms/step - loss: 8.5552e-07 - val_loss: 8.5201e-07\n",
      "Epoch 75/200\n",
      "18/18 [==============================] - 0s 25ms/step - loss: 8.4760e-07 - val_loss: 8.4413e-07\n",
      "Epoch 76/200\n",
      "18/18 [==============================] - 0s 25ms/step - loss: 8.3973e-07 - val_loss: 8.3617e-07\n",
      "Epoch 77/200\n",
      "18/18 [==============================] - 0s 26ms/step - loss: 8.3193e-07 - val_loss: 8.2838e-07\n",
      "Epoch 78/200\n",
      "18/18 [==============================] - 0s 25ms/step - loss: 8.2411e-07 - val_loss: 8.2057e-07\n",
      "Epoch 79/200\n",
      "18/18 [==============================] - 0s 25ms/step - loss: 8.1635e-07 - val_loss: 8.1274e-07\n",
      "Epoch 80/200\n",
      "18/18 [==============================] - 0s 25ms/step - loss: 8.0864e-07 - val_loss: 8.0504e-07\n",
      "Epoch 81/200\n",
      "18/18 [==============================] - 0s 25ms/step - loss: 8.0097e-07 - val_loss: 7.9723e-07\n",
      "Epoch 82/200\n",
      "18/18 [==============================] - 0s 25ms/step - loss: 7.9324e-07 - val_loss: 7.8964e-07\n",
      "Epoch 83/200\n",
      "18/18 [==============================] - 0s 25ms/step - loss: 7.8557e-07 - val_loss: 7.8195e-07\n",
      "Epoch 84/200\n",
      "18/18 [==============================] - 0s 25ms/step - loss: 7.7788e-07 - val_loss: 7.7422e-07\n",
      "Epoch 85/200\n",
      "18/18 [==============================] - 0s 25ms/step - loss: 7.7034e-07 - val_loss: 7.6675e-07\n",
      "Epoch 86/200\n",
      "18/18 [==============================] - 0s 25ms/step - loss: 7.6281e-07 - val_loss: 7.5930e-07\n",
      "Epoch 87/200\n",
      "18/18 [==============================] - 0s 25ms/step - loss: 7.5540e-07 - val_loss: 7.5186e-07\n",
      "Epoch 88/200\n",
      "18/18 [==============================] - 0s 25ms/step - loss: 7.4800e-07 - val_loss: 7.4456e-07\n",
      "Epoch 89/200\n",
      "18/18 [==============================] - 0s 25ms/step - loss: 7.4078e-07 - val_loss: 7.3739e-07\n",
      "Epoch 90/200\n",
      "18/18 [==============================] - 0s 26ms/step - loss: 7.3360e-07 - val_loss: 7.3021e-07\n",
      "Epoch 91/200\n",
      "18/18 [==============================] - 0s 25ms/step - loss: 7.2649e-07 - val_loss: 7.2329e-07\n",
      "Epoch 92/200\n",
      "18/18 [==============================] - 0s 25ms/step - loss: 7.1956e-07 - val_loss: 7.1645e-07\n",
      "Epoch 93/200\n",
      "18/18 [==============================] - 0s 25ms/step - loss: 7.1271e-07 - val_loss: 7.0984e-07\n",
      "Epoch 94/200\n",
      "18/18 [==============================] - 0s 25ms/step - loss: 7.0598e-07 - val_loss: 7.0318e-07\n",
      "Epoch 95/200\n",
      "18/18 [==============================] - 0s 25ms/step - loss: 6.9941e-07 - val_loss: 6.9652e-07\n",
      "Epoch 96/200\n",
      "18/18 [==============================] - 0s 25ms/step - loss: 6.9299e-07 - val_loss: 6.9016e-07\n",
      "Epoch 97/200\n",
      "18/18 [==============================] - 0s 25ms/step - loss: 6.8662e-07 - val_loss: 6.8407e-07\n",
      "Epoch 98/200\n",
      "18/18 [==============================] - 0s 25ms/step - loss: 6.8047e-07 - val_loss: 6.7788e-07\n",
      "Epoch 99/200\n",
      "18/18 [==============================] - 0s 25ms/step - loss: 6.7436e-07 - val_loss: 6.7187e-07\n",
      "Epoch 100/200\n",
      "18/18 [==============================] - 0s 25ms/step - loss: 6.6848e-07 - val_loss: 6.6598e-07\n",
      "Epoch 101/200\n",
      "18/18 [==============================] - 0s 25ms/step - loss: 6.6259e-07 - val_loss: 6.6022e-07\n",
      "Epoch 102/200\n",
      "18/18 [==============================] - 0s 25ms/step - loss: 6.5691e-07 - val_loss: 6.5475e-07\n",
      "Epoch 103/200\n",
      "18/18 [==============================] - 0s 25ms/step - loss: 6.5136e-07 - val_loss: 6.4911e-07\n",
      "Epoch 104/200\n",
      "18/18 [==============================] - 0s 25ms/step - loss: 6.4597e-07 - val_loss: 6.4416e-07\n",
      "Epoch 105/200\n",
      "18/18 [==============================] - 0s 25ms/step - loss: 6.4081e-07 - val_loss: 6.3883e-07\n",
      "Epoch 106/200\n",
      "18/18 [==============================] - 0s 25ms/step - loss: 6.3565e-07 - val_loss: 6.3387e-07\n",
      "Epoch 107/200\n",
      "18/18 [==============================] - 0s 25ms/step - loss: 6.3065e-07 - val_loss: 6.2927e-07\n",
      "Epoch 108/200\n",
      "18/18 [==============================] - 0s 25ms/step - loss: 6.2577e-07 - val_loss: 6.2434e-07\n",
      "Epoch 109/200\n",
      "18/18 [==============================] - 0s 25ms/step - loss: 6.2102e-07 - val_loss: 6.1966e-07\n",
      "Epoch 110/200\n",
      "18/18 [==============================] - 0s 25ms/step - loss: 6.1643e-07 - val_loss: 6.1506e-07\n",
      "Epoch 111/200\n",
      "18/18 [==============================] - 0s 25ms/step - loss: 6.1196e-07 - val_loss: 6.1097e-07\n",
      "Epoch 112/200\n",
      "18/18 [==============================] - 0s 25ms/step - loss: 6.0763e-07 - val_loss: 6.0660e-07\n",
      "Epoch 113/200\n",
      "18/18 [==============================] - 0s 25ms/step - loss: 6.0340e-07 - val_loss: 6.0230e-07\n",
      "Epoch 114/200\n",
      "18/18 [==============================] - 0s 25ms/step - loss: 5.9924e-07 - val_loss: 5.9846e-07\n",
      "Epoch 115/200\n",
      "18/18 [==============================] - 0s 25ms/step - loss: 5.9526e-07 - val_loss: 5.9453e-07\n",
      "Epoch 116/200\n",
      "18/18 [==============================] - 0s 25ms/step - loss: 5.9133e-07 - val_loss: 5.9064e-07\n",
      "Epoch 117/200\n",
      "18/18 [==============================] - 0s 25ms/step - loss: 5.8764e-07 - val_loss: 5.8728e-07\n",
      "Epoch 118/200\n",
      "18/18 [==============================] - 0s 25ms/step - loss: 5.8383e-07 - val_loss: 5.8359e-07\n",
      "Epoch 119/200\n",
      "18/18 [==============================] - 0s 25ms/step - loss: 5.8034e-07 - val_loss: 5.8033e-07\n",
      "Epoch 120/200\n",
      "18/18 [==============================] - 0s 25ms/step - loss: 5.7692e-07 - val_loss: 5.7681e-07\n",
      "Epoch 121/200\n",
      "18/18 [==============================] - 0s 25ms/step - loss: 5.7354e-07 - val_loss: 5.7362e-07\n",
      "Epoch 122/200\n",
      "18/18 [==============================] - 0s 25ms/step - loss: 5.7036e-07 - val_loss: 5.7057e-07\n",
      "Epoch 123/200\n",
      "18/18 [==============================] - 0s 25ms/step - loss: 5.6716e-07 - val_loss: 5.6754e-07\n",
      "Epoch 124/200\n",
      "18/18 [==============================] - 0s 25ms/step - loss: 5.6410e-07 - val_loss: 5.6444e-07\n",
      "Epoch 125/200\n",
      "18/18 [==============================] - 0s 25ms/step - loss: 5.6116e-07 - val_loss: 5.6187e-07\n",
      "Epoch 126/200\n",
      "18/18 [==============================] - 0s 25ms/step - loss: 5.5836e-07 - val_loss: 5.5895e-07\n",
      "Epoch 127/200\n",
      "18/18 [==============================] - 0s 28ms/step - loss: 5.5553e-07 - val_loss: 5.5617e-07\n",
      "Epoch 128/200\n",
      "18/18 [==============================] - 0s 25ms/step - loss: 5.5289e-07 - val_loss: 5.5363e-07\n",
      "Epoch 129/200\n",
      "18/18 [==============================] - 0s 25ms/step - loss: 5.5024e-07 - val_loss: 5.5104e-07\n",
      "Epoch 130/200\n",
      "18/18 [==============================] - 0s 25ms/step - loss: 5.4770e-07 - val_loss: 5.4880e-07\n",
      "Epoch 131/200\n",
      "18/18 [==============================] - 0s 25ms/step - loss: 5.4523e-07 - val_loss: 5.4625e-07\n",
      "Epoch 132/200\n",
      "18/18 [==============================] - 0s 25ms/step - loss: 5.4287e-07 - val_loss: 5.4402e-07\n",
      "Epoch 133/200\n",
      "18/18 [==============================] - 0s 25ms/step - loss: 5.4049e-07 - val_loss: 5.4186e-07\n",
      "Epoch 134/200\n",
      "18/18 [==============================] - 0s 25ms/step - loss: 5.3838e-07 - val_loss: 5.3949e-07\n",
      "Epoch 135/200\n",
      "18/18 [==============================] - 0s 25ms/step - loss: 5.3602e-07 - val_loss: 5.3752e-07\n",
      "Epoch 136/200\n",
      "18/18 [==============================] - 0s 25ms/step - loss: 5.3402e-07 - val_loss: 5.3529e-07\n",
      "Epoch 137/200\n",
      "18/18 [==============================] - 0s 25ms/step - loss: 5.3184e-07 - val_loss: 5.3331e-07\n",
      "Epoch 138/200\n",
      "18/18 [==============================] - 0s 25ms/step - loss: 5.2997e-07 - val_loss: 5.3131e-07\n",
      "Epoch 139/200\n",
      "18/18 [==============================] - 0s 25ms/step - loss: 5.2800e-07 - val_loss: 5.2945e-07\n",
      "Epoch 140/200\n",
      "18/18 [==============================] - 0s 25ms/step - loss: 5.2607e-07 - val_loss: 5.2778e-07\n",
      "Epoch 141/200\n",
      "18/18 [==============================] - 0s 25ms/step - loss: 5.2438e-07 - val_loss: 5.2596e-07\n",
      "Epoch 142/200\n",
      "18/18 [==============================] - 0s 25ms/step - loss: 5.2246e-07 - val_loss: 5.2409e-07\n",
      "Epoch 143/200\n",
      "18/18 [==============================] - 0s 25ms/step - loss: 5.2074e-07 - val_loss: 5.2253e-07\n",
      "Epoch 144/200\n",
      "18/18 [==============================] - 0s 25ms/step - loss: 5.1900e-07 - val_loss: 5.2096e-07\n",
      "Epoch 145/200\n",
      "18/18 [==============================] - 0s 25ms/step - loss: 5.1746e-07 - val_loss: 5.1931e-07\n",
      "Epoch 146/200\n",
      "18/18 [==============================] - 0s 25ms/step - loss: 5.1589e-07 - val_loss: 5.1794e-07\n",
      "Epoch 147/200\n",
      "18/18 [==============================] - 0s 25ms/step - loss: 5.1425e-07 - val_loss: 5.1630e-07\n",
      "Epoch 148/200\n",
      "18/18 [==============================] - 0s 25ms/step - loss: 5.1285e-07 - val_loss: 5.1480e-07\n",
      "Epoch 149/200\n",
      "18/18 [==============================] - 0s 25ms/step - loss: 5.1128e-07 - val_loss: 5.1333e-07\n",
      "Epoch 150/200\n",
      "18/18 [==============================] - 0s 25ms/step - loss: 5.0980e-07 - val_loss: 5.1198e-07\n",
      "Epoch 151/200\n",
      "18/18 [==============================] - 0s 25ms/step - loss: 5.0840e-07 - val_loss: 5.1035e-07\n",
      "Epoch 152/200\n",
      "18/18 [==============================] - 0s 25ms/step - loss: 5.0704e-07 - val_loss: 5.0929e-07\n",
      "Epoch 153/200\n",
      "18/18 [==============================] - 0s 25ms/step - loss: 5.0579e-07 - val_loss: 5.0819e-07\n",
      "Epoch 154/200\n",
      "18/18 [==============================] - 0s 25ms/step - loss: 5.0446e-07 - val_loss: 5.0664e-07\n",
      "Epoch 155/200\n",
      "18/18 [==============================] - 0s 25ms/step - loss: 5.0315e-07 - val_loss: 5.0549e-07\n",
      "Epoch 156/200\n",
      "18/18 [==============================] - 0s 25ms/step - loss: 5.0198e-07 - val_loss: 5.0435e-07\n",
      "Epoch 157/200\n",
      "18/18 [==============================] - 0s 25ms/step - loss: 5.0073e-07 - val_loss: 5.0351e-07\n",
      "Epoch 158/200\n",
      "18/18 [==============================] - 0s 25ms/step - loss: 4.9970e-07 - val_loss: 5.0237e-07\n",
      "Epoch 159/200\n",
      "18/18 [==============================] - 0s 25ms/step - loss: 4.9866e-07 - val_loss: 5.0117e-07\n",
      "Epoch 160/200\n",
      "18/18 [==============================] - 0s 25ms/step - loss: 4.9745e-07 - val_loss: 4.9998e-07\n",
      "Epoch 161/200\n",
      "18/18 [==============================] - 0s 25ms/step - loss: 4.9635e-07 - val_loss: 4.9899e-07\n",
      "Epoch 162/200\n",
      "18/18 [==============================] - 0s 25ms/step - loss: 4.9530e-07 - val_loss: 4.9775e-07\n",
      "Epoch 163/200\n",
      "18/18 [==============================] - 0s 25ms/step - loss: 4.9419e-07 - val_loss: 4.9675e-07\n",
      "Epoch 164/200\n",
      "18/18 [==============================] - 0s 25ms/step - loss: 4.9323e-07 - val_loss: 4.9596e-07\n",
      "Epoch 165/200\n",
      "18/18 [==============================] - 0s 25ms/step - loss: 4.9236e-07 - val_loss: 4.9515e-07\n",
      "Epoch 166/200\n",
      "18/18 [==============================] - 0s 25ms/step - loss: 4.9133e-07 - val_loss: 4.9405e-07\n",
      "Epoch 167/200\n",
      "18/18 [==============================] - 0s 25ms/step - loss: 4.9043e-07 - val_loss: 4.9325e-07\n",
      "Epoch 168/200\n",
      "18/18 [==============================] - 0s 25ms/step - loss: 4.8952e-07 - val_loss: 4.9224e-07\n",
      "Epoch 169/200\n",
      "18/18 [==============================] - 0s 25ms/step - loss: 4.8855e-07 - val_loss: 4.9135e-07\n",
      "Epoch 170/200\n",
      "18/18 [==============================] - 0s 25ms/step - loss: 4.8780e-07 - val_loss: 4.9058e-07\n",
      "Epoch 171/200\n",
      "18/18 [==============================] - 0s 25ms/step - loss: 4.8691e-07 - val_loss: 4.8963e-07\n",
      "Epoch 172/200\n",
      "18/18 [==============================] - 0s 25ms/step - loss: 4.8600e-07 - val_loss: 4.8901e-07\n",
      "Epoch 173/200\n",
      "18/18 [==============================] - 0s 25ms/step - loss: 4.8533e-07 - val_loss: 4.8796e-07\n",
      "Epoch 174/200\n",
      "18/18 [==============================] - 0s 25ms/step - loss: 4.8446e-07 - val_loss: 4.8727e-07\n",
      "Epoch 175/200\n",
      "18/18 [==============================] - 0s 25ms/step - loss: 4.8355e-07 - val_loss: 4.8649e-07\n",
      "Epoch 176/200\n",
      "18/18 [==============================] - 0s 25ms/step - loss: 4.8299e-07 - val_loss: 4.8588e-07\n",
      "Epoch 177/200\n",
      "18/18 [==============================] - 0s 25ms/step - loss: 4.8216e-07 - val_loss: 4.8507e-07\n",
      "Epoch 178/200\n",
      "18/18 [==============================] - 0s 25ms/step - loss: 4.8132e-07 - val_loss: 4.8433e-07\n",
      "Epoch 179/200\n",
      "18/18 [==============================] - 0s 25ms/step - loss: 4.8063e-07 - val_loss: 4.8396e-07\n",
      "Epoch 180/200\n",
      "18/18 [==============================] - 0s 25ms/step - loss: 4.7990e-07 - val_loss: 4.8297e-07\n",
      "Epoch 181/200\n",
      "18/18 [==============================] - 0s 25ms/step - loss: 4.7929e-07 - val_loss: 4.8230e-07\n",
      "Epoch 182/200\n",
      "18/18 [==============================] - 0s 25ms/step - loss: 4.7862e-07 - val_loss: 4.8177e-07\n",
      "Epoch 183/200\n",
      "18/18 [==============================] - 0s 25ms/step - loss: 4.7807e-07 - val_loss: 4.8142e-07\n",
      "Epoch 184/200\n",
      "18/18 [==============================] - 0s 25ms/step - loss: 4.7747e-07 - val_loss: 4.8061e-07\n",
      "Epoch 185/200\n",
      "18/18 [==============================] - 0s 25ms/step - loss: 4.7670e-07 - val_loss: 4.7970e-07\n",
      "Epoch 186/200\n",
      "18/18 [==============================] - 0s 25ms/step - loss: 4.7612e-07 - val_loss: 4.7917e-07\n",
      "Epoch 187/200\n",
      "18/18 [==============================] - 0s 25ms/step - loss: 4.7550e-07 - val_loss: 4.7867e-07\n",
      "Epoch 188/200\n",
      "18/18 [==============================] - 0s 25ms/step - loss: 4.7480e-07 - val_loss: 4.7803e-07\n",
      "Epoch 189/200\n",
      "18/18 [==============================] - 0s 25ms/step - loss: 4.7420e-07 - val_loss: 4.7721e-07\n",
      "Epoch 190/200\n",
      "18/18 [==============================] - 0s 25ms/step - loss: 4.7361e-07 - val_loss: 4.7698e-07\n",
      "Epoch 191/200\n",
      "18/18 [==============================] - 0s 25ms/step - loss: 4.7304e-07 - val_loss: 4.7659e-07\n",
      "Epoch 192/200\n",
      "18/18 [==============================] - 0s 25ms/step - loss: 4.7265e-07 - val_loss: 4.7679e-07\n",
      "Epoch 193/200\n",
      "18/18 [==============================] - 0s 25ms/step - loss: 4.7218e-07 - val_loss: 4.7538e-07\n",
      "Epoch 194/200\n",
      "18/18 [==============================] - 0s 25ms/step - loss: 4.7143e-07 - val_loss: 4.7463e-07\n",
      "Epoch 195/200\n",
      "18/18 [==============================] - 0s 25ms/step - loss: 4.7085e-07 - val_loss: 4.7433e-07\n",
      "Epoch 196/200\n",
      "18/18 [==============================] - 0s 25ms/step - loss: 4.7048e-07 - val_loss: 4.7412e-07\n",
      "Epoch 197/200\n",
      "18/18 [==============================] - 0s 25ms/step - loss: 4.6985e-07 - val_loss: 4.7315e-07\n",
      "Epoch 198/200\n",
      "18/18 [==============================] - 0s 25ms/step - loss: 4.6927e-07 - val_loss: 4.7276e-07\n",
      "Epoch 199/200\n",
      "18/18 [==============================] - 0s 25ms/step - loss: 4.6885e-07 - val_loss: 4.7239e-07\n",
      "Epoch 200/200\n",
      "18/18 [==============================] - 0s 24ms/step - loss: 4.6825e-07 - val_loss: 4.7192e-07\n"
     ]
    }
   ],
   "source": [
    "solver = TSPSolver(num_cities=N, hidden_dim = 512)\n",
    "solver.train(X_train, Y_train, epochs=200, batch_size=3064)\n",
    "#solver.model.load_weights('./my.weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "62d163f6-4665-4bd4-8314-8f1adcff6ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# solver.model.save_weights('./my.weights.h5')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e54e75b-d252-47e7-85b9-12b19b07673a",
   "metadata": {},
   "source": [
    "### Проверка обученной модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1ace715c-70d6-4fdd-ae43-2c3161e2aa5a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def predict_route(solver, dist_matrix, is_rnd = False, num_iter = 500):\n",
    "    ap = solver.model(distance_matrix[np.newaxis, ...], training=False)[0].numpy()\n",
    "    num_cities = dist_matrix.shape[0]\n",
    "    best_route = []\n",
    "    best_dist = np.inf\n",
    "    for _ in range(num_iter):\n",
    "        current = 0\n",
    "        route = [current]\n",
    "        total_dist = 0\n",
    "        for _ in range(num_cities-1):\n",
    "            probs = ap[current].copy()\n",
    "            if is_rnd: probs = np.full_like(probs, 1) \n",
    "            # Маскируем посещенные города\n",
    "            probs[route] = 0\n",
    "            # Выбираем следующий город\n",
    "            next_city = rand.choice(range(num_cities), p=(probs / np.sum(probs)))\n",
    "            route.append(next_city)\n",
    "            total_dist += dist_matrix[current, next_city]\n",
    "            current = next_city\n",
    "        total_dist += dist_matrix[route[-1], route[0]]\n",
    "        if total_dist < best_dist:\n",
    "            best_dist = total_dist\n",
    "            best_route = route\n",
    "    return np.array(best_route), best_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ccbb483f-f33a-4802-aba9-618090ae7034",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def predict_geedy_route(dist_matrix):\n",
    "    num_cities = dist_matrix.shape[0]\n",
    "    current = 0\n",
    "    route = [current]\n",
    "    next_city = np.argmax(dist_matrix[current])\n",
    "    route.append(next_city)\n",
    "    total_dist = dist_matrix[current, next_city]\n",
    "    current = next_city\n",
    "    for _ in range(num_cities-2):\n",
    "        a = dist_matrix[current].copy()\n",
    "        # Маскируем посещенные города\n",
    "        a[route] = np.inf\n",
    "        # Выбираем следующий город\n",
    "        next_city = np.argmin(a)\n",
    "        route.append(next_city)\n",
    "        total_dist += dist_matrix[current, next_city]\n",
    "        current = next_city\n",
    "    total_dist += dist_matrix[route[-1], route[0]]\n",
    "    return np.array(route), total_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "35e3caef-5686-4b9a-ae45-64b4c032f4e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_beam_exp(solver, distance_matrix, beam_width=3):\n",
    "    ap = solver.model(distance_matrix[np.newaxis, ...], training=False)[0].numpy() # solver.model.predict(distance_matrix[np.newaxis, ...], verbose=0)[0]\n",
    "    num_cities = distance_matrix.shape[0]\n",
    "    beams = [([0], set(range(0, num_cities)) - {0}, 0, 0)]\n",
    "    delta = 1 / num_cities\n",
    "    w = 0.8 # (i+1) * delta\n",
    "    for i in range(1, num_cities):\n",
    "        new_beams = []\n",
    "        for route, remaining, dist, sap in beams:\n",
    "            # Топ-K городов\n",
    "            w = 0\n",
    "            top_cities = sorted(remaining, key=lambda x: w*distance_matrix[route[-1],x] +\n",
    "                                (1-w)*(1-ap[route[-1],x])\n",
    "                               )[:beam_width] #ap[route[-1],x], reverse=True\n",
    "            for city in top_cities:\n",
    "                new_route = route + [city]\n",
    "                new_remaining = remaining - {city}\n",
    "                new_dist = dist + distance_matrix[new_route[-2], new_route[-1]]\n",
    "                new_sap = sap + 1.0 - ap[new_route[-2], new_route[-1]]\n",
    "                if i == num_cities-1:\n",
    "                    new_dist += distance_matrix[new_route[-1], new_route[0]]\n",
    "                    new_sap += 1.0-ap[new_route[-1], new_route[0]]\n",
    "                new_beams.append((new_route, new_remaining, new_dist, new_sap))\n",
    "        # Выбираем лучшие beam_width вариантов\n",
    "        w = 0.8\n",
    "        beams = sorted(new_beams, key=lambda x: w*x[2] + (1-w)*x[3])[:beam_width]\n",
    "        \n",
    "    # Лучший маршрут\n",
    "    best_route, _, best_dist, _ = min(beams, key=lambda x: x[2])\n",
    "    return best_route, best_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "24b74467-17e5-44e2-8948-59f8a4c57e2f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3000/3000 [00:24<00:00, 121.17it/s]\n"
     ]
    }
   ],
   "source": [
    "ld = []\n",
    "lp = []\n",
    "lg = []\n",
    "lr = []\n",
    "lb = []\n",
    "lls = []\n",
    "lsa = []\n",
    "for i in tqdm(range(X_test.shape[0])):\n",
    "    a = X_test[i]\n",
    "    route = Y_test[i]\n",
    "    distance = sum(a[route[j],route[j+1]] for j in range(N-1))+a[route[-1],route[0]]\n",
    "    #_, total_dist = predict_route(solver, a, False, 500)\n",
    "    #route, dist = predict_geedy_route(a)\n",
    "    #route, rdist = predict_route(solver, a, True, 500)\n",
    "    _, bdist = predict_beam_exp(solver, a, beam_width=4)\n",
    "    #_, ls_dist = solve_tsp_local_search(a)\n",
    "    #_, sa_dist = solve_tsp_simulated_annealing(a)\n",
    "    ld.append(distance)\n",
    "    #lp.append(total_dist)\n",
    "    #lg.append(dist)\n",
    "    #lr.append(rdist)\n",
    "    lb.append(bdist)\n",
    "    #lls.append(ls_dist)\n",
    "    #lsa.append(sa_dist)\n",
    "    \n",
    "#Y_predict = np.array(lp)\n",
    "Y_true = np.array(ld)\n",
    "#Y_greedy = np.array(lg)\n",
    "#Y_rnd = np.array(lr)\n",
    "Y_beam = np.array(lb)\n",
    "#Y_ls = np.array(lls)\n",
    "#Y_sa = np.array(lsa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "71f20ce5-7641-4f84-8079-a407ea5a2fdb",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.07922057356307602"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#mean_absolute_percentage_error(Y_true, Y_predict), \\\n",
    "mean_absolute_percentage_error(Y_true, Y_beam) #, \\\n",
    "#mean_absolute_percentage_error(Y_true, Y_greedy), \\\n",
    "#mean_absolute_percentage_error(Y_true, Y_rnd), \\\n",
    "#mean_absolute_percentage_error(Y_true, Y_ls), \\\n",
    "#mean_absolute_percentage_error(Y_true, Y_sa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "25a2cd21-dc07-4901-96d9-a773d3272aa4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Сколько случаев действительно плохого прогноза\n",
    "#sum(((Y_predict - Y_true) / Y_true) > 0.2), \\\n",
    "sum(((Y_beam - Y_true) / Y_true) > 0.2) #, \\\n",
    "#sum(((Y_greedy - Y_true) / Y_true) > 0.2), \\\n",
    "#sum(((Y_rnd - Y_true) / Y_true) > 0.2), \\\n",
    "#sum(((Y_ls - Y_true) / Y_true) > 0.2), \\\n",
    "#sum(((Y_sa - Y_true) / Y_true) > 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a54a0660-06dd-458f-b00f-b480e1e73fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_gr = np.argsort(solver.model(X_test, training=False).numpy()[:,0,:], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ae1f0c01-0213-44e1-84cf-a2a5a193106e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9116666666666666"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum((Y_gr[:,-1] == Y_test[:,1]) | (Y_gr[:,-2] == Y_test[:,1]) | (Y_gr[:,-3] == Y_test[:,1]) | (Y_gr[:,-4] == Y_test[:,1]))/3000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "a19c42d3-6a57-41f4-952d-84d770f29da9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.62 ms ± 124 μs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit predict_beam_exp(solver, a, beam_width=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1c70e7e-8c11-464e-be47-2bfc51640c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit predict_geedy_route(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "58ba83b6-4aa1-402e-9bc2-69df8c20f20a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "358 ms ± 1.63 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit predict_route(solver, a, False, 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa8e82b1-0f8d-47d8-89fe-2817f73866af",
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit solve_tsp_dynamic_programming(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "89c149d9-dbb1-4f73-a667-1138709614af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "888 ms ± 5.48 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit solve_tsp_branch_and_bound(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "1107a9b3-c5aa-4226-b2c8-61b68f2948ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.02 ms ± 149 μs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit solve_tsp_local_search(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db205318-629b-40e8-9d10-47b8e5b81488",
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit solve_tsp_simulated_annealing(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "id": "d2f36e6a-0bf1-4054-8807-5417225975ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0, 13,  8,  9, 17,  2, 19, 14, 12, 10, 16,  5,  3,  1,  6, 18, 11,\n",
       "         4, 15,  7]),\n",
       " np.float64(6.568715342983737))"
      ]
     },
     "execution_count": 284,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_test[795], Y_true[795]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "id": "a4e3cb63-d9f5-443e-baed-b681b527c610",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0, 12, 10, 16, 15, 7, 19, 1, 6, 18, 5, 3, 13, 8, 9, 14, 4, 2, 17, 11],\n",
       " np.float64(7.588053043444236))"
      ]
     },
     "execution_count": 283,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_beam_exp(solver, X_test[795], beam_width=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "id": "199d94ec-2174-4cb4-9b6f-c32d49eec0ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0     1     2     3     4     5     6     7     8     9     10    11  \\\n",
       "0   0.0  0.00  0.01  0.00  0.03  0.01  0.00  0.05  0.01  0.02  0.25  0.21   \n",
       "1   0.0  0.00  0.08  0.05  0.10  0.01  0.35  0.00  0.12  0.00  0.00  0.04   \n",
       "2   0.0  0.01  0.01  0.01  0.06  0.00  0.02  0.08  0.02  0.01  0.00  0.00   \n",
       "3   0.0  0.27  0.06  0.01  0.00  0.00  0.01  0.03  0.15  0.00  0.00  0.08   \n",
       "4   0.0  0.00  0.06  0.01  0.04  0.05  0.00  0.01  0.16  0.30  0.00  0.00   \n",
       "5   0.0  0.00  0.00  0.36  0.03  0.00  0.05  0.01  0.05  0.00  0.00  0.00   \n",
       "6   0.0  0.01  0.00  0.00  0.05  0.17  0.00  0.05  0.00  0.00  0.01  0.09   \n",
       "7   0.0  0.02  0.02  0.06  0.04  0.02  0.00  0.00  0.07  0.00  0.25  0.00   \n",
       "8   0.0  0.00  0.00  0.28  0.00  0.13  0.03  0.00  0.00  0.13  0.04  0.00   \n",
       "9   0.0  0.17  0.03  0.00  0.00  0.03  0.16  0.00  0.01  0.00  0.03  0.01   \n",
       "10  0.0  0.02  0.00  0.00  0.07  0.00  0.00  0.28  0.02  0.01  0.03  0.00   \n",
       "11  0.0  0.16  0.02  0.05  0.14  0.00  0.01  0.26  0.01  0.12  0.00  0.00   \n",
       "12  0.0  0.00  0.00  0.01  0.25  0.00  0.00  0.11  0.00  0.00  0.15  0.00   \n",
       "13  0.0  0.01  0.15  0.03  0.03  0.00  0.00  0.00  0.29  0.00  0.05  0.00   \n",
       "14  0.0  0.00  0.03  0.00  0.14  0.00  0.09  0.01  0.01  0.00  0.02  0.00   \n",
       "15  0.0  0.10  0.01  0.01  0.00  0.04  0.00  0.55  0.00  0.00  0.00  0.00   \n",
       "16  0.0  0.18  0.00  0.00  0.02  0.24  0.01  0.05  0.00  0.04  0.08  0.09   \n",
       "17  0.0  0.08  0.10  0.00  0.00  0.02  0.14  0.01  0.00  0.00  0.09  0.09   \n",
       "18  0.0  0.00  0.00  0.00  0.11  0.14  0.00  0.00  0.25  0.00  0.14  0.27   \n",
       "19  0.0  0.32  0.00  0.00  0.13  0.02  0.16  0.00  0.00  0.03  0.00  0.03   \n",
       "\n",
       "      12    13    14    15    16    17    18    19  \n",
       "0   0.12  0.22  0.00  0.00  0.00  0.00  0.00  0.06  \n",
       "1   0.00  0.00  0.00  0.00  0.17  0.06  0.00  0.00  \n",
       "2   0.00  0.01  0.00  0.00  0.03  0.04  0.00  0.72  \n",
       "3   0.00  0.25  0.00  0.00  0.01  0.00  0.06  0.05  \n",
       "4   0.00  0.00  0.00  0.34  0.00  0.01  0.03  0.00  \n",
       "5   0.42  0.00  0.01  0.02  0.01  0.02  0.03  0.00  \n",
       "6   0.00  0.01  0.12  0.00  0.07  0.00  0.42  0.00  \n",
       "7   0.02  0.00  0.00  0.04  0.05  0.00  0.00  0.40  \n",
       "8   0.09  0.16  0.00  0.00  0.08  0.04  0.01  0.01  \n",
       "9   0.00  0.00  0.27  0.05  0.07  0.03  0.02  0.10  \n",
       "10  0.01  0.02  0.00  0.17  0.18  0.02  0.16  0.01  \n",
       "11  0.02  0.03  0.00  0.11  0.01  0.01  0.01  0.04  \n",
       "12  0.06  0.00  0.00  0.11  0.01  0.01  0.00  0.29  \n",
       "13  0.20  0.00  0.06  0.11  0.01  0.01  0.00  0.01  \n",
       "14  0.42  0.00  0.00  0.00  0.01  0.01  0.00  0.25  \n",
       "15  0.14  0.09  0.00  0.00  0.00  0.01  0.02  0.00  \n",
       "16  0.03  0.00  0.00  0.25  0.00  0.00  0.00  0.00  \n",
       "17  0.02  0.00  0.02  0.01  0.00  0.00  0.39  0.02  \n",
       "18  0.00  0.00  0.00  0.08  0.00  0.00  0.00  0.00  \n",
       "19  0.01  0.00  0.18  0.00  0.00  0.00  0.11  0.00  "
      ]
     },
     "execution_count": 294,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = X_test[795]\n",
    "ap = np.round(solver.model(a[np.newaxis, ...], training=False)[0].numpy(),2)\n",
    "route = Y_test[795]\n",
    "pd.DataFrame(ap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "id": "c865083c-341c-4ceb-abb3-36df5bf71432",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(3)"
      ]
     },
     "execution_count": 290,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(ap[0] > 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "id": "8a5a7033-67bf-4da9-b2df-64b94a603928",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(np.int64(0), np.int64(13), np.float32(0.22)),\n",
       " (np.int64(13), np.int64(8), np.float32(0.29)),\n",
       " (np.int64(8), np.int64(9), np.float32(0.13)),\n",
       " (np.int64(9), np.int64(17), np.float32(0.03)),\n",
       " (np.int64(17), np.int64(2), np.float32(0.1)),\n",
       " (np.int64(2), np.int64(19), np.float32(0.72)),\n",
       " (np.int64(19), np.int64(14), np.float32(0.18)),\n",
       " (np.int64(14), np.int64(12), np.float32(0.42)),\n",
       " (np.int64(12), np.int64(10), np.float32(0.15)),\n",
       " (np.int64(10), np.int64(16), np.float32(0.18)),\n",
       " (np.int64(16), np.int64(5), np.float32(0.24)),\n",
       " (np.int64(5), np.int64(3), np.float32(0.36)),\n",
       " (np.int64(3), np.int64(1), np.float32(0.27)),\n",
       " (np.int64(1), np.int64(6), np.float32(0.35)),\n",
       " (np.int64(6), np.int64(18), np.float32(0.42)),\n",
       " (np.int64(18), np.int64(11), np.float32(0.27)),\n",
       " (np.int64(11), np.int64(4), np.float32(0.14)),\n",
       " (np.int64(4), np.int64(15), np.float32(0.34)),\n",
       " (np.int64(15), np.int64(7), np.float32(0.55))]"
      ]
     },
     "execution_count": 296,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(i,j,ap[i,j]) for i,j in zip(route[:-1],route[1:])] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "id": "28146bed-fe76-4c99-9f19-dbfd142c212d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.76</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.51</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.83</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.81</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.47</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.57</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.41</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.77</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.34</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.53</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.81</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.42</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0.34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.51</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.75</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.60</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.83</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.64</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.41</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.89</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.73</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.81</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.57</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.64</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.81</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.55</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.65</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.58</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.43</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.73</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.59</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.41</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      0     1     2     3     4     5     6     7     8     9     10    11  \\\n",
       "0   0.00  0.74  0.62  0.69  0.55  0.60  0.77  0.52  0.63  0.56  0.37  0.42   \n",
       "1   0.76  0.00  0.43  0.50  0.45  0.60  0.36  0.68  0.44  0.76  0.75  0.51   \n",
       "2   0.51  0.63  0.00  0.64  0.51  0.83  0.59  0.44  0.56  0.61  0.85  0.81   \n",
       "3   0.47  0.29  0.44  0.00  0.62  0.63  0.53  0.52  0.39  0.98  0.60  0.46   \n",
       "4   0.57  0.72  0.41  0.57  0.00  0.46  0.63  0.56  0.36  0.29  0.75  0.70   \n",
       "5   0.77  0.69  0.71  0.30  0.54  0.00  0.50  0.63  0.52  0.76  0.71  0.69   \n",
       "6   0.53  0.64  0.77  0.81  0.53  0.39  0.00  0.52  0.64  0.74  0.60  0.50   \n",
       "7   0.42  0.53  0.57  0.50  0.55  0.59  0.70  0.00  0.49  0.82  0.38  0.68   \n",
       "8   0.51  0.85  0.64  0.42  0.65  0.45  0.57  0.71  0.00  0.47  0.54  0.64   \n",
       "9   0.75  0.44  0.50  0.69  0.85  0.50  0.36  0.61  0.55  0.00  0.54  0.52   \n",
       "10  0.60  0.53  0.69  0.64  0.47  0.83  0.90  0.32  0.52  0.60  0.00  0.95   \n",
       "11  0.64  0.49  0.62  0.57  0.53  0.78  0.65  0.41  0.68  0.56  0.69  0.00   \n",
       "12  0.89  0.67  0.73  0.49  0.31  0.69  0.69  0.35  0.62  0.80  0.06  0.74   \n",
       "13  0.57  0.63  0.47  0.58  0.58  0.75  0.68  0.75  0.40  0.74  0.54  0.67   \n",
       "14  0.64  0.74  0.51  0.65  0.42  0.64  0.44  0.60  0.57  0.78  0.55  0.81   \n",
       "15  0.55  0.43  0.56  0.53  0.61  0.48  0.62  0.17  0.80  0.72  0.71  0.70   \n",
       "16  0.65  0.37  0.79  0.66  0.53  0.36  0.57  0.48  0.79  0.47  0.44  0.46   \n",
       "17  0.58  0.47  0.43  0.89  0.71  0.56  0.44  0.60  0.95  0.69  0.47  0.51   \n",
       "18  0.43  0.75  0.72  0.57  0.36  0.35  0.73  0.60  0.31  0.68  0.36  0.28   \n",
       "19  0.59  0.36  0.69  0.61  0.44  0.51  0.40  0.70  0.64  0.51  0.69  0.53   \n",
       "\n",
       "      12    13    14    15    16    17    18    19  \n",
       "0   0.47  0.40  0.69  0.69  0.67  0.72  0.79  0.51  \n",
       "1   0.64  0.77  0.87  0.66  0.40  0.49  0.74  0.74  \n",
       "2   0.68  0.62  0.63  0.91  0.53  0.57  0.78  0.20  \n",
       "3   0.76  0.37  0.59  0.63  0.53  0.68  0.43  0.43  \n",
       "4   0.71  0.58  0.72  0.12  0.97  0.56  0.50  0.70  \n",
       "5   0.34  1.00  0.62  0.61  0.67  0.58  0.54  0.71  \n",
       "6   0.80  0.62  0.43  0.86  0.49  0.69  0.33  0.78  \n",
       "7   0.55  0.67  0.61  0.54  0.48  0.67  0.74  0.34  \n",
       "8   0.51  0.46  0.67  0.65  0.50  0.55  0.63  0.61  \n",
       "9   0.69  0.74  0.36  0.53  0.46  0.47  0.53  0.44  \n",
       "10  0.58  0.56  0.84  0.40  0.36  0.55  0.42  0.57  \n",
       "11  0.64  0.61  0.69  0.53  0.65  0.65  0.64  0.59  \n",
       "12  0.00  0.81  0.60  0.43  0.53  0.51  0.66  0.32  \n",
       "13  0.42  0.00  0.52  0.49  0.62  0.61  0.65  0.61  \n",
       "14  0.24  0.68  0.00  0.69  0.56  0.60  0.64  0.33  \n",
       "15  0.40  0.46  0.72  0.00  0.62  0.58  0.52  0.62  \n",
       "16  0.48  0.60  0.67  0.35  0.00  0.85  0.78  0.58  \n",
       "17  0.56  0.69  0.57  0.63  0.80  0.00  0.34  0.58  \n",
       "18  0.67  0.68  0.54  0.35  0.59  0.55  0.00  0.61  \n",
       "19  0.56  0.63  0.38  0.68  0.77  0.64  0.41  0.00  "
      ]
     },
     "execution_count": 298,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(np.round(a,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "996610b9-8f74-4630-bbee-20fcb79acce6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

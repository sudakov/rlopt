{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ecce1ac2-f2f3-4dda-9c18-344393964b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import logging\n",
    "import os\n",
    "from pathlib import Path\n",
    "import ray\n",
    "from graphenv.examples.tsp.graph_utils import make_complete_planar_graph\n",
    "from graphenv.examples.tsp.tsp_model import TSPModel, TSPQModel\n",
    "from graphenv.examples.tsp.tsp_nfp_model import TSPGNNModel\n",
    "from graphenv.examples.tsp.tsp_nfp_state import TSPNFPState\n",
    "from graphenv.examples.tsp.tsp_state import TSPState\n",
    "from graphenv.graph_env import GraphEnv\n",
    "from networkx.algorithms.approximation.traveling_salesman import greedy_tsp\n",
    "from ray import tune\n",
    "from ray.rllib.algorithms.a3c import A3CConfig\n",
    "from ray.rllib.algorithms.dqn import DQNConfig\n",
    "from ray.rllib.algorithms.marwil import MARWILConfig\n",
    "from ray.rllib.algorithms.ppo import PPOConfig\n",
    "from ray.rllib.models import ModelCatalog\n",
    "from ray.rllib.utils.framework import try_import_tf\n",
    "from ray.tune.registry import register_env\n",
    "import networkx as nx\n",
    "from ray.rllib.algorithms.algorithm import Algorithm\n",
    "from ray.tune import ExperimentAnalysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf5c0c65-0fe9-4515-a05a-f769f828f36a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_StoreAction(option_strings=['--log-level'], dest='log_level', nargs=None, const=None, default='INFO', type=<class 'str'>, choices=None, required=False, help=None, metavar=None)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf1, tf, tfv = try_import_tf()\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\n",
    "    \"--run\",\n",
    "    type=str,\n",
    "    default=\"PPO\",\n",
    "    choices=[\"PPO\", \"DQN\", \"A3C\", \"MARWIL\"],\n",
    "    help=\"The RLlib-registered algorithm to use.\",\n",
    ")\n",
    "parser.add_argument(\"--N\", type=int, default=5, help=\"Number of nodes in TSP network\")\n",
    "parser.add_argument(\n",
    "    \"--use-gnn\", action=\"store_true\", help=\"use the nfp state and gnn model\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--max-num-neighbors\",\n",
    "    type=int,\n",
    "    default=5,\n",
    "    help=\"Number of nearest neighbors for the gnn model\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--seed\", type=int, default=0, help=\"Random seed used to generate networkx graph\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--num-workers\", type=int, default=1, help=\"Number of rllib workers\"\n",
    ")\n",
    "parser.add_argument(\"--num-gpus\", type=int, default=0, help=\"Number of GPUs\")\n",
    "parser.add_argument(\"--lr\", type=float, default=1e-4, help=\"learning rate\")\n",
    "parser.add_argument(\n",
    "    \"--entropy-coeff\", type=float, default=0.0, help=\"entropy coefficient\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--rollouts-per-worker\",\n",
    "    type=int,\n",
    "    default=1,\n",
    "    help=\"Number of rollouts for each worker to collect\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--stop-iters\", type=int, default=100, help=\"Number of iterations to train.\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--stop-timesteps\", type=int, default=100000, help=\"Number of timesteps to train.\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--stop-reward\", type=float, default=0.0, help=\"Reward at which we stop training.\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--local-mode\",\n",
    "    action=\"store_true\",\n",
    "    help=\"Init Ray in local mode for easier debugging.\",\n",
    ")\n",
    "parser.add_argument(\"--log-level\", type=str, default=\"INFO\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3a8c3210-76d3-4fd4-b291-d629a5fb818c",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = parser.parse_args(args=[])\n",
    "args.use_gnn=True\n",
    "#args.num_gpus=0\n",
    "#args.num_workers = 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "296a6bf2-92f8-4061-8447-12f29cb46b08",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running with following CLI options: Namespace(run='PPO', N=5, use_gnn=True, max_num_neighbors=5, seed=0, num_workers=1, num_gpus=0, lr=0.0001, entropy_coeff=0.0, rollouts_per_worker=1, stop_iters=100, stop_timesteps=100000, stop_reward=0.0, local_mode=False, log_level='INFO')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-10 18:55:29,811\tINFO worker.py:1553 -- Started a local Ray instance.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "    <div style=\"margin-left: 50px;display: flex;flex-direction: row;align-items: center\">\n",
       "        <h3 style=\"color: var(--jp-ui-font-color0)\">Ray</h3>\n",
       "        <svg version=\"1.1\" id=\"ray\" width=\"3em\" viewBox=\"0 0 144.5 144.6\" style=\"margin-left: 3em;margin-right: 3em\">\n",
       "            <g id=\"layer-1\">\n",
       "                <path fill=\"#00a2e9\" class=\"st0\" d=\"M97.3,77.2c-3.8-1.1-6.2,0.9-8.3,5.1c-3.5,6.8-9.9,9.9-17.4,9.6S58,88.1,54.8,81.2c-1.4-3-3-4-6.3-4.1\n",
       "                    c-5.6-0.1-9.9,0.1-13.1,6.4c-3.8,7.6-13.6,10.2-21.8,7.6C5.2,88.4-0.4,80.5,0,71.7c0.1-8.4,5.7-15.8,13.8-18.2\n",
       "                    c8.4-2.6,17.5,0.7,22.3,8c1.3,1.9,1.3,5.2,3.6,5.6c3.9,0.6,8,0.2,12,0.2c1.8,0,1.9-1.6,2.4-2.8c3.5-7.8,9.7-11.8,18-11.9\n",
       "                    c8.2-0.1,14.4,3.9,17.8,11.4c1.3,2.8,2.9,3.6,5.7,3.3c1-0.1,2,0.1,3,0c2.8-0.5,6.4,1.7,8.1-2.7s-2.3-5.5-4.1-7.5\n",
       "                    c-5.1-5.7-10.9-10.8-16.1-16.3C84,38,81.9,37.1,78,38.3C66.7,42,56.2,35.7,53,24.1C50.3,14,57.3,2.8,67.7,0.5\n",
       "                    C78.4-2,89,4.7,91.5,15.3c0.1,0.3,0.1,0.5,0.2,0.8c0.7,3.4,0.7,6.9-0.8,9.8c-1.7,3.2-0.8,5,1.5,7.2c6.7,6.5,13.3,13,19.8,19.7\n",
       "                    c1.8,1.8,3,2.1,5.5,1.2c9.1-3.4,17.9-0.6,23.4,7c4.8,6.9,4.6,16.1-0.4,22.9c-5.4,7.2-14.2,9.9-23.1,6.5c-2.3-0.9-3.5-0.6-5.1,1.1\n",
       "                    c-6.7,6.9-13.6,13.7-20.5,20.4c-1.8,1.8-2.5,3.2-1.4,5.9c3.5,8.7,0.3,18.6-7.7,23.6c-7.9,5-18.2,3.8-24.8-2.9\n",
       "                    c-6.4-6.4-7.4-16.2-2.5-24.3c4.9-7.8,14.5-11,23.1-7.8c3,1.1,4.7,0.5,6.9-1.7C91.7,98.4,98,92.3,104.2,86c1.6-1.6,4.1-2.7,2.6-6.2\n",
       "                    c-1.4-3.3-3.8-2.5-6.2-2.6C99.8,77.2,98.9,77.2,97.3,77.2z M72.1,29.7c5.5,0.1,9.9-4.3,10-9.8c0-0.1,0-0.2,0-0.3\n",
       "                    C81.8,14,77,9.8,71.5,10.2c-5,0.3-9,4.2-9.3,9.2c-0.2,5.5,4,10.1,9.5,10.3C71.8,29.7,72,29.7,72.1,29.7z M72.3,62.3\n",
       "                    c-5.4-0.1-9.9,4.2-10.1,9.7c0,0.2,0,0.3,0,0.5c0.2,5.4,4.5,9.7,9.9,10c5.1,0.1,9.9-4.7,10.1-9.8c0.2-5.5-4-10-9.5-10.3\n",
       "                    C72.6,62.3,72.4,62.3,72.3,62.3z M115,72.5c0.1,5.4,4.5,9.7,9.8,9.9c5.6-0.2,10-4.8,10-10.4c-0.2-5.4-4.6-9.7-10-9.7\n",
       "                    c-5.3-0.1-9.8,4.2-9.9,9.5C115,72.1,115,72.3,115,72.5z M19.5,62.3c-5.4,0.1-9.8,4.4-10,9.8c-0.1,5.1,5.2,10.4,10.2,10.3\n",
       "                    c5.6-0.2,10-4.9,9.8-10.5c-0.1-5.4-4.5-9.7-9.9-9.6C19.6,62.3,19.5,62.3,19.5,62.3z M71.8,134.6c5.9,0.2,10.3-3.9,10.4-9.6\n",
       "                    c0.5-5.5-3.6-10.4-9.1-10.8c-5.5-0.5-10.4,3.6-10.8,9.1c0,0.5,0,0.9,0,1.4c-0.2,5.3,4,9.8,9.3,10\n",
       "                    C71.6,134.6,71.7,134.6,71.8,134.6z\"/>\n",
       "            </g>\n",
       "        </svg>\n",
       "        <table>\n",
       "            <tr>\n",
       "                <td style=\"text-align: left\"><b>Python version:</b></td>\n",
       "                <td style=\"text-align: left\"><b>3.9.21</b></td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                <td style=\"text-align: left\"><b>Ray version:</b></td>\n",
       "                <td style=\"text-align: left\"><b> 2.3.1</b></td>\n",
       "            </tr>\n",
       "            \n",
       "        </table>\n",
       "    </div>\n",
       "</div>\n"
      ],
      "text/plain": [
       "RayContext(dashboard_url='', python_version='3.9.21', ray_version='2.3.1', ray_commit='5f14cee8dfc6d61ec4fd3bc2c440f9944e92b33a', address_info={'node_ip_address': '192.168.0.126', 'raylet_ip_address': '192.168.0.126', 'redis_address': None, 'object_store_address': '/tmp/ray/session_2025-05-10_18-55-27_974904_40845/sockets/plasma_store', 'raylet_socket_name': '/tmp/ray/session_2025-05-10_18-55-27_974904_40845/sockets/raylet', 'webui_url': '', 'session_dir': '/tmp/ray/session_2025-05-10_18-55-27_974904_40845', 'metrics_export_port': 48200, 'gcs_address': '192.168.0.126:61467', 'address': '192.168.0.126:61467', 'dashboard_agent_listen_port': 52365, 'node_id': '2c8e8410a5ca7f3fe5b0e24c35ebe77be7ac38880c25daef155dd01d'})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=42601)\u001b[0m WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "\u001b[2m\u001b[36m(pid=42601)\u001b[0m I0000 00:00:1746903367.000684   42601 fork_posix.cc:75] Other threads are currently calling into gRPC, skipping fork() handlers\n",
      "\u001b[2m\u001b[36m(PPO pid=42601)\u001b[0m 2025-05-10 18:56:08,645\tWARNING algorithm_config.py:596 -- Cannot create PPOConfig from given `config_dict`! Property __stdout_file__ not supported.\n",
      "\u001b[2m\u001b[36m(PPO pid=42601)\u001b[0m 2025-05-10 18:56:08,645\tINFO algorithm_config.py:2888 -- Executing eagerly (framework='tf2'), with eager_tracing=tf2. For production workloads, make sure to set eager_tracing=True  in order to match the speed of tf-static-graph (framework='tf'). For debugging purposes, `eager_tracing=False` is the best choice.\n",
      "\u001b[2m\u001b[36m(pid=42734)\u001b[0m WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "\u001b[2m\u001b[36m(pid=42734)\u001b[0m I0000 00:00:1746903370.370971   42734 fork_posix.cc:75] Other threads are currently calling into gRPC, skipping fork() handlers\n",
      "\u001b[2m\u001b[36m(pid=42735)\u001b[0m WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "\u001b[2m\u001b[36m(pid=42735)\u001b[0m I0000 00:00:1746903370.402369   42735 fork_posix.cc:75] Other threads are currently calling into gRPC, skipping fork() handlers\n",
      "\u001b[2m\u001b[36m(pid=42736)\u001b[0m WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "\u001b[2m\u001b[36m(pid=42736)\u001b[0m I0000 00:00:1746903370.390215   42736 fork_posix.cc:75] Other threads are currently calling into gRPC, skipping fork() handlers\n",
      "\u001b[2m\u001b[36m(pid=42733)\u001b[0m WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "\u001b[2m\u001b[36m(pid=42733)\u001b[0m I0000 00:00:1746903370.408903   42733 fork_posix.cc:75] Other threads are currently calling into gRPC, skipping fork() handlers\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=42735)\u001b[0m /home/vladimir/.pyenv/versions/3.9.21/lib/python3.9/site-packages/gymnasium/spaces/box.py:227: UserWarning: \u001b[33mWARN: Casting input x to numpy array.\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=42735)\u001b[0m   logger.warn(\"Casting input x to numpy array.\")\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=42734)\u001b[0m /home/vladimir/.pyenv/versions/3.9.21/lib/python3.9/site-packages/gymnasium/spaces/box.py:227: UserWarning: \u001b[33mWARN: Casting input x to numpy array.\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=42734)\u001b[0m   logger.warn(\"Casting input x to numpy array.\")\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=42736)\u001b[0m /home/vladimir/.pyenv/versions/3.9.21/lib/python3.9/site-packages/gymnasium/spaces/box.py:227: UserWarning: \u001b[33mWARN: Casting input x to numpy array.\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=42736)\u001b[0m   logger.warn(\"Casting input x to numpy array.\")\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=42733)\u001b[0m 2025-05-10 18:56:12,126\tWARNING env.py:156 -- Your env doesn't have a .spec.max_episode_steps attribute. Your horizon will default to infinity, and your environment will not be reset.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=42733)\u001b[0m /home/vladimir/.pyenv/versions/3.9.21/lib/python3.9/site-packages/gymnasium/spaces/box.py:227: UserWarning: \u001b[33mWARN: Casting input x to numpy array.\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=42733)\u001b[0m   logger.warn(\"Casting input x to numpy array.\")\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=42736)\u001b[0m 2025-05-10 18:56:13,184\tINFO eager_tf_policy_v2.py:75 -- Creating TF-eager policy running on CPU.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=42736)\u001b[0m 2025-05-10 18:56:13,198\tINFO policy.py:1214 -- Policy (worker=4) running on CPU.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=42734)\u001b[0m 2025-05-10 18:56:13,258\tINFO eager_tf_policy_v2.py:75 -- Creating TF-eager policy running on CPU.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=42734)\u001b[0m 2025-05-10 18:56:13,273\tINFO policy.py:1214 -- Policy (worker=2) running on CPU.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=42734)\u001b[0m 2025-05-10 18:56:13,282\tINFO catalog.py:477 -- Wrapping <class 'graphenv.examples.tsp.tsp_nfp_model.TSPGNNModel'> as None\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=42736)\u001b[0m 2025-05-10 18:56:13,206\tINFO catalog.py:477 -- Wrapping <class 'graphenv.examples.tsp.tsp_nfp_model.TSPGNNModel'> as None\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=42733)\u001b[0m 2025-05-10 18:56:13,258\tINFO eager_tf_policy_v2.py:75 -- Creating TF-eager policy running on CPU.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=42733)\u001b[0m 2025-05-10 18:56:13,273\tINFO policy.py:1214 -- Policy (worker=1) running on CPU.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=42733)\u001b[0m 2025-05-10 18:56:13,282\tINFO catalog.py:477 -- Wrapping <class 'graphenv.examples.tsp.tsp_nfp_model.TSPGNNModel'> as None\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=42735)\u001b[0m 2025-05-10 18:56:13,336\tINFO eager_tf_policy_v2.py:75 -- Creating TF-eager policy running on CPU.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=42735)\u001b[0m 2025-05-10 18:56:13,351\tINFO policy.py:1214 -- Policy (worker=3) running on CPU.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=42735)\u001b[0m 2025-05-10 18:56:13,360\tINFO catalog.py:477 -- Wrapping <class 'graphenv.examples.tsp.tsp_nfp_model.TSPGNNModel'> as None\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=42736)\u001b[0m 2025-05-10 18:56:14,319\tINFO util.py:122 -- Using connectors:\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=42736)\u001b[0m 2025-05-10 18:56:14,320\tINFO util.py:123 --     AgentConnectorPipeline\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=42736)\u001b[0m         ObsPreprocessorConnector\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=42736)\u001b[0m         StateBufferConnector\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=42736)\u001b[0m         ViewRequirementAgentConnector\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=42736)\u001b[0m 2025-05-10 18:56:14,320\tINFO util.py:124 --     ActionConnectorPipeline\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=42736)\u001b[0m         ConvertToNumpyConnector\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=42736)\u001b[0m         NormalizeActionsConnector\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=42736)\u001b[0m         ImmutableActionsConnector\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=42733)\u001b[0m 2025-05-10 18:56:14,409\tINFO util.py:122 -- Using connectors:\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=42733)\u001b[0m 2025-05-10 18:56:14,409\tINFO util.py:123 --     AgentConnectorPipeline\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=42733)\u001b[0m         ObsPreprocessorConnector\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=42733)\u001b[0m         StateBufferConnector\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=42733)\u001b[0m         ViewRequirementAgentConnector\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=42733)\u001b[0m 2025-05-10 18:56:14,409\tINFO util.py:124 --     ActionConnectorPipeline\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=42733)\u001b[0m         ConvertToNumpyConnector\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=42733)\u001b[0m         NormalizeActionsConnector\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=42733)\u001b[0m         ImmutableActionsConnector\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=42734)\u001b[0m 2025-05-10 18:56:14,422\tINFO util.py:122 -- Using connectors:\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=42734)\u001b[0m 2025-05-10 18:56:14,422\tINFO util.py:123 --     AgentConnectorPipeline\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=42734)\u001b[0m         ObsPreprocessorConnector\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=42734)\u001b[0m         StateBufferConnector\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=42734)\u001b[0m         ViewRequirementAgentConnector\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=42734)\u001b[0m 2025-05-10 18:56:14,422\tINFO util.py:124 --     ActionConnectorPipeline\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=42734)\u001b[0m         ConvertToNumpyConnector\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=42734)\u001b[0m         NormalizeActionsConnector\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=42734)\u001b[0m         ImmutableActionsConnector\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=42735)\u001b[0m 2025-05-10 18:56:14,520\tINFO util.py:122 -- Using connectors:\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=42735)\u001b[0m 2025-05-10 18:56:14,520\tINFO util.py:123 --     AgentConnectorPipeline\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=42735)\u001b[0m         ObsPreprocessorConnector\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=42735)\u001b[0m         StateBufferConnector\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=42735)\u001b[0m         ViewRequirementAgentConnector\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=42735)\u001b[0m 2025-05-10 18:56:14,520\tINFO util.py:124 --     ActionConnectorPipeline\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=42735)\u001b[0m         ConvertToNumpyConnector\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=42735)\u001b[0m         NormalizeActionsConnector\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=42735)\u001b[0m         ImmutableActionsConnector\n",
      "\u001b[2m\u001b[36m(PPO pid=42601)\u001b[0m 2025-05-10 18:56:14,526\tINFO worker_set.py:310 -- Inferred observation/action spaces from remote worker (local worker has no env): {'default_policy': (Repeated(Dict('connectivity': Box(0, 5, (20, 2), int64), 'current_node': Box(0, 5, (), int64), 'distance': Box(0.0, 1.4142135623730951, (), float64), 'edge_weights': Box(0.0, 1.4142135623730951, (20,), float64), 'node_visited': Box(0, 2, (5,), int64)), 6), Discrete(5)), '__env__': (Repeated(Dict('connectivity': Box(0, 5, (20, 2), int64), 'current_node': Box(0, 5, (), int64), 'distance': Box(0.0, 1.4142135623730951, (), float64), 'edge_weights': Box(0.0, 1.4142135623730951, (20,), float64), 'node_visited': Box(0, 2, (5,), int64)), 6), Discrete(5))}\n",
      "\u001b[2m\u001b[36m(PPO pid=42601)\u001b[0m 2025-05-10 18:56:15,725\tINFO eager_tf_policy_v2.py:75 -- Creating TF-eager policy running on CPU.\n",
      "\u001b[2m\u001b[36m(PPO pid=42601)\u001b[0m 2025-05-10 18:56:15,740\tINFO policy.py:1214 -- Policy (worker=local) running on CPU.\n",
      "\u001b[2m\u001b[36m(PPO pid=42601)\u001b[0m 2025-05-10 18:56:15,749\tINFO catalog.py:477 -- Wrapping <class 'graphenv.examples.tsp.tsp_nfp_model.TSPGNNModel'> as None\n",
      "\u001b[2m\u001b[36m(PPO pid=42601)\u001b[0m 2025-05-10 18:56:16,934\tINFO util.py:122 -- Using connectors:\n",
      "\u001b[2m\u001b[36m(PPO pid=42601)\u001b[0m 2025-05-10 18:56:16,934\tINFO util.py:123 --     AgentConnectorPipeline\n",
      "\u001b[2m\u001b[36m(PPO pid=42601)\u001b[0m         ObsPreprocessorConnector\n",
      "\u001b[2m\u001b[36m(PPO pid=42601)\u001b[0m         StateBufferConnector\n",
      "\u001b[2m\u001b[36m(PPO pid=42601)\u001b[0m         ViewRequirementAgentConnector\n",
      "\u001b[2m\u001b[36m(PPO pid=42601)\u001b[0m 2025-05-10 18:56:16,934\tINFO util.py:124 --     ActionConnectorPipeline\n",
      "\u001b[2m\u001b[36m(PPO pid=42601)\u001b[0m         ConvertToNumpyConnector\n",
      "\u001b[2m\u001b[36m(PPO pid=42601)\u001b[0m         NormalizeActionsConnector\n",
      "\u001b[2m\u001b[36m(PPO pid=42601)\u001b[0m         ImmutableActionsConnector\n",
      "\u001b[2m\u001b[36m(PPO pid=42601)\u001b[0m 2025-05-10 18:56:16,934\tINFO rollout_worker.py:2043 -- Built policy map: <PolicyMap lru-caching-capacity=100 policy-IDs=['default_policy']>\n",
      "\u001b[2m\u001b[36m(PPO pid=42601)\u001b[0m 2025-05-10 18:56:16,934\tINFO rollout_worker.py:2044 -- Built preprocessor map: {'default_policy': None}\n",
      "\u001b[2m\u001b[36m(PPO pid=42601)\u001b[0m 2025-05-10 18:56:16,934\tINFO rollout_worker.py:760 -- Built filter map: defaultdict(<class 'ray.rllib.utils.filter.NoFilter'>, {})\n",
      "\u001b[2m\u001b[36m(PPO pid=42601)\u001b[0m 2025-05-10 18:56:16,943\tINFO algorithm_config.py:2888 -- Executing eagerly (framework='tf2'), with eager_tracing=tf2. For production workloads, make sure to set eager_tracing=True  in order to match the speed of tf-static-graph (framework='tf'). For debugging purposes, `eager_tracing=False` is the best choice.\n",
      "\u001b[2m\u001b[36m(PPO pid=42601)\u001b[0m 2025-05-10 18:56:16,947\tWARNING env.py:156 -- Your env doesn't have a .spec.max_episode_steps attribute. Your horizon will default to infinity, and your environment will not be reset.\n",
      "\u001b[2m\u001b[36m(PPO pid=42601)\u001b[0m /home/vladimir/.pyenv/versions/3.9.21/lib/python3.9/site-packages/gymnasium/spaces/box.py:227: UserWarning: \u001b[33mWARN: Casting input x to numpy array.\u001b[0m\n",
      "\u001b[2m\u001b[36m(PPO pid=42601)\u001b[0m   logger.warn(\"Casting input x to numpy array.\")\n",
      "\u001b[2m\u001b[36m(PPO pid=42601)\u001b[0m 2025-05-10 18:56:16,953\tINFO eager_tf_policy_v2.py:75 -- Creating TF-eager policy running on CPU.\n",
      "\u001b[2m\u001b[36m(PPO pid=42601)\u001b[0m 2025-05-10 18:56:16,955\tINFO policy.py:1214 -- Policy (worker=local) running on CPU.\n",
      "\u001b[2m\u001b[36m(PPO pid=42601)\u001b[0m 2025-05-10 18:56:16,955\tINFO catalog.py:477 -- Wrapping <class 'graphenv.examples.tsp.tsp_nfp_model.TSPGNNModel'> as None\n",
      "\u001b[2m\u001b[36m(PPO pid=42601)\u001b[0m 2025-05-10 18:56:17,492\tINFO util.py:122 -- Using connectors:\n",
      "\u001b[2m\u001b[36m(PPO pid=42601)\u001b[0m 2025-05-10 18:56:17,492\tINFO util.py:123 --     AgentConnectorPipeline\n",
      "\u001b[2m\u001b[36m(PPO pid=42601)\u001b[0m         ObsPreprocessorConnector\n",
      "\u001b[2m\u001b[36m(PPO pid=42601)\u001b[0m         StateBufferConnector\n",
      "\u001b[2m\u001b[36m(PPO pid=42601)\u001b[0m         ViewRequirementAgentConnector\n",
      "\u001b[2m\u001b[36m(PPO pid=42601)\u001b[0m 2025-05-10 18:56:17,492\tINFO util.py:124 --     ActionConnectorPipeline\n",
      "\u001b[2m\u001b[36m(PPO pid=42601)\u001b[0m         ConvertToNumpyConnector\n",
      "\u001b[2m\u001b[36m(PPO pid=42601)\u001b[0m         NormalizeActionsConnector\n",
      "\u001b[2m\u001b[36m(PPO pid=42601)\u001b[0m         ImmutableActionsConnector\n",
      "\u001b[2m\u001b[36m(PPO pid=42601)\u001b[0m 2025-05-10 18:56:17,492\tINFO rollout_worker.py:2043 -- Built policy map: <PolicyMap lru-caching-capacity=100 policy-IDs=['default_policy']>\n",
      "\u001b[2m\u001b[36m(PPO pid=42601)\u001b[0m 2025-05-10 18:56:17,492\tINFO rollout_worker.py:2044 -- Built preprocessor map: {'default_policy': None}\n",
      "\u001b[2m\u001b[36m(PPO pid=42601)\u001b[0m 2025-05-10 18:56:17,492\tINFO rollout_worker.py:760 -- Built filter map: defaultdict(<class 'ray.rllib.utils.filter.NoFilter'>, {})\n",
      "\u001b[2m\u001b[36m(PPO pid=42601)\u001b[0m 2025-05-10 18:56:17,492\tWARNING util.py:67 -- Install gputil for GPU system monitoring.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=42733)\u001b[0m 2025-05-10 18:56:17,496\tINFO rollout_worker.py:908 -- Generating sample batch of size 5\n",
      "\u001b[2m\u001b[36m(PPO pid=42601)\u001b[0m 2025-05-10 18:56:18,443\tWARNING deprecation.py:50 -- DeprecationWarning: `_get_slice_indices` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=42601)\u001b[0m /home/vladimir/.pyenv/versions/3.9.21/lib/python3.9/site-packages/numpy/core/_methods.py:180: RuntimeWarning: overflow encountered in reduce\n",
      "\u001b[2m\u001b[36m(PPO pid=42601)\u001b[0m   ret = umr_sum(arr, axis, dtype, out, keepdims, where=where)\n",
      "\u001b[2m\u001b[36m(PPO pid=42601)\u001b[0m 2025-05-10 18:56:18,444\tINFO rollout_worker.py:1004 -- Training on concatenated sample batches:\n",
      "\u001b[2m\u001b[36m(PPO pid=42601)\u001b[0m \n",
      "\u001b[2m\u001b[36m(PPO pid=42601)\u001b[0m { 'count': 2,\n",
      "\u001b[2m\u001b[36m(PPO pid=42601)\u001b[0m   'policy_batches': { 'default_policy': { 'action_dist_inputs': np.ndarray((2, 5), dtype=float32, min=-3.4028234663852886e+38, max=-4.113, mean=-inf),\n",
      "\u001b[2m\u001b[36m(PPO pid=42601)\u001b[0m                                           'action_logp': np.ndarray((2,), dtype=float32, min=-0.049, max=-0.0, mean=-0.025),\n",
      "\u001b[2m\u001b[36m(PPO pid=42601)\u001b[0m                                           'actions': np.ndarray((2,), dtype=int32, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(PPO pid=42601)\u001b[0m                                           'advantages': np.ndarray((2,), dtype=float32, min=-0.415, max=2.061, mean=0.823),\n",
      "\u001b[2m\u001b[36m(PPO pid=42601)\u001b[0m                                           'agent_index': np.ndarray((2,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(PPO pid=42601)\u001b[0m                                           'eps_id': np.ndarray((2,), dtype=int64, min=7.070023125329706e+17, max=7.070023125329706e+17, mean=7.070023125329706e+17),\n",
      "\u001b[2m\u001b[36m(PPO pid=42601)\u001b[0m                                           'new_obs': np.ndarray((2, 403), dtype=float32, min=0.0, max=4.0, mean=0.496),\n",
      "\u001b[2m\u001b[36m(PPO pid=42601)\u001b[0m                                           'obs': np.ndarray((2, 403), dtype=float32, min=0.0, max=4.0, mean=0.741),\n",
      "\u001b[2m\u001b[36m(PPO pid=42601)\u001b[0m                                           'rewards': np.ndarray((2,), dtype=float32, min=-0.531, max=-0.206, mean=-0.368),\n",
      "\u001b[2m\u001b[36m(PPO pid=42601)\u001b[0m                                           't': np.ndarray((2,), dtype=int64, min=1.0, max=4.0, mean=2.5),\n",
      "\u001b[2m\u001b[36m(PPO pid=42601)\u001b[0m                                           'terminateds': np.ndarray((2,), dtype=bool, min=0.0, max=1.0, mean=0.5),\n",
      "\u001b[2m\u001b[36m(PPO pid=42601)\u001b[0m                                           'truncateds': np.ndarray((2,), dtype=bool, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(PPO pid=42601)\u001b[0m                                           'unroll_id': np.ndarray((2,), dtype=int64, min=1.0, max=1.0, mean=1.0),\n",
      "\u001b[2m\u001b[36m(PPO pid=42601)\u001b[0m                                           'value_targets': np.ndarray((2,), dtype=float32, min=-1.682, max=-0.531, mean=-1.107),\n",
      "\u001b[2m\u001b[36m(PPO pid=42601)\u001b[0m                                           'vf_preds': np.ndarray((2,), dtype=float32, min=-14.631, max=-3.573, mean=-9.102)}},\n",
      "\u001b[2m\u001b[36m(PPO pid=42601)\u001b[0m   'type': 'MultiAgentBatch'}\n",
      "\u001b[2m\u001b[36m(PPO pid=42601)\u001b[0m \n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=42733)\u001b[0m /home/vladimir/.pyenv/versions/3.9.21/lib/python3.9/site-packages/numpy/core/_methods.py:180: RuntimeWarning: overflow encountered in reduce\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=42733)\u001b[0m   ret = umr_sum(arr, axis, dtype, out, keepdims, where=where)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=42733)\u001b[0m 2025-05-10 18:56:18,406\tINFO rollout_worker.py:949 -- Completed sample batch:\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=42733)\u001b[0m \n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=42733)\u001b[0m { 'count': 5,\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=42733)\u001b[0m   'policy_batches': { 'default_policy': { 'action_dist_inputs': np.ndarray((5, 5), dtype=float32, min=-3.4028234663852886e+38, max=-2.861, mean=-inf),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=42733)\u001b[0m                                           'action_logp': np.ndarray((5,), dtype=float32, min=-1.277, max=-0.0, mean=-0.265),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=42733)\u001b[0m                                           'actions': np.ndarray((5,), dtype=int32, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=42733)\u001b[0m                                           'advantages': np.ndarray((5,), dtype=float32, min=-1.844, max=14.1, mean=4.087),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=42733)\u001b[0m                                           'agent_index': np.ndarray((5,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=42733)\u001b[0m                                           'eps_id': np.ndarray((5,), dtype=int64, min=7.070023125329706e+17, max=7.070023125329706e+17, mean=7.070023125329706e+17),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=42733)\u001b[0m                                           'new_obs': np.ndarray((5, 403), dtype=float32, min=0.0, max=4.0, mean=0.596),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=42733)\u001b[0m                                           'obs': np.ndarray((5, 403), dtype=float32, min=0.0, max=5.0, mean=0.788),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=42733)\u001b[0m                                           'rewards': np.ndarray((5,), dtype=float32, min=-0.732, max=-0.179, mean=-0.379),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=42733)\u001b[0m                                           't': np.ndarray((5,), dtype=int64, min=0.0, max=4.0, mean=2.0),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=42733)\u001b[0m                                           'terminateds': np.ndarray((5,), dtype=bool, min=0.0, max=1.0, mean=0.2),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=42733)\u001b[0m                                           'truncateds': np.ndarray((5,), dtype=bool, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=42733)\u001b[0m                                           'unroll_id': np.ndarray((5,), dtype=int64, min=1.0, max=1.0, mean=1.0),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=42733)\u001b[0m                                           'value_targets': np.ndarray((5,), dtype=float32, min=-1.844, max=-0.531, mean=-1.361),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=42733)\u001b[0m                                           'vf_preds': np.ndarray((5,), dtype=float32, min=-14.631, max=0.0, mean=-5.448)}},\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=42733)\u001b[0m   'type': 'MultiAgentBatch'}\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=42733)\u001b[0m \n",
      "\u001b[2m\u001b[36m(PPO pid=42601)\u001b[0m 2025-05-10 18:56:18,944\tWARNING tf_utils.py:576 -- KL divergence is non-finite, this will likely destabilize your model and the training process. Action(s) in a specific state have near-zero probability. This can happen naturally in deterministic environments where the optimal policy has zero mass for a specific action. To fix this issue, consider setting the coefficient for the KL loss term to zero or increasing policy entropy.\n",
      "\u001b[2m\u001b[36m(PPO pid=42601)\u001b[0m 2025-05-10 18:56:19,669\tINFO eager_tf_policy_v2.py:930 -- Optimizing variable node_embedding/embeddings:0\n",
      "\u001b[2m\u001b[36m(PPO pid=42601)\u001b[0m 2025-05-10 18:56:19,670\tINFO eager_tf_policy_v2.py:930 -- Optimizing variable edge_update/concat_dense/dense/kernel:0\n",
      "\u001b[2m\u001b[36m(PPO pid=42601)\u001b[0m 2025-05-10 18:56:19,670\tINFO eager_tf_policy_v2.py:930 -- Optimizing variable edge_update/concat_dense/dense/bias:0\n",
      "\u001b[2m\u001b[36m(PPO pid=42601)\u001b[0m 2025-05-10 18:56:19,670\tINFO eager_tf_policy_v2.py:930 -- Optimizing variable edge_update/concat_dense/dense_1/kernel:0\n",
      "\u001b[2m\u001b[36m(PPO pid=42601)\u001b[0m 2025-05-10 18:56:19,670\tINFO eager_tf_policy_v2.py:930 -- Optimizing variable edge_update/concat_dense/dense_1/bias:0\n",
      "\u001b[2m\u001b[36m(PPO pid=42601)\u001b[0m 2025-05-10 18:56:19,670\tINFO eager_tf_policy_v2.py:930 -- Optimizing variable node_update/concat_dense/dense_2/kernel:0\n",
      "\u001b[2m\u001b[36m(PPO pid=42601)\u001b[0m 2025-05-10 18:56:19,670\tINFO eager_tf_policy_v2.py:930 -- Optimizing variable node_update/concat_dense/dense_2/bias:0\n",
      "\u001b[2m\u001b[36m(PPO pid=42601)\u001b[0m 2025-05-10 18:56:19,670\tINFO eager_tf_policy_v2.py:930 -- Optimizing variable node_update/concat_dense/dense_3/kernel:0\n",
      "\u001b[2m\u001b[36m(PPO pid=42601)\u001b[0m 2025-05-10 18:56:19,670\tINFO eager_tf_policy_v2.py:930 -- Optimizing variable node_update/concat_dense/dense_3/bias:0\n",
      "\u001b[2m\u001b[36m(PPO pid=42601)\u001b[0m 2025-05-10 18:56:19,670\tINFO eager_tf_policy_v2.py:930 -- Optimizing variable node_update/dense/kernel:0\n",
      "\u001b[2m\u001b[36m(PPO pid=42601)\u001b[0m 2025-05-10 18:56:19,670\tINFO eager_tf_policy_v2.py:930 -- Optimizing variable node_update/dense/bias:0\n",
      "\u001b[2m\u001b[36m(PPO pid=42601)\u001b[0m 2025-05-10 18:56:19,670\tINFO eager_tf_policy_v2.py:930 -- Optimizing variable node_update/dense_1/kernel:0\n",
      "\u001b[2m\u001b[36m(PPO pid=42601)\u001b[0m 2025-05-10 18:56:19,670\tINFO eager_tf_policy_v2.py:930 -- Optimizing variable node_update/dense_1/bias:0\n",
      "\u001b[2m\u001b[36m(PPO pid=42601)\u001b[0m 2025-05-10 18:56:19,670\tINFO eager_tf_policy_v2.py:930 -- Optimizing variable edge_update_1/concat_dense/dense/kernel:0\n",
      "\u001b[2m\u001b[36m(PPO pid=42601)\u001b[0m 2025-05-10 18:56:19,670\tINFO eager_tf_policy_v2.py:930 -- Optimizing variable edge_update_1/concat_dense/dense/bias:0\n",
      "\u001b[2m\u001b[36m(PPO pid=42601)\u001b[0m 2025-05-10 18:56:19,670\tINFO eager_tf_policy_v2.py:930 -- Optimizing variable edge_update_1/concat_dense/dense_1/kernel:0\n",
      "\u001b[2m\u001b[36m(PPO pid=42601)\u001b[0m 2025-05-10 18:56:19,670\tINFO eager_tf_policy_v2.py:930 -- Optimizing variable edge_update_1/concat_dense/dense_1/bias:0\n",
      "\u001b[2m\u001b[36m(PPO pid=42601)\u001b[0m 2025-05-10 18:56:19,670\tINFO eager_tf_policy_v2.py:930 -- Optimizing variable node_update_1/concat_dense/dense_2/kernel:0\n",
      "\u001b[2m\u001b[36m(PPO pid=42601)\u001b[0m 2025-05-10 18:56:19,670\tINFO eager_tf_policy_v2.py:930 -- Optimizing variable node_update_1/concat_dense/dense_2/bias:0\n",
      "\u001b[2m\u001b[36m(PPO pid=42601)\u001b[0m 2025-05-10 18:56:19,670\tINFO eager_tf_policy_v2.py:930 -- Optimizing variable node_update_1/concat_dense/dense_3/kernel:0\n",
      "\u001b[2m\u001b[36m(PPO pid=42601)\u001b[0m 2025-05-10 18:56:19,670\tINFO eager_tf_policy_v2.py:930 -- Optimizing variable node_update_1/concat_dense/dense_3/bias:0\n",
      "\u001b[2m\u001b[36m(PPO pid=42601)\u001b[0m 2025-05-10 18:56:19,670\tINFO eager_tf_policy_v2.py:930 -- Optimizing variable node_update_1/dense/kernel:0\n",
      "\u001b[2m\u001b[36m(PPO pid=42601)\u001b[0m 2025-05-10 18:56:19,670\tINFO eager_tf_policy_v2.py:930 -- Optimizing variable node_update_1/dense/bias:0\n",
      "\u001b[2m\u001b[36m(PPO pid=42601)\u001b[0m 2025-05-10 18:56:19,670\tINFO eager_tf_policy_v2.py:930 -- Optimizing variable node_update_1/dense_1/kernel:0\n",
      "\u001b[2m\u001b[36m(PPO pid=42601)\u001b[0m 2025-05-10 18:56:19,670\tINFO eager_tf_policy_v2.py:930 -- Optimizing variable node_update_1/dense_1/bias:0\n",
      "\u001b[2m\u001b[36m(PPO pid=42601)\u001b[0m 2025-05-10 18:56:19,670\tINFO eager_tf_policy_v2.py:930 -- Optimizing variable edge_update_2/concat_dense/dense/kernel:0\n",
      "\u001b[2m\u001b[36m(PPO pid=42601)\u001b[0m 2025-05-10 18:56:19,670\tINFO eager_tf_policy_v2.py:930 -- Optimizing variable edge_update_2/concat_dense/dense/bias:0\n",
      "\u001b[2m\u001b[36m(PPO pid=42601)\u001b[0m 2025-05-10 18:56:19,670\tINFO eager_tf_policy_v2.py:930 -- Optimizing variable edge_update_2/concat_dense/dense_1/kernel:0\n",
      "\u001b[2m\u001b[36m(PPO pid=42601)\u001b[0m 2025-05-10 18:56:19,670\tINFO eager_tf_policy_v2.py:930 -- Optimizing variable edge_update_2/concat_dense/dense_1/bias:0\n",
      "\u001b[2m\u001b[36m(PPO pid=42601)\u001b[0m 2025-05-10 18:56:19,670\tINFO eager_tf_policy_v2.py:930 -- Optimizing variable node_update_2/concat_dense/dense_2/kernel:0\n",
      "\u001b[2m\u001b[36m(PPO pid=42601)\u001b[0m 2025-05-10 18:56:19,670\tINFO eager_tf_policy_v2.py:930 -- Optimizing variable node_update_2/concat_dense/dense_2/bias:0\n",
      "\u001b[2m\u001b[36m(PPO pid=42601)\u001b[0m 2025-05-10 18:56:19,670\tINFO eager_tf_policy_v2.py:930 -- Optimizing variable node_update_2/concat_dense/dense_3/kernel:0\n",
      "\u001b[2m\u001b[36m(PPO pid=42601)\u001b[0m 2025-05-10 18:56:19,670\tINFO eager_tf_policy_v2.py:930 -- Optimizing variable node_update_2/concat_dense/dense_3/bias:0\n",
      "\u001b[2m\u001b[36m(PPO pid=42601)\u001b[0m 2025-05-10 18:56:19,670\tINFO eager_tf_policy_v2.py:930 -- Optimizing variable node_update_2/dense/kernel:0\n",
      "\u001b[2m\u001b[36m(PPO pid=42601)\u001b[0m 2025-05-10 18:56:19,670\tINFO eager_tf_policy_v2.py:930 -- Optimizing variable node_update_2/dense/bias:0\n",
      "\u001b[2m\u001b[36m(PPO pid=42601)\u001b[0m 2025-05-10 18:56:19,670\tINFO eager_tf_policy_v2.py:930 -- Optimizing variable node_update_2/dense_1/kernel:0\n",
      "\u001b[2m\u001b[36m(PPO pid=42601)\u001b[0m 2025-05-10 18:56:19,671\tINFO eager_tf_policy_v2.py:930 -- Optimizing variable node_update_2/dense_1/bias:0\n",
      "\u001b[2m\u001b[36m(PPO pid=42601)\u001b[0m 2025-05-10 18:56:19,671\tINFO eager_tf_policy_v2.py:930 -- Optimizing variable distance_values/kernel:0\n",
      "\u001b[2m\u001b[36m(PPO pid=42601)\u001b[0m 2025-05-10 18:56:19,671\tINFO eager_tf_policy_v2.py:930 -- Optimizing variable distance_values/bias:0\n",
      "\u001b[2m\u001b[36m(PPO pid=42601)\u001b[0m 2025-05-10 18:56:19,671\tINFO eager_tf_policy_v2.py:930 -- Optimizing variable action_value_output/kernel:0\n",
      "\u001b[2m\u001b[36m(PPO pid=42601)\u001b[0m 2025-05-10 18:56:19,671\tINFO eager_tf_policy_v2.py:930 -- Optimizing variable action_value_output/bias:0\n",
      "\u001b[2m\u001b[36m(PPO pid=42601)\u001b[0m 2025-05-10 18:56:19,671\tINFO eager_tf_policy_v2.py:930 -- Optimizing variable distance__weights/kernel:0\n",
      "\u001b[2m\u001b[36m(PPO pid=42601)\u001b[0m 2025-05-10 18:56:19,671\tINFO eager_tf_policy_v2.py:930 -- Optimizing variable distance__weights/bias:0\n",
      "\u001b[2m\u001b[36m(PPO pid=42601)\u001b[0m 2025-05-10 18:56:19,671\tINFO eager_tf_policy_v2.py:930 -- Optimizing variable action_weight_output/kernel:0\n",
      "\u001b[2m\u001b[36m(PPO pid=42601)\u001b[0m 2025-05-10 18:56:19,671\tINFO eager_tf_policy_v2.py:930 -- Optimizing variable action_weight_output/bias:0\n",
      "\u001b[2m\u001b[36m(PPO pid=42601)\u001b[0m 2025-05-10 18:56:20,500\tWARNING tf_utils.py:576 -- KL divergence is non-finite, this will likely destabilize your model and the training process. Action(s) in a specific state have near-zero probability. This can happen naturally in deterministic environments where the optimal policy has zero mass for a specific action. To fix this issue, consider setting the coefficient for the KL loss term to zero or increasing policy entropy.\n",
      "\u001b[2m\u001b[36m(PPO pid=42601)\u001b[0m 2025-05-10 18:56:22,215\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=42601)\u001b[0m 2025-05-10 18:56:22,215\tINFO rollout_worker.py:908 -- Generating sample batch of size 1\n",
      "\u001b[2m\u001b[36m(PPO pid=42601)\u001b[0m 2025-05-10 18:56:23,007\tINFO rollout_worker.py:949 -- Completed sample batch:\n",
      "\u001b[2m\u001b[36m(PPO pid=42601)\u001b[0m \n",
      "\u001b[2m\u001b[36m(PPO pid=42601)\u001b[0m { 'count': 5,\n",
      "\u001b[2m\u001b[36m(PPO pid=42601)\u001b[0m   'policy_batches': { 'default_policy': { 'advantages': np.ndarray((5,), dtype=float32, min=-2.233, max=13.806, mean=4.131),\n",
      "\u001b[2m\u001b[36m(PPO pid=42601)\u001b[0m                                           'agent_index': np.ndarray((5,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(PPO pid=42601)\u001b[0m                                           'eps_id': np.ndarray((5,), dtype=int64, min=8.63463704837785e+16, max=8.63463704837785e+16, mean=8.63463704837785e+16),\n",
      "\u001b[2m\u001b[36m(PPO pid=42601)\u001b[0m                                           'new_obs': np.ndarray((5, 403), dtype=float32, min=0.0, max=4.0, mean=0.595),\n",
      "\u001b[2m\u001b[36m(PPO pid=42601)\u001b[0m                                           'obs': np.ndarray((5, 403), dtype=float32, min=0.0, max=5.0, mean=0.787),\n",
      "\u001b[2m\u001b[36m(PPO pid=42601)\u001b[0m                                           'rewards': np.ndarray((5,), dtype=float32, min=-0.732, max=-0.143, mean=-0.399),\n",
      "\u001b[2m\u001b[36m(PPO pid=42601)\u001b[0m                                           't': np.ndarray((5,), dtype=int64, min=0.0, max=4.0, mean=2.0),\n",
      "\u001b[2m\u001b[36m(PPO pid=42601)\u001b[0m                                           'terminateds': np.ndarray((5,), dtype=bool, min=0.0, max=1.0, mean=0.2),\n",
      "\u001b[2m\u001b[36m(PPO pid=42601)\u001b[0m                                           'truncateds': np.ndarray((5,), dtype=bool, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(PPO pid=42601)\u001b[0m                                           'unroll_id': np.ndarray((5,), dtype=int64, min=1.0, max=1.0, mean=1.0),\n",
      "\u001b[2m\u001b[36m(PPO pid=42601)\u001b[0m                                           'value_targets': np.ndarray((5,), dtype=float32, min=-1.943, max=-0.531, mean=-1.436),\n",
      "\u001b[2m\u001b[36m(PPO pid=42601)\u001b[0m                                           'vf_preds': np.ndarray((5,), dtype=float32, min=-14.337, max=0.29, mean=-5.566)}},\n",
      "\u001b[2m\u001b[36m(PPO pid=42601)\u001b[0m   'type': 'MultiAgentBatch'}\n",
      "\u001b[2m\u001b[36m(PPO pid=42601)\u001b[0m \n",
      "\u001b[2m\u001b[36m(PPO pid=42601)\u001b[0m 2025-05-10 18:56:24,337\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=42601)\u001b[0m 2025-05-10 18:56:25,555\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=42601)\u001b[0m 2025-05-10 18:56:26,751\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=42601)\u001b[0m 2025-05-10 18:56:27,937\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=42601)\u001b[0m 2025-05-10 18:56:29,123\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=42601)\u001b[0m 2025-05-10 18:56:30,316\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=42601)\u001b[0m 2025-05-10 18:56:31,511\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=42601)\u001b[0m 2025-05-10 18:56:32,697\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=42601)\u001b[0m 2025-05-10 18:56:33,887\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=42601)\u001b[0m 2025-05-10 18:56:35,132\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=42601)\u001b[0m 2025-05-10 18:56:36,351\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=42601)\u001b[0m 2025-05-10 18:56:37,561\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=42601)\u001b[0m 2025-05-10 18:56:38,777\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=42601)\u001b[0m 2025-05-10 18:56:39,998\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=42601)\u001b[0m 2025-05-10 18:56:41,185\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=42601)\u001b[0m 2025-05-10 18:56:42,374\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=42601)\u001b[0m 2025-05-10 18:56:43,572\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=42601)\u001b[0m 2025-05-10 18:56:44,762\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=42601)\u001b[0m 2025-05-10 18:56:45,949\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=42601)\u001b[0m 2025-05-10 18:56:47,154\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=42601)\u001b[0m 2025-05-10 18:56:48,369\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=42601)\u001b[0m 2025-05-10 18:56:49,577\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=42601)\u001b[0m 2025-05-10 18:56:50,784\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=42601)\u001b[0m 2025-05-10 18:56:51,997\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=42601)\u001b[0m 2025-05-10 18:56:53,193\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=42601)\u001b[0m 2025-05-10 18:56:54,422\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=42601)\u001b[0m 2025-05-10 18:56:55,632\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=42601)\u001b[0m 2025-05-10 18:56:56,848\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=42601)\u001b[0m 2025-05-10 18:56:58,066\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=42601)\u001b[0m 2025-05-10 18:56:59,305\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=42601)\u001b[0m 2025-05-10 18:57:00,524\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=42601)\u001b[0m 2025-05-10 18:57:01,770\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=42601)\u001b[0m 2025-05-10 18:57:02,955\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=42601)\u001b[0m 2025-05-10 18:57:04,162\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=42601)\u001b[0m 2025-05-10 18:57:05,347\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=42601)\u001b[0m 2025-05-10 18:57:06,539\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=42601)\u001b[0m 2025-05-10 18:57:07,725\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=42601)\u001b[0m 2025-05-10 18:57:08,921\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=42601)\u001b[0m 2025-05-10 18:57:10,110\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=42601)\u001b[0m 2025-05-10 18:57:11,335\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=42601)\u001b[0m 2025-05-10 18:57:12,535\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=42601)\u001b[0m 2025-05-10 18:57:13,725\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=42601)\u001b[0m 2025-05-10 18:57:14,926\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=42601)\u001b[0m 2025-05-10 18:57:16,123\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=42601)\u001b[0m 2025-05-10 18:57:17,336\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=42601)\u001b[0m 2025-05-10 18:57:18,556\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=42601)\u001b[0m 2025-05-10 18:57:19,761\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=42601)\u001b[0m 2025-05-10 18:57:20,961\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=42601)\u001b[0m 2025-05-10 18:57:22,173\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=42601)\u001b[0m 2025-05-10 18:57:23,399\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=42601)\u001b[0m 2025-05-10 18:57:24,607\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=42601)\u001b[0m 2025-05-10 18:57:25,831\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=42601)\u001b[0m 2025-05-10 18:57:27,055\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=42601)\u001b[0m 2025-05-10 18:57:28,280\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=42601)\u001b[0m 2025-05-10 18:57:29,492\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=42601)\u001b[0m 2025-05-10 18:57:30,711\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=42601)\u001b[0m 2025-05-10 18:57:31,910\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=42601)\u001b[0m 2025-05-10 18:57:33,107\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=42601)\u001b[0m 2025-05-10 18:57:34,303\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=42601)\u001b[0m 2025-05-10 18:57:35,510\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=42601)\u001b[0m 2025-05-10 18:57:36,707\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=42601)\u001b[0m 2025-05-10 18:57:37,900\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=42601)\u001b[0m 2025-05-10 18:57:39,103\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=42601)\u001b[0m 2025-05-10 18:57:40,314\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=42601)\u001b[0m 2025-05-10 18:57:41,509\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=42601)\u001b[0m 2025-05-10 18:57:42,706\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=42601)\u001b[0m 2025-05-10 18:57:43,910\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=42601)\u001b[0m 2025-05-10 18:57:45,122\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=42601)\u001b[0m 2025-05-10 18:57:46,321\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=42601)\u001b[0m 2025-05-10 18:57:47,551\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=42601)\u001b[0m 2025-05-10 18:57:48,770\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=42601)\u001b[0m 2025-05-10 18:57:49,974\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=42601)\u001b[0m 2025-05-10 18:57:51,163\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=42601)\u001b[0m 2025-05-10 18:57:52,383\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=42601)\u001b[0m 2025-05-10 18:57:53,599\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=42601)\u001b[0m 2025-05-10 18:57:54,826\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=42601)\u001b[0m 2025-05-10 18:57:56,048\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=42601)\u001b[0m 2025-05-10 18:57:57,266\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=42601)\u001b[0m 2025-05-10 18:57:58,480\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=42601)\u001b[0m 2025-05-10 18:57:59,732\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=42601)\u001b[0m 2025-05-10 18:58:00,951\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=42601)\u001b[0m 2025-05-10 18:58:02,170\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=42601)\u001b[0m 2025-05-10 18:58:03,374\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=42601)\u001b[0m 2025-05-10 18:58:04,589\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=42601)\u001b[0m 2025-05-10 18:58:05,799\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=42601)\u001b[0m 2025-05-10 18:58:07,023\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=42601)\u001b[0m 2025-05-10 18:58:08,245\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=42601)\u001b[0m 2025-05-10 18:58:09,474\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=42601)\u001b[0m 2025-05-10 18:58:10,676\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=42601)\u001b[0m 2025-05-10 18:58:11,919\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=42601)\u001b[0m 2025-05-10 18:58:13,133\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=42601)\u001b[0m 2025-05-10 18:58:14,347\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=42601)\u001b[0m 2025-05-10 18:58:15,548\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=42601)\u001b[0m 2025-05-10 18:58:16,752\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=42601)\u001b[0m 2025-05-10 18:58:17,963\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=42601)\u001b[0m 2025-05-10 18:58:19,169\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=42601)\u001b[0m 2025-05-10 18:58:20,367\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=42601)\u001b[0m 2025-05-10 18:58:21,563\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=42601)\u001b[0m 2025-05-10 18:58:22,760\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(pid=43761)\u001b[0m WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "\u001b[2m\u001b[36m(pid=43761)\u001b[0m I0000 00:00:1746903521.033053   43761 fork_posix.cc:75] Other threads are currently calling into gRPC, skipping fork() handlers\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=43761)\u001b[0m /home/vladimir/.pyenv/versions/3.9.21/lib/python3.9/site-packages/gymnasium/spaces/box.py:227: UserWarning: \u001b[33mWARN: Casting input x to numpy array.\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=43761)\u001b[0m   logger.warn(\"Casting input x to numpy array.\")\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=43761)\u001b[0m 2025-05-10 18:58:42,740\tINFO eager_tf_policy_v2.py:75 -- Creating TF-eager policy running on CPU.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=43761)\u001b[0m 2025-05-10 18:58:42,755\tINFO policy.py:1214 -- Policy (worker=3) running on CPU.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=43761)\u001b[0m 2025-05-10 18:58:42,763\tINFO catalog.py:477 -- Wrapping <class 'graphenv.examples.tsp.tsp_nfp_model.TSPGNNModel'> as None\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=43760)\u001b[0m /home/vladimir/.pyenv/versions/3.9.21/lib/python3.9/site-packages/gymnasium/spaces/box.py:227: UserWarning: \u001b[33mWARN: Casting input x to numpy array.\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=43760)\u001b[0m   logger.warn(\"Casting input x to numpy array.\")\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=43759)\u001b[0m 2025-05-10 18:58:42,908\tWARNING env.py:156 -- Your env doesn't have a .spec.max_episode_steps attribute. Your horizon will default to infinity, and your environment will not be reset.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=43759)\u001b[0m /home/vladimir/.pyenv/versions/3.9.21/lib/python3.9/site-packages/gymnasium/spaces/box.py:227: UserWarning: \u001b[33mWARN: Casting input x to numpy array.\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=43759)\u001b[0m   logger.warn(\"Casting input x to numpy array.\")\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=43762)\u001b[0m /home/vladimir/.pyenv/versions/3.9.21/lib/python3.9/site-packages/gymnasium/spaces/box.py:227: UserWarning: \u001b[33mWARN: Casting input x to numpy array.\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=43762)\u001b[0m   logger.warn(\"Casting input x to numpy array.\")\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=43762)\u001b[0m 2025-05-10 18:58:42,919\tINFO eager_tf_policy_v2.py:75 -- Creating TF-eager policy running on CPU.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=43762)\u001b[0m 2025-05-10 18:58:42,934\tINFO policy.py:1214 -- Policy (worker=4) running on CPU.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=43762)\u001b[0m 2025-05-10 18:58:42,942\tINFO catalog.py:477 -- Wrapping <class 'graphenv.examples.tsp.tsp_nfp_model.TSPGNNModel'> as None\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=43760)\u001b[0m 2025-05-10 18:58:43,229\tINFO eager_tf_policy_v2.py:75 -- Creating TF-eager policy running on CPU.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=43759)\u001b[0m 2025-05-10 18:58:43,211\tINFO eager_tf_policy_v2.py:75 -- Creating TF-eager policy running on CPU.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=43759)\u001b[0m 2025-05-10 18:58:43,226\tINFO policy.py:1214 -- Policy (worker=1) running on CPU.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=43760)\u001b[0m 2025-05-10 18:58:43,244\tINFO policy.py:1214 -- Policy (worker=2) running on CPU.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=43760)\u001b[0m 2025-05-10 18:58:43,253\tINFO catalog.py:477 -- Wrapping <class 'graphenv.examples.tsp.tsp_nfp_model.TSPGNNModel'> as None\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=43759)\u001b[0m 2025-05-10 18:58:43,235\tINFO catalog.py:477 -- Wrapping <class 'graphenv.examples.tsp.tsp_nfp_model.TSPGNNModel'> as None\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=43761)\u001b[0m 2025-05-10 18:58:43,892\tINFO util.py:122 -- Using connectors:\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=43761)\u001b[0m 2025-05-10 18:58:43,892\tINFO util.py:123 --     AgentConnectorPipeline\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=43761)\u001b[0m         ObsPreprocessorConnector\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=43761)\u001b[0m         StateBufferConnector\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=43761)\u001b[0m         ViewRequirementAgentConnector\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=43761)\u001b[0m 2025-05-10 18:58:43,892\tINFO util.py:124 --     ActionConnectorPipeline\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=43761)\u001b[0m         ConvertToNumpyConnector\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=43761)\u001b[0m         NormalizeActionsConnector\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=43761)\u001b[0m         ImmutableActionsConnector\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=43762)\u001b[0m 2025-05-10 18:58:44,075\tINFO util.py:122 -- Using connectors:\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=43762)\u001b[0m 2025-05-10 18:58:44,075\tINFO util.py:123 --     AgentConnectorPipeline\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=43762)\u001b[0m         ObsPreprocessorConnector\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=43762)\u001b[0m         StateBufferConnector\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=43762)\u001b[0m         ViewRequirementAgentConnector\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=43762)\u001b[0m 2025-05-10 18:58:44,075\tINFO util.py:124 --     ActionConnectorPipeline\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=43762)\u001b[0m         ConvertToNumpyConnector\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=43762)\u001b[0m         NormalizeActionsConnector\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=43762)\u001b[0m         ImmutableActionsConnector\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=43760)\u001b[0m 2025-05-10 18:58:44,374\tINFO util.py:122 -- Using connectors:\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=43760)\u001b[0m 2025-05-10 18:58:44,374\tINFO util.py:123 --     AgentConnectorPipeline\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=43760)\u001b[0m         ObsPreprocessorConnector\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=43760)\u001b[0m         StateBufferConnector\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=43760)\u001b[0m         ViewRequirementAgentConnector\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=43760)\u001b[0m 2025-05-10 18:58:44,374\tINFO util.py:124 --     ActionConnectorPipeline\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=43760)\u001b[0m         ConvertToNumpyConnector\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=43760)\u001b[0m         NormalizeActionsConnector\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=43760)\u001b[0m         ImmutableActionsConnector\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=43759)\u001b[0m 2025-05-10 18:58:44,367\tINFO util.py:122 -- Using connectors:\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=43759)\u001b[0m 2025-05-10 18:58:44,367\tINFO util.py:123 --     AgentConnectorPipeline\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=43759)\u001b[0m         ObsPreprocessorConnector\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=43759)\u001b[0m         StateBufferConnector\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=43759)\u001b[0m         ViewRequirementAgentConnector\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=43759)\u001b[0m 2025-05-10 18:58:44,367\tINFO util.py:124 --     ActionConnectorPipeline\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=43759)\u001b[0m         ConvertToNumpyConnector\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=43759)\u001b[0m         NormalizeActionsConnector\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=43759)\u001b[0m         ImmutableActionsConnector\n"
     ]
    }
   ],
   "source": [
    "print(f\"Running with following CLI options: {args}\")\n",
    "logging.basicConfig(level=args.log_level.upper())\n",
    "ray.init(local_mode=args.local_mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7ac9d4e5-2477-461b-a467-4f0f457826fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = args.N\n",
    "G = make_complete_planar_graph(N=N, seed=args.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a9a56466-059f-45a7-ac36-aa4a93e27ea4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Networkx heuristic reward: -1.700\n",
      "[0, 4, 1, 3, 2, 0]\n",
      "Networkx greedy reward: -1.996\n",
      "[0, 2, 1, 3, 4, 0]\n"
     ]
    }
   ],
   "source": [
    "tsp_approx = nx.approximation.traveling_salesman_problem\n",
    "path = tsp_approx(G, cycle=True)\n",
    "reward_baseline = -sum([G[path[i]][path[i + 1]][\"weight\"] for i in range(0, N)])\n",
    "print(f\"Networkx heuristic reward: {reward_baseline:1.3f}\")\n",
    "print(path)\n",
    "path = tsp_approx(G, cycle=True, method=greedy_tsp)\n",
    "reward_baseline = -sum([G[path[i]][path[i + 1]][\"weight\"] for i in range(0, N)])\n",
    "print(f\"Networkx greedy reward: {reward_baseline:1.3f}\")\n",
    "print(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c508cb95-5a0b-49c9-bc4a-1aa49ab413ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Algorithm-specific config, common ones are in the main config dict below\n",
    "if args.run == \"PPO\":\n",
    "    run_config = PPOConfig()\n",
    "    train_batch_size = args.rollouts_per_worker * N * args.num_workers\n",
    "    sgd_minibatch_size = 16 if train_batch_size > 16 else 2\n",
    "    run_config.training(entropy_coeff=args.entropy_coeff,\n",
    "                        sgd_minibatch_size=sgd_minibatch_size,\n",
    "                        num_sgd_iter=5,\n",
    "    )\n",
    "elif args.run in [\"DQN\"]:\n",
    "    run_config = DQNConfig()\n",
    "    # Update here with custom config\n",
    "    run_config.training(hiddens=False,\n",
    "                    dueling=False\n",
    "    )\n",
    "    run_config.exploration(exploration_config={\"epsilon_timesteps\": 250000})\n",
    "elif args.run == \"A3C\":\n",
    "    run_config = A3CConfig()\n",
    "elif args.run == \"MARWIL\":\n",
    "    run_config = MARWILConfig()\n",
    "else:\n",
    "    raise ValueError(f\"Import agent {args.run} and try again\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "27e446a3-976f-46f0-899a-da10ab18e3a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "use_gnn\n"
     ]
    }
   ],
   "source": [
    "# Define custom_model, config, and state based on GNN yes/no\n",
    "if args.use_gnn:\n",
    "    custom_model = \"TSPGNNModel\"\n",
    "    custom_model_config = {\"num_messages\": 3, \"embed_dim\": 32}\n",
    "    print('use_gnn')\n",
    "    ModelCatalog.register_custom_model(custom_model, TSPGNNModel)\n",
    "    _tag = \"gnn\"\n",
    "    state = TSPNFPState(\n",
    "        lambda: G,\n",
    "        max_num_neighbors=args.max_num_neighbors,\n",
    "    )\n",
    "else:\n",
    "    custom_model_config = {\"hidden_dim\": 256, \"embed_dim\": 256, \"num_nodes\": N}\n",
    "    custom_model = \"TSPModel\"\n",
    "    Model = TSPQModel if args.run in [\"DQN\", \"R2D2\"] else TSPModel\n",
    "    ModelCatalog.register_custom_model(custom_model, Model)\n",
    "    _tag = f\"basic{args.run}\"\n",
    "    state = TSPState(lambda: G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "505c95ca-d686-4c5a-890f-cbd9a64531de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register env name with hyperparams that will help tracking experiments\n",
    "# via tensorboard\n",
    "env_name = f\"mygraphenv-v0\" #_{N}_{_tag}_lr={args.lr}\n",
    "register_env(env_name, lambda config: GraphEnv(config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e0d687e6-fe6f-498e-880e-fee875f2fd0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "args.num_gpus = 0\n",
    "args.num_workers = 4\n",
    "run_config = (\n",
    "    run_config\n",
    "    .resources(num_gpus=args.num_gpus) \n",
    "    .framework(\"tf2\") #tf ?\n",
    "    .rollouts(num_rollout_workers=args.num_workers, \n",
    "              # a multiple of N (collect whole episodes)\n",
    "              rollout_fragment_length=N\n",
    "             )\n",
    "    .environment(env=env_name,\n",
    "                 env_config={\"state\": state, \n",
    "                             \"max_num_children\": G.number_of_nodes()}\n",
    "              )\n",
    "    .training(lr=args.lr,\n",
    "              train_batch_size=args.rollouts_per_worker * N * args.num_workers,\n",
    "              model={\"custom_model\": custom_model, \n",
    "                     \"custom_model_config\": custom_model_config}\n",
    "              )\n",
    "    .evaluation(evaluation_config={\"explore\": False},\n",
    "                evaluation_interval=1, \n",
    "                evaluation_duration=100,\n",
    "              )\n",
    "    .debugging(log_level=args.log_level)\n",
    "    .framework(eager_tracing=True)\n",
    ")\n",
    "\n",
    "stop = {\n",
    "    \"training_iteration\": args.stop_iters,\n",
    "    \"timesteps_total\": args.stop_timesteps,\n",
    "    \"episode_reward_mean\": args.stop_reward,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d29b8901-6bec-42a3-a0f6-97358ab62ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_path = Path(\"/home/vladimir/work/graph_test/scratch/ray_results\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e7557961-396a-49cf-839c-6251fc2f7d07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2025-05-10 18:58:23</td></tr>\n",
       "<tr><td>Running for: </td><td>00:02:18.25        </td></tr>\n",
       "<tr><td>Memory:      </td><td>5.0/125.7 GiB      </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      Using FIFO scheduling algorithm.<br>Resources requested: 0/20 CPUs, 0/2 GPUs, 0.0/77.93 GiB heap, 0.0/37.39 GiB objects (0.0/1.0 accelerator_type:G)\n",
       "    </div>\n",
       "    \n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name                   </th><th>status    </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_mygraphenv-v0_6d1f5_00000</td><td>TERMINATED</td><td>192.168.0.126:42601</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">         124.098</td><td style=\"text-align: right;\">2000</td><td style=\"text-align: right;\">-1.71081</td><td style=\"text-align: right;\">            -1.58714</td><td style=\"text-align: right;\">            -1.94072</td><td style=\"text-align: right;\">                 5</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-10 18:56:05,584\tINFO algorithm_config.py:2888 -- Executing eagerly (framework='tf2'), with eager_tracing=tf2. For production workloads, make sure to set eager_tracing=True  in order to match the speed of tf-static-graph (framework='tf'). For debugging purposes, `eager_tracing=False` is the best choice.\n",
      "2025-05-10 18:56:05,585\tINFO algorithm_config.py:2888 -- Executing eagerly (framework='tf2'), with eager_tracing=tf2. For production workloads, make sure to set eager_tracing=True  in order to match the speed of tf-static-graph (framework='tf'). For debugging purposes, `eager_tracing=False` is the best choice.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"trialProgress\">\n",
       "  <h3>Trial Progress</h3>\n",
       "  <table>\n",
       "<thead>\n",
       "<tr><th>Trial name                   </th><th style=\"text-align: right;\">  agent_timesteps_total</th><th>connector_metrics                                                                                                                                             </th><th>counters                                                                                                                        </th><th>custom_metrics  </th><th>date               </th><th>done  </th><th style=\"text-align: right;\">  episode_len_mean</th><th>episode_media  </th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_mean</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episodes_this_iter</th><th style=\"text-align: right;\">  episodes_total</th><th>evaluation                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    </th><th>experiment_id                   </th><th>hostname  </th><th>info                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    </th><th style=\"text-align: right;\">  iterations_since_restore</th><th>node_ip      </th><th style=\"text-align: right;\">  num_agent_steps_sampled</th><th style=\"text-align: right;\">  num_agent_steps_trained</th><th style=\"text-align: right;\">  num_env_steps_sampled</th><th style=\"text-align: right;\">  num_env_steps_sampled_this_iter</th><th style=\"text-align: right;\">  num_env_steps_trained</th><th style=\"text-align: right;\">  num_env_steps_trained_this_iter</th><th style=\"text-align: right;\">  num_faulty_episodes</th><th style=\"text-align: right;\">  num_healthy_workers</th><th style=\"text-align: right;\">  num_in_flight_async_reqs</th><th style=\"text-align: right;\">  num_remote_worker_restarts</th><th style=\"text-align: right;\">  num_steps_trained_this_iter</th><th>perf                                               </th><th style=\"text-align: right;\">  pid</th><th>policy_reward_max  </th><th>policy_reward_mean  </th><th>policy_reward_min  </th><th>sampler_perf                                                                                                                                                                                                    </th><th>sampler_results                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       </th><th style=\"text-align: right;\">  time_since_restore</th><th style=\"text-align: right;\">  time_this_iter_s</th><th style=\"text-align: right;\">  time_total_s</th><th>timers                                                                                                                      </th><th style=\"text-align: right;\">  timestamp</th><th style=\"text-align: right;\">  timesteps_since_restore</th><th style=\"text-align: right;\">  timesteps_total</th><th style=\"text-align: right;\">  training_iteration</th><th>trial_id   </th><th style=\"text-align: right;\">  warmup_time</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_mygraphenv-v0_6d1f5_00000</td><td style=\"text-align: right;\">                   2000</td><td>{&#x27;ObsPreprocessorConnector_ms&#x27;: 0.05004119873046875, &#x27;StateBufferConnector_ms&#x27;: 0.007983207702636719, &#x27;ViewRequirementAgentConnector_ms&#x27;: 0.10598039627075195}</td><td>{&#x27;num_env_steps_sampled&#x27;: 2000, &#x27;num_env_steps_trained&#x27;: 2000, &#x27;num_agent_steps_sampled&#x27;: 2000, &#x27;num_agent_steps_trained&#x27;: 2000}</td><td>{}              </td><td>2025-05-10_18-58-23</td><td>True  </td><td style=\"text-align: right;\">                 5</td><td>{}             </td><td style=\"text-align: right;\">            -1.58714</td><td style=\"text-align: right;\">             -1.71081</td><td style=\"text-align: right;\">            -1.94072</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">             400</td><td>{&#x27;episode_reward_max&#x27;: -1.5871424799869809, &#x27;episode_reward_min&#x27;: -1.5871424799869809, &#x27;episode_reward_mean&#x27;: -1.587142479986981, &#x27;episode_len_mean&#x27;: 5.0, &#x27;episode_media&#x27;: {}, &#x27;episodes_this_iter&#x27;: 100, &#x27;policy_reward_min&#x27;: {}, &#x27;policy_reward_max&#x27;: {}, &#x27;policy_reward_mean&#x27;: {}, &#x27;custom_metrics&#x27;: {}, &#x27;hist_stats&#x27;: {&#x27;episode_reward&#x27;: [-1.5871424799869809, -1.5871424799869809, -1.5871424799869809, -1.5871424799869809, -1.5871424799869809, -1.5871424799869809, -1.5871424799869809, -1.5871424799869809, -1.5871424799869809, -1.5871424799869809, -1.5871424799869809, -1.5871424799869809, -1.5871424799869809, -1.5871424799869809, -1.5871424799869809, -1.5871424799869809, -1.5871424799869809, -1.5871424799869809, -1.5871424799869809, -1.5871424799869809, -1.5871424799869809, -1.5871424799869809, -1.5871424799869809, -1.5871424799869809, -1.5871424799869809, -1.5871424799869809, -1.5871424799869809, -1.5871424799869809, -1.5871424799869809, -1.5871424799869809, -1.5871424799869809, -1.5871424799869809, -1.5871424799869809, -1.5871424799869809, -1.5871424799869809, -1.5871424799869809, -1.5871424799869809, -1.5871424799869809, -1.5871424799869809, -1.5871424799869809, -1.5871424799869809, -1.5871424799869809, -1.5871424799869809, -1.5871424799869809, -1.5871424799869809, -1.5871424799869809, -1.5871424799869809, -1.5871424799869809, -1.5871424799869809, -1.5871424799869809, -1.5871424799869809, -1.5871424799869809, -1.5871424799869809, -1.5871424799869809, -1.5871424799869809, -1.5871424799869809, -1.5871424799869809, -1.5871424799869809, -1.5871424799869809, -1.5871424799869809, -1.5871424799869809, -1.5871424799869809, -1.5871424799869809, -1.5871424799869809, -1.5871424799869809, -1.5871424799869809, -1.5871424799869809, -1.5871424799869809, -1.5871424799869809, -1.5871424799869809, -1.5871424799869809, -1.5871424799869809, -1.5871424799869809, -1.5871424799869809, -1.5871424799869809, -1.5871424799869809, -1.5871424799869809, -1.5871424799869809, -1.5871424799869809, -1.5871424799869809, -1.5871424799869809, -1.5871424799869809, -1.5871424799869809, -1.5871424799869809, -1.5871424799869809, -1.5871424799869809, -1.5871424799869809, -1.5871424799869809, -1.5871424799869809, -1.5871424799869809, -1.5871424799869809, -1.5871424799869809, -1.5871424799869809, -1.5871424799869809, -1.5871424799869809, -1.5871424799869809, -1.5871424799869809, -1.5871424799869809, -1.5871424799869809, -1.5871424799869809], &#x27;episode_lengths&#x27;: [5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5]}, &#x27;sampler_perf&#x27;: {&#x27;mean_raw_obs_processing_ms&#x27;: 0.4983578717021242, &#x27;mean_inference_ms&#x27;: 1.2904544708330687, &#x27;mean_action_processing_ms&#x27;: 0.08943228670979561, &#x27;mean_env_wait_ms&#x27;: 0.06638082303356374, &#x27;mean_env_render_ms&#x27;: 0.0}, &#x27;num_faulty_episodes&#x27;: 0, &#x27;connector_metrics&#x27;: {&#x27;ObsPreprocessorConnector_ms&#x27;: 0.04060626029968262, &#x27;StateBufferConnector_ms&#x27;: 0.00610804557800293, &#x27;ViewRequirementAgentConnector_ms&#x27;: 0.08330035209655762}, &#x27;num_agent_steps_sampled_this_iter&#x27;: 500, &#x27;num_env_steps_sampled_this_iter&#x27;: 500, &#x27;timesteps_this_iter&#x27;: 500, &#x27;num_healthy_workers&#x27;: 0, &#x27;num_in_flight_async_reqs&#x27;: 0, &#x27;num_remote_worker_restarts&#x27;: 0}</td><td>a7ceabf2b41c4819b8a6fc26ffe882af</td><td>srv5      </td><td>{&#x27;learner&#x27;: {&#x27;default_policy&#x27;: {&#x27;learner_stats&#x27;: {&#x27;cur_kl_coeff&#x27;: 1.0463794097859136e-07, &#x27;cur_lr&#x27;: 9.999999747378752e-05, &#x27;total_loss&#x27;: 0.1437203, &#x27;policy_loss&#x27;: 7.5178745e-05, &#x27;vf_loss&#x27;: 0.14364511, &#x27;vf_explained_var&#x27;: 0.31599823, &#x27;kl&#x27;: 0.0024756247, &#x27;entropy&#x27;: 0.23174913, &#x27;entropy_coeff&#x27;: 0.0}, &#x27;custom_metrics&#x27;: {}, &#x27;num_agent_steps_trained&#x27;: 2.0, &#x27;num_grad_updates_lifetime&#x27;: 4975.5, &#x27;diff_num_grad_updates_vs_sampler_policy&#x27;: 24.5}}, &#x27;num_env_steps_sampled&#x27;: 2000, &#x27;num_env_steps_trained&#x27;: 2000, &#x27;num_agent_steps_sampled&#x27;: 2000, &#x27;num_agent_steps_trained&#x27;: 2000}</td><td style=\"text-align: right;\">                       100</td><td>192.168.0.126</td><td style=\"text-align: right;\">                     2000</td><td style=\"text-align: right;\">                     2000</td><td style=\"text-align: right;\">                   2000</td><td style=\"text-align: right;\">                               20</td><td style=\"text-align: right;\">                   2000</td><td style=\"text-align: right;\">                               20</td><td style=\"text-align: right;\">                    0</td><td style=\"text-align: right;\">                    4</td><td style=\"text-align: right;\">                         0</td><td style=\"text-align: right;\">                           0</td><td style=\"text-align: right;\">                           20</td><td>{&#x27;cpu_util_percent&#x27;: 6.85, &#x27;ram_util_percent&#x27;: 4.0}</td><td style=\"text-align: right;\">42601</td><td>{}                 </td><td>{}                  </td><td>{}                 </td><td>{&#x27;mean_raw_obs_processing_ms&#x27;: 0.6515747097674068, &#x27;mean_inference_ms&#x27;: 3.7383319936540165, &#x27;mean_action_processing_ms&#x27;: 0.1107845738046373, &#x27;mean_env_wait_ms&#x27;: 0.08886962517409602, &#x27;mean_env_render_ms&#x27;: 0.0}</td><td>{&#x27;episode_reward_max&#x27;: -1.5871424799869809, &#x27;episode_reward_min&#x27;: -1.9407205546191035, &#x27;episode_reward_mean&#x27;: -1.7108066392581531, &#x27;episode_len_mean&#x27;: 5.0, &#x27;episode_media&#x27;: {}, &#x27;episodes_this_iter&#x27;: 4, &#x27;policy_reward_min&#x27;: {}, &#x27;policy_reward_max&#x27;: {}, &#x27;policy_reward_mean&#x27;: {}, &#x27;custom_metrics&#x27;: {}, &#x27;hist_stats&#x27;: {&#x27;episode_reward&#x27;: [-1.893276848138505, -1.7000894302075489, -1.893276848138505, -1.7000894302075489, -1.7000894302075489, -1.7000894302075489, -1.7000894302075489, -1.7000894302075489, -1.893276848138505, -1.893276848138505, -1.7000894302075489, -1.9407205546191035, -1.6842902103837378, -1.6842902103837378, -1.6842902103837378, -1.893276848138505, -1.5871424799869809, -1.6842902103837378, -1.6842902103837378, -1.7000894302075489, -1.5871424799869809, -1.7000894302075489, -1.7000894302075489, -1.5871424799869809, -1.7000894302075489, -1.893276848138505, -1.893276848138505, -1.7000894302075489, -1.5871424799869809, -1.7000894302075489, -1.893276848138505, -1.9407205546191035, -1.7000894302075489, -1.924921334795293, -1.7000894302075489, -1.893276848138505, -1.7000894302075489, -1.7000894302075489, -1.7000894302075489, -1.7000894302075489, -1.7000894302075489, -1.7000894302075489, -1.7000894302075489, -1.9407205546191035, -1.893276848138505, -1.7000894302075489, -1.893276848138505, -1.5871424799869809, -1.7000894302075489, -1.893276848138505, -1.7000894302075489, -1.7000894302075489, -1.7000894302075489, -1.5871424799869809, -1.7000894302075489, -1.7000894302075489, -1.5871424799869809, -1.7000894302075489, -1.7000894302075489, -1.893276848138505, -1.7000894302075489, -1.5871424799869809, -1.7000894302075489, -1.7000894302075489, -1.7000894302075489, -1.7000894302075489, -1.7000894302075489, -1.7000894302075489, -1.7000894302075489, -1.893276848138505, -1.5871424799869809, -1.5871424799869809, -1.5871424799869809, -1.7000894302075489, -1.5871424799869809, -1.6842902103837378, -1.7000894302075489, -1.7000894302075489, -1.7000894302075489, -1.7000894302075489, -1.5871424799869809, -1.7000894302075489, -1.7000894302075489, -1.5871424799869809, -1.7000894302075489, -1.5871424799869809, -1.7000894302075489, -1.5871424799869809, -1.5871424799869809, -1.7000894302075489, -1.5871424799869809, -1.7000894302075489, -1.7000894302075489, -1.5871424799869809, -1.5871424799869809, -1.7000894302075489, -1.7000894302075489, -1.5871424799869809, -1.7000894302075489, -1.5871424799869809], &#x27;episode_lengths&#x27;: [5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5]}, &#x27;sampler_perf&#x27;: {&#x27;mean_raw_obs_processing_ms&#x27;: 0.6515747097674068, &#x27;mean_inference_ms&#x27;: 3.7383319936540165, &#x27;mean_action_processing_ms&#x27;: 0.1107845738046373, &#x27;mean_env_wait_ms&#x27;: 0.08886962517409602, &#x27;mean_env_render_ms&#x27;: 0.0}, &#x27;num_faulty_episodes&#x27;: 0, &#x27;connector_metrics&#x27;: {&#x27;ObsPreprocessorConnector_ms&#x27;: 0.05004119873046875, &#x27;StateBufferConnector_ms&#x27;: 0.007983207702636719, &#x27;ViewRequirementAgentConnector_ms&#x27;: 0.10598039627075195}}</td><td style=\"text-align: right;\">             124.098</td><td style=\"text-align: right;\">           1.17975</td><td style=\"text-align: right;\">       124.098</td><td>{&#x27;training_iteration_time_ms&#x27;: 186.545, &#x27;learn_time_ms&#x27;: 162.604, &#x27;learn_throughput&#x27;: 122.998, &#x27;synch_weights_time_ms&#x27;: 7.3}</td><td style=\"text-align: right;\"> 1746903503</td><td style=\"text-align: right;\">                        0</td><td style=\"text-align: right;\">             2000</td><td style=\"text-align: right;\">                 100</td><td>6d1f5_00000</td><td style=\"text-align: right;\">      8.70584</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "</div>\n",
       "<style>\n",
       ".trialProgress {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".trialProgress h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".trialProgress td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-10 18:58:24,159\tINFO tune.py:798 -- Total run time: 138.66 seconds (138.21 seconds for the tuning loop).\n"
     ]
    }
   ],
   "source": [
    "res = tune.run(\n",
    "    args.run,\n",
    "    config=run_config.to_dict(),\n",
    "    stop=stop,\n",
    "    local_dir=my_path,\n",
    "    checkpoint_freq = 10,\n",
    "    checkpoint_at_end=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4eae4fe9-c52d-4451-884b-7bed2662d361",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-10 18:58:24,187\tINFO experiment_analysis.py:789 -- No `self.trials`. Drawing logdirs from checkpoint file. This may result in some information that is out of sync, as checkpointing is periodic.\n"
     ]
    }
   ],
   "source": [
    "#  \n",
    "analysis = ExperimentAnalysis(my_path)\n",
    "best_trial = analysis.get_best_trial(metric=\"episode_reward_mean\", mode=\"max\")\n",
    "best_checkpoint = analysis.get_best_checkpoint(best_trial, mode = \"max\")\n",
    "#best_checkpoint = analysis.get_last_checkpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f642ad15-8584-4449-a466-7bbbedec047e",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = GraphEnv({\"state\": state, \n",
    "          \"max_num_children\": G.number_of_nodes()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4f6df3ee-a3a5-42e8-a2c5-cc619b021e77",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-10 18:58:37,865\tWARNING algorithm_config.py:596 -- Cannot create PPOConfig from given `config_dict`! Property __stdout_file__ not supported.\n",
      "2025-05-10 18:58:39,865\tINFO algorithm_config.py:2888 -- Executing eagerly (framework='tf2'), with eager_tracing=tf2. For production workloads, make sure to set eager_tracing=True  in order to match the speed of tf-static-graph (framework='tf'). For debugging purposes, `eager_tracing=False` is the best choice.\n",
      "2025-05-10 18:58:44,379\tINFO worker_set.py:310 -- Inferred observation/action spaces from remote worker (local worker has no env): {'default_policy': (Repeated(Dict('connectivity': Box(0, 5, (20, 2), int64), 'current_node': Box(0, 5, (), int64), 'distance': Box(0.0, 1.4142135623730951, (), float64), 'edge_weights': Box(0.0, 1.4142135623730951, (20,), float64), 'node_visited': Box(0, 2, (5,), int64)), 6), Discrete(5)), '__env__': (Repeated(Dict('connectivity': Box(0, 5, (20, 2), int64), 'current_node': Box(0, 5, (), int64), 'distance': Box(0.0, 1.4142135623730951, (), float64), 'edge_weights': Box(0.0, 1.4142135623730951, (20,), float64), 'node_visited': Box(0, 2, (5,), int64)), 6), Discrete(5))}\n",
      "2025-05-10 18:58:44,384\tINFO eager_tf_policy_v2.py:75 -- Creating TF-eager policy running on GPU.\n",
      "2025-05-10 18:58:44,387\tINFO policy.py:1214 -- Policy (worker=local) running on CPU.\n",
      "2025-05-10 18:58:44,388\tINFO catalog.py:477 -- Wrapping <class 'graphenv.examples.tsp.tsp_nfp_model.TSPGNNModel'> as None\n",
      "2025-05-10 18:58:46,025\tINFO util.py:122 -- Using connectors:\n",
      "2025-05-10 18:58:46,025\tINFO util.py:123 --     AgentConnectorPipeline\n",
      "        ObsPreprocessorConnector\n",
      "        StateBufferConnector\n",
      "        ViewRequirementAgentConnector\n",
      "2025-05-10 18:58:46,026\tINFO util.py:124 --     ActionConnectorPipeline\n",
      "        ConvertToNumpyConnector\n",
      "        NormalizeActionsConnector\n",
      "        ImmutableActionsConnector\n",
      "2025-05-10 18:58:46,026\tINFO rollout_worker.py:2043 -- Built policy map: <PolicyMap lru-caching-capacity=100 policy-IDs=['default_policy']>\n",
      "2025-05-10 18:58:46,026\tINFO rollout_worker.py:2044 -- Built preprocessor map: {'default_policy': None}\n",
      "2025-05-10 18:58:46,027\tINFO rollout_worker.py:760 -- Built filter map: defaultdict(<class 'ray.rllib.utils.filter.NoFilter'>, {})\n",
      "2025-05-10 18:58:46,037\tINFO algorithm_config.py:2888 -- Executing eagerly (framework='tf2'), with eager_tracing=tf2. For production workloads, make sure to set eager_tracing=True  in order to match the speed of tf-static-graph (framework='tf'). For debugging purposes, `eager_tracing=False` is the best choice.\n",
      "2025-05-10 18:58:46,042\tWARNING env.py:156 -- Your env doesn't have a .spec.max_episode_steps attribute. Your horizon will default to infinity, and your environment will not be reset.\n",
      "/home/vladimir/.pyenv/versions/3.9.21/lib/python3.9/site-packages/gymnasium/spaces/box.py:227: UserWarning: \u001b[33mWARN: Casting input x to numpy array.\u001b[0m\n",
      "  logger.warn(\"Casting input x to numpy array.\")\n",
      "2025-05-10 18:58:46,048\tINFO eager_tf_policy_v2.py:75 -- Creating TF-eager policy running on GPU.\n",
      "2025-05-10 18:58:46,050\tINFO policy.py:1214 -- Policy (worker=local) running on CPU.\n",
      "2025-05-10 18:58:46,051\tINFO catalog.py:477 -- Wrapping <class 'graphenv.examples.tsp.tsp_nfp_model.TSPGNNModel'> as None\n",
      "2025-05-10 18:58:46,606\tINFO util.py:122 -- Using connectors:\n",
      "2025-05-10 18:58:46,606\tINFO util.py:123 --     AgentConnectorPipeline\n",
      "        ObsPreprocessorConnector\n",
      "        StateBufferConnector\n",
      "        ViewRequirementAgentConnector\n",
      "2025-05-10 18:58:46,607\tINFO util.py:124 --     ActionConnectorPipeline\n",
      "        ConvertToNumpyConnector\n",
      "        NormalizeActionsConnector\n",
      "        ImmutableActionsConnector\n",
      "2025-05-10 18:58:46,607\tINFO rollout_worker.py:2043 -- Built policy map: <PolicyMap lru-caching-capacity=100 policy-IDs=['default_policy']>\n",
      "2025-05-10 18:58:46,607\tINFO rollout_worker.py:2044 -- Built preprocessor map: {'default_policy': None}\n",
      "2025-05-10 18:58:46,608\tINFO rollout_worker.py:760 -- Built filter map: defaultdict(<class 'ray.rllib.utils.filter.NoFilter'>, {})\n",
      "2025-05-10 18:58:46,609\tWARNING util.py:67 -- Install gputil for GPU system monitoring.\n"
     ]
    }
   ],
   "source": [
    "algo = Algorithm.from_checkpoint(best_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dd17c22a-08e2-46d0-b34b-731bf9245e93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0, 1, 2, 3, 4}\n",
      "0 3 -0.5311840924120236 0\n",
      "1 0 -0.3953628417186544 4\n",
      "2 0 -0.20562852490057235 1\n",
      "3 0 -0.24627330250392146 2\n",
      "4 0 -0.20869371845180915 3\n",
      "-1.5871424799869809 [0, 4, 1, 2, 3]\n"
     ]
    }
   ],
   "source": [
    "episode_reward = 0\n",
    "terminated = truncated = False\n",
    "obs, info = env.reset(G=G)\n",
    "i = 0\n",
    "path = [] #[obs[0]['node_idx'][0]]\n",
    "nn = {ob['current_node'] for ob in obs}\n",
    "print(nn)\n",
    "while not terminated and not truncated and i < 20: \n",
    "    action = algo.compute_single_action(obs, explore = False)\n",
    "    obs, reward, terminated, truncated, info = env.step(action)\n",
    "    nn_new = {ob['current_node'] for ob in obs}\n",
    "    cc = list(nn - nn_new)[0]\n",
    "    print(i, action, reward, cc)\n",
    "    episode_reward += reward\n",
    "    path.append(cc)\n",
    " #   path.append(obs[0]['node_idx'][0])\n",
    "    i += 1\n",
    "    nn = nn_new\n",
    "print( episode_reward, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b83d782c-3653-4a66-89e3-b21920905ce9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 4, 1, 2, 3, 0]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.state.tour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdbb10bf-5944-4e51-932e-1977cb5b792a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from python_tsp.exact import solve_tsp_dynamic_programming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15730e9f-c600-4415-9d7b-4c98e0be896c",
   "metadata": {},
   "outputs": [],
   "source": [
    "solve_tsp_dynamic_programming(nx.to_numpy_array(G))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa3f9e74-c93a-4410-a623-1f6f40f394d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0359244b-4de2-4607-be46-5a3a6fad9fe5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

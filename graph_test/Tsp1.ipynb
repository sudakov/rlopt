{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ecce1ac2-f2f3-4dda-9c18-344393964b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import logging\n",
    "import os\n",
    "from pathlib import Path\n",
    "import ray\n",
    "from graphenv.examples.tsp.graph_utils import make_complete_planar_graph\n",
    "from graphenv.examples.tsp.tsp_model import TSPModel, TSPQModel\n",
    "from graphenv.examples.tsp.tsp_nfp_model import TSPGNNModel\n",
    "from graphenv.examples.tsp.tsp_nfp_state import TSPNFPState\n",
    "from graphenv.examples.tsp.tsp_state import TSPState\n",
    "from graphenv.graph_env import GraphEnv\n",
    "from networkx.algorithms.approximation.traveling_salesman import greedy_tsp\n",
    "from ray import tune\n",
    "from ray.rllib.algorithms.a3c import A3CConfig\n",
    "from ray.rllib.algorithms.dqn import DQNConfig\n",
    "from ray.rllib.algorithms.marwil import MARWILConfig\n",
    "from ray.rllib.algorithms.ppo import PPOConfig\n",
    "from ray.rllib.models import ModelCatalog\n",
    "from ray.rllib.utils.framework import try_import_tf\n",
    "from ray.tune.registry import register_env\n",
    "import networkx as nx\n",
    "from ray.rllib.algorithms.algorithm import Algorithm\n",
    "from ray.tune import ExperimentAnalysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf5c0c65-0fe9-4515-a05a-f769f828f36a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_StoreAction(option_strings=['--log-level'], dest='log_level', nargs=None, const=None, default='INFO', type=<class 'str'>, choices=None, required=False, help=None, metavar=None)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf1, tf, tfv = try_import_tf()\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\n",
    "    \"--run\",\n",
    "    type=str,\n",
    "    default=\"PPO\",\n",
    "    choices=[\"PPO\", \"DQN\", \"A3C\", \"MARWIL\"],\n",
    "    help=\"The RLlib-registered algorithm to use.\",\n",
    ")\n",
    "parser.add_argument(\"--N\", type=int, default=5, help=\"Number of nodes in TSP network\")\n",
    "parser.add_argument(\n",
    "    \"--use-gnn\", action=\"store_true\", help=\"use the nfp state and gnn model\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--max-num-neighbors\",\n",
    "    type=int,\n",
    "    default=5,\n",
    "    help=\"Number of nearest neighbors for the gnn model\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--seed\", type=int, default=0, help=\"Random seed used to generate networkx graph\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--num-workers\", type=int, default=1, help=\"Number of rllib workers\"\n",
    ")\n",
    "parser.add_argument(\"--num-gpus\", type=int, default=0, help=\"Number of GPUs\")\n",
    "parser.add_argument(\"--lr\", type=float, default=1e-4, help=\"learning rate\")\n",
    "parser.add_argument(\n",
    "    \"--entropy-coeff\", type=float, default=0.0, help=\"entropy coefficient\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--rollouts-per-worker\",\n",
    "    type=int,\n",
    "    default=1,\n",
    "    help=\"Number of rollouts for each worker to collect\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--stop-iters\", type=int, default=100, help=\"Number of iterations to train.\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--stop-timesteps\", type=int, default=100000, help=\"Number of timesteps to train.\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--stop-reward\", type=float, default=0.0, help=\"Reward at which we stop training.\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--local-mode\",\n",
    "    action=\"store_true\",\n",
    "    help=\"Init Ray in local mode for easier debugging.\",\n",
    ")\n",
    "parser.add_argument(\"--log-level\", type=str, default=\"INFO\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3a8c3210-76d3-4fd4-b291-d629a5fb818c",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = parser.parse_args(args=[])\n",
    "args.use_gnn=True\n",
    "#args.num_gpus=0\n",
    "#args.num_workers = 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "296a6bf2-92f8-4061-8447-12f29cb46b08",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running with following CLI options: Namespace(run='PPO', N=5, use_gnn=True, max_num_neighbors=5, seed=0, num_workers=1, num_gpus=0, lr=0.0001, entropy_coeff=0.0, rollouts_per_worker=1, stop_iters=100, stop_timesteps=100000, stop_reward=0.0, local_mode=False, log_level='INFO')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-10 16:28:42,354\tINFO worker.py:1553 -- Started a local Ray instance.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "    <div style=\"margin-left: 50px;display: flex;flex-direction: row;align-items: center\">\n",
       "        <h3 style=\"color: var(--jp-ui-font-color0)\">Ray</h3>\n",
       "        <svg version=\"1.1\" id=\"ray\" width=\"3em\" viewBox=\"0 0 144.5 144.6\" style=\"margin-left: 3em;margin-right: 3em\">\n",
       "            <g id=\"layer-1\">\n",
       "                <path fill=\"#00a2e9\" class=\"st0\" d=\"M97.3,77.2c-3.8-1.1-6.2,0.9-8.3,5.1c-3.5,6.8-9.9,9.9-17.4,9.6S58,88.1,54.8,81.2c-1.4-3-3-4-6.3-4.1\n",
       "                    c-5.6-0.1-9.9,0.1-13.1,6.4c-3.8,7.6-13.6,10.2-21.8,7.6C5.2,88.4-0.4,80.5,0,71.7c0.1-8.4,5.7-15.8,13.8-18.2\n",
       "                    c8.4-2.6,17.5,0.7,22.3,8c1.3,1.9,1.3,5.2,3.6,5.6c3.9,0.6,8,0.2,12,0.2c1.8,0,1.9-1.6,2.4-2.8c3.5-7.8,9.7-11.8,18-11.9\n",
       "                    c8.2-0.1,14.4,3.9,17.8,11.4c1.3,2.8,2.9,3.6,5.7,3.3c1-0.1,2,0.1,3,0c2.8-0.5,6.4,1.7,8.1-2.7s-2.3-5.5-4.1-7.5\n",
       "                    c-5.1-5.7-10.9-10.8-16.1-16.3C84,38,81.9,37.1,78,38.3C66.7,42,56.2,35.7,53,24.1C50.3,14,57.3,2.8,67.7,0.5\n",
       "                    C78.4-2,89,4.7,91.5,15.3c0.1,0.3,0.1,0.5,0.2,0.8c0.7,3.4,0.7,6.9-0.8,9.8c-1.7,3.2-0.8,5,1.5,7.2c6.7,6.5,13.3,13,19.8,19.7\n",
       "                    c1.8,1.8,3,2.1,5.5,1.2c9.1-3.4,17.9-0.6,23.4,7c4.8,6.9,4.6,16.1-0.4,22.9c-5.4,7.2-14.2,9.9-23.1,6.5c-2.3-0.9-3.5-0.6-5.1,1.1\n",
       "                    c-6.7,6.9-13.6,13.7-20.5,20.4c-1.8,1.8-2.5,3.2-1.4,5.9c3.5,8.7,0.3,18.6-7.7,23.6c-7.9,5-18.2,3.8-24.8-2.9\n",
       "                    c-6.4-6.4-7.4-16.2-2.5-24.3c4.9-7.8,14.5-11,23.1-7.8c3,1.1,4.7,0.5,6.9-1.7C91.7,98.4,98,92.3,104.2,86c1.6-1.6,4.1-2.7,2.6-6.2\n",
       "                    c-1.4-3.3-3.8-2.5-6.2-2.6C99.8,77.2,98.9,77.2,97.3,77.2z M72.1,29.7c5.5,0.1,9.9-4.3,10-9.8c0-0.1,0-0.2,0-0.3\n",
       "                    C81.8,14,77,9.8,71.5,10.2c-5,0.3-9,4.2-9.3,9.2c-0.2,5.5,4,10.1,9.5,10.3C71.8,29.7,72,29.7,72.1,29.7z M72.3,62.3\n",
       "                    c-5.4-0.1-9.9,4.2-10.1,9.7c0,0.2,0,0.3,0,0.5c0.2,5.4,4.5,9.7,9.9,10c5.1,0.1,9.9-4.7,10.1-9.8c0.2-5.5-4-10-9.5-10.3\n",
       "                    C72.6,62.3,72.4,62.3,72.3,62.3z M115,72.5c0.1,5.4,4.5,9.7,9.8,9.9c5.6-0.2,10-4.8,10-10.4c-0.2-5.4-4.6-9.7-10-9.7\n",
       "                    c-5.3-0.1-9.8,4.2-9.9,9.5C115,72.1,115,72.3,115,72.5z M19.5,62.3c-5.4,0.1-9.8,4.4-10,9.8c-0.1,5.1,5.2,10.4,10.2,10.3\n",
       "                    c5.6-0.2,10-4.9,9.8-10.5c-0.1-5.4-4.5-9.7-9.9-9.6C19.6,62.3,19.5,62.3,19.5,62.3z M71.8,134.6c5.9,0.2,10.3-3.9,10.4-9.6\n",
       "                    c0.5-5.5-3.6-10.4-9.1-10.8c-5.5-0.5-10.4,3.6-10.8,9.1c0,0.5,0,0.9,0,1.4c-0.2,5.3,4,9.8,9.3,10\n",
       "                    C71.6,134.6,71.7,134.6,71.8,134.6z\"/>\n",
       "            </g>\n",
       "        </svg>\n",
       "        <table>\n",
       "            <tr>\n",
       "                <td style=\"text-align: left\"><b>Python version:</b></td>\n",
       "                <td style=\"text-align: left\"><b>3.9.21</b></td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                <td style=\"text-align: left\"><b>Ray version:</b></td>\n",
       "                <td style=\"text-align: left\"><b> 2.3.1</b></td>\n",
       "            </tr>\n",
       "            \n",
       "        </table>\n",
       "    </div>\n",
       "</div>\n"
      ],
      "text/plain": [
       "RayContext(dashboard_url='', python_version='3.9.21', ray_version='2.3.1', ray_commit='5f14cee8dfc6d61ec4fd3bc2c440f9944e92b33a', address_info={'node_ip_address': '192.168.0.126', 'raylet_ip_address': '192.168.0.126', 'redis_address': None, 'object_store_address': '/tmp/ray/session_2025-05-10_16-28-40_649795_34058/sockets/plasma_store', 'raylet_socket_name': '/tmp/ray/session_2025-05-10_16-28-40_649795_34058/sockets/raylet', 'webui_url': '', 'session_dir': '/tmp/ray/session_2025-05-10_16-28-40_649795_34058', 'metrics_export_port': 61692, 'gcs_address': '192.168.0.126:64958', 'address': '192.168.0.126:64958', 'dashboard_agent_listen_port': 52365, 'node_id': 'bafd3f65df92acedf1c1b311c8e3d08d6c53301e6e47a54866c2caa1'})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=35792)\u001b[0m WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "\u001b[2m\u001b[36m(pid=35792)\u001b[0m I0000 00:00:1746894525.039553   35792 fork_posix.cc:75] Other threads are currently calling into gRPC, skipping fork() handlers\n",
      "\u001b[2m\u001b[36m(PPO pid=35792)\u001b[0m 2025-05-10 16:28:46,662\tWARNING algorithm_config.py:596 -- Cannot create PPOConfig from given `config_dict`! Property __stdout_file__ not supported.\n",
      "\u001b[2m\u001b[36m(PPO pid=35792)\u001b[0m 2025-05-10 16:28:46,662\tINFO algorithm_config.py:2888 -- Executing eagerly (framework='tf2'), with eager_tracing=tf2. For production workloads, make sure to set eager_tracing=True  in order to match the speed of tf-static-graph (framework='tf'). For debugging purposes, `eager_tracing=False` is the best choice.\n",
      "\u001b[2m\u001b[36m(pid=35926)\u001b[0m WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "\u001b[2m\u001b[36m(pid=35926)\u001b[0m I0000 00:00:1746894528.388408   35926 fork_posix.cc:75] Other threads are currently calling into gRPC, skipping fork() handlers\n",
      "\u001b[2m\u001b[36m(pid=35925)\u001b[0m WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "\u001b[2m\u001b[36m(pid=35925)\u001b[0m I0000 00:00:1746894528.392534   35925 fork_posix.cc:75] Other threads are currently calling into gRPC, skipping fork() handlers\n",
      "\u001b[2m\u001b[36m(pid=35923)\u001b[0m WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "\u001b[2m\u001b[36m(pid=35923)\u001b[0m I0000 00:00:1746894528.417367   35923 fork_posix.cc:75] Other threads are currently calling into gRPC, skipping fork() handlers\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=35926)\u001b[0m /home/vladimir/.pyenv/versions/3.9.21/lib/python3.9/site-packages/gymnasium/spaces/box.py:227: UserWarning: \u001b[33mWARN: Casting input x to numpy array.\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=35926)\u001b[0m   logger.warn(\"Casting input x to numpy array.\")\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=35925)\u001b[0m /home/vladimir/.pyenv/versions/3.9.21/lib/python3.9/site-packages/gymnasium/spaces/box.py:227: UserWarning: \u001b[33mWARN: Casting input x to numpy array.\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=35925)\u001b[0m   logger.warn(\"Casting input x to numpy array.\")\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=35923)\u001b[0m 2025-05-10 16:28:50,133\tWARNING env.py:156 -- Your env doesn't have a .spec.max_episode_steps attribute. Your horizon will default to infinity, and your environment will not be reset.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=35923)\u001b[0m /home/vladimir/.pyenv/versions/3.9.21/lib/python3.9/site-packages/gymnasium/spaces/box.py:227: UserWarning: \u001b[33mWARN: Casting input x to numpy array.\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=35923)\u001b[0m   logger.warn(\"Casting input x to numpy array.\")\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=35924)\u001b[0m /home/vladimir/.pyenv/versions/3.9.21/lib/python3.9/site-packages/gymnasium/spaces/box.py:227: UserWarning: \u001b[33mWARN: Casting input x to numpy array.\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=35924)\u001b[0m   logger.warn(\"Casting input x to numpy array.\")\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=35925)\u001b[0m 2025-05-10 16:28:51,172\tINFO eager_tf_policy_v2.py:75 -- Creating TF-eager policy running on CPU.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=35925)\u001b[0m 2025-05-10 16:28:51,189\tINFO policy.py:1214 -- Policy (worker=3) running on CPU.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=35925)\u001b[0m 2025-05-10 16:28:51,197\tINFO catalog.py:477 -- Wrapping <class 'graphenv.examples.tsp.tsp_nfp_model.TSPGNNModel'> as None\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=35924)\u001b[0m 2025-05-10 16:28:51,246\tINFO eager_tf_policy_v2.py:75 -- Creating TF-eager policy running on CPU.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=35924)\u001b[0m 2025-05-10 16:28:51,261\tINFO policy.py:1214 -- Policy (worker=2) running on CPU.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=35924)\u001b[0m 2025-05-10 16:28:51,270\tINFO catalog.py:477 -- Wrapping <class 'graphenv.examples.tsp.tsp_nfp_model.TSPGNNModel'> as None\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=35926)\u001b[0m 2025-05-10 16:28:51,246\tINFO eager_tf_policy_v2.py:75 -- Creating TF-eager policy running on CPU.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=35926)\u001b[0m 2025-05-10 16:28:51,261\tINFO policy.py:1214 -- Policy (worker=4) running on CPU.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=35926)\u001b[0m 2025-05-10 16:28:51,270\tINFO catalog.py:477 -- Wrapping <class 'graphenv.examples.tsp.tsp_nfp_model.TSPGNNModel'> as None\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=35923)\u001b[0m 2025-05-10 16:28:51,323\tINFO eager_tf_policy_v2.py:75 -- Creating TF-eager policy running on CPU.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=35923)\u001b[0m 2025-05-10 16:28:51,338\tINFO policy.py:1214 -- Policy (worker=1) running on CPU.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=35923)\u001b[0m 2025-05-10 16:28:51,346\tINFO catalog.py:477 -- Wrapping <class 'graphenv.examples.tsp.tsp_nfp_model.TSPGNNModel'> as None\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=35925)\u001b[0m 2025-05-10 16:28:52,311\tINFO util.py:122 -- Using connectors:\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=35925)\u001b[0m 2025-05-10 16:28:52,311\tINFO util.py:123 --     AgentConnectorPipeline\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=35925)\u001b[0m         ObsPreprocessorConnector\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=35925)\u001b[0m         StateBufferConnector\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=35925)\u001b[0m         ViewRequirementAgentConnector\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=35925)\u001b[0m 2025-05-10 16:28:52,311\tINFO util.py:124 --     ActionConnectorPipeline\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=35925)\u001b[0m         ConvertToNumpyConnector\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=35925)\u001b[0m         NormalizeActionsConnector\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=35925)\u001b[0m         ImmutableActionsConnector\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=35924)\u001b[0m 2025-05-10 16:28:52,421\tINFO util.py:122 -- Using connectors:\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=35924)\u001b[0m 2025-05-10 16:28:52,421\tINFO util.py:123 --     AgentConnectorPipeline\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=35924)\u001b[0m         ObsPreprocessorConnector\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=35924)\u001b[0m         StateBufferConnector\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=35924)\u001b[0m         ViewRequirementAgentConnector\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=35924)\u001b[0m 2025-05-10 16:28:52,421\tINFO util.py:124 --     ActionConnectorPipeline\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=35924)\u001b[0m         ConvertToNumpyConnector\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=35924)\u001b[0m         NormalizeActionsConnector\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=35924)\u001b[0m         ImmutableActionsConnector\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=35926)\u001b[0m 2025-05-10 16:28:52,404\tINFO util.py:122 -- Using connectors:\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=35926)\u001b[0m 2025-05-10 16:28:52,404\tINFO util.py:123 --     AgentConnectorPipeline\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=35926)\u001b[0m         ObsPreprocessorConnector\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=35926)\u001b[0m         StateBufferConnector\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=35926)\u001b[0m         ViewRequirementAgentConnector\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=35926)\u001b[0m 2025-05-10 16:28:52,404\tINFO util.py:124 --     ActionConnectorPipeline\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=35926)\u001b[0m         ConvertToNumpyConnector\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=35926)\u001b[0m         NormalizeActionsConnector\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=35926)\u001b[0m         ImmutableActionsConnector\n",
      "\u001b[2m\u001b[36m(PPO pid=35792)\u001b[0m 2025-05-10 16:28:52,494\tINFO worker_set.py:310 -- Inferred observation/action spaces from remote worker (local worker has no env): {'default_policy': (Repeated(Dict('connectivity': Box(0, 5, (20, 2), int64), 'current_node': Box(0, 5, (), int64), 'distance': Box(0.0, 1.4142135623730951, (), float64), 'edge_weights': Box(0.0, 1.4142135623730951, (20,), float64), 'node_visited': Box(0, 2, (5,), int64)), 6), Discrete(5)), '__env__': (Repeated(Dict('connectivity': Box(0, 5, (20, 2), int64), 'current_node': Box(0, 5, (), int64), 'distance': Box(0.0, 1.4142135623730951, (), float64), 'edge_weights': Box(0.0, 1.4142135623730951, (20,), float64), 'node_visited': Box(0, 2, (5,), int64)), 6), Discrete(5))}\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=35923)\u001b[0m 2025-05-10 16:28:52,489\tINFO util.py:122 -- Using connectors:\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=35923)\u001b[0m 2025-05-10 16:28:52,489\tINFO util.py:123 --     AgentConnectorPipeline\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=35923)\u001b[0m         ObsPreprocessorConnector\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=35923)\u001b[0m         StateBufferConnector\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=35923)\u001b[0m         ViewRequirementAgentConnector\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=35923)\u001b[0m 2025-05-10 16:28:52,489\tINFO util.py:124 --     ActionConnectorPipeline\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=35923)\u001b[0m         ConvertToNumpyConnector\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=35923)\u001b[0m         NormalizeActionsConnector\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=35923)\u001b[0m         ImmutableActionsConnector\n",
      "\u001b[2m\u001b[36m(PPO pid=35792)\u001b[0m 2025-05-10 16:28:53,692\tINFO eager_tf_policy_v2.py:75 -- Creating TF-eager policy running on CPU.\n",
      "\u001b[2m\u001b[36m(PPO pid=35792)\u001b[0m 2025-05-10 16:28:53,706\tINFO policy.py:1214 -- Policy (worker=local) running on CPU.\n",
      "\u001b[2m\u001b[36m(PPO pid=35792)\u001b[0m 2025-05-10 16:28:53,715\tINFO catalog.py:477 -- Wrapping <class 'graphenv.examples.tsp.tsp_nfp_model.TSPGNNModel'> as None\n",
      "\u001b[2m\u001b[36m(PPO pid=35792)\u001b[0m 2025-05-10 16:28:54,888\tINFO util.py:122 -- Using connectors:\n",
      "\u001b[2m\u001b[36m(PPO pid=35792)\u001b[0m 2025-05-10 16:28:54,889\tINFO util.py:123 --     AgentConnectorPipeline\n",
      "\u001b[2m\u001b[36m(PPO pid=35792)\u001b[0m         ObsPreprocessorConnector\n",
      "\u001b[2m\u001b[36m(PPO pid=35792)\u001b[0m         StateBufferConnector\n",
      "\u001b[2m\u001b[36m(PPO pid=35792)\u001b[0m         ViewRequirementAgentConnector\n",
      "\u001b[2m\u001b[36m(PPO pid=35792)\u001b[0m 2025-05-10 16:28:54,889\tINFO util.py:124 --     ActionConnectorPipeline\n",
      "\u001b[2m\u001b[36m(PPO pid=35792)\u001b[0m         ConvertToNumpyConnector\n",
      "\u001b[2m\u001b[36m(PPO pid=35792)\u001b[0m         NormalizeActionsConnector\n",
      "\u001b[2m\u001b[36m(PPO pid=35792)\u001b[0m         ImmutableActionsConnector\n",
      "\u001b[2m\u001b[36m(PPO pid=35792)\u001b[0m 2025-05-10 16:28:54,889\tINFO rollout_worker.py:2043 -- Built policy map: <PolicyMap lru-caching-capacity=100 policy-IDs=['default_policy']>\n",
      "\u001b[2m\u001b[36m(PPO pid=35792)\u001b[0m 2025-05-10 16:28:54,889\tINFO rollout_worker.py:2044 -- Built preprocessor map: {'default_policy': None}\n",
      "\u001b[2m\u001b[36m(PPO pid=35792)\u001b[0m 2025-05-10 16:28:54,889\tINFO rollout_worker.py:760 -- Built filter map: defaultdict(<class 'ray.rllib.utils.filter.NoFilter'>, {})\n",
      "\u001b[2m\u001b[36m(PPO pid=35792)\u001b[0m 2025-05-10 16:28:54,897\tINFO algorithm_config.py:2888 -- Executing eagerly (framework='tf2'), with eager_tracing=tf2. For production workloads, make sure to set eager_tracing=True  in order to match the speed of tf-static-graph (framework='tf'). For debugging purposes, `eager_tracing=False` is the best choice.\n",
      "\u001b[2m\u001b[36m(PPO pid=35792)\u001b[0m 2025-05-10 16:28:54,901\tWARNING env.py:156 -- Your env doesn't have a .spec.max_episode_steps attribute. Your horizon will default to infinity, and your environment will not be reset.\n",
      "\u001b[2m\u001b[36m(PPO pid=35792)\u001b[0m /home/vladimir/.pyenv/versions/3.9.21/lib/python3.9/site-packages/gymnasium/spaces/box.py:227: UserWarning: \u001b[33mWARN: Casting input x to numpy array.\u001b[0m\n",
      "\u001b[2m\u001b[36m(PPO pid=35792)\u001b[0m   logger.warn(\"Casting input x to numpy array.\")\n",
      "\u001b[2m\u001b[36m(PPO pid=35792)\u001b[0m 2025-05-10 16:28:54,907\tINFO eager_tf_policy_v2.py:75 -- Creating TF-eager policy running on CPU.\n",
      "\u001b[2m\u001b[36m(PPO pid=35792)\u001b[0m 2025-05-10 16:28:54,908\tINFO policy.py:1214 -- Policy (worker=local) running on CPU.\n",
      "\u001b[2m\u001b[36m(PPO pid=35792)\u001b[0m 2025-05-10 16:28:54,908\tINFO catalog.py:477 -- Wrapping <class 'graphenv.examples.tsp.tsp_nfp_model.TSPGNNModel'> as None\n",
      "\u001b[2m\u001b[36m(PPO pid=35792)\u001b[0m 2025-05-10 16:28:55,437\tINFO util.py:122 -- Using connectors:\n",
      "\u001b[2m\u001b[36m(PPO pid=35792)\u001b[0m 2025-05-10 16:28:55,437\tINFO util.py:123 --     AgentConnectorPipeline\n",
      "\u001b[2m\u001b[36m(PPO pid=35792)\u001b[0m         ObsPreprocessorConnector\n",
      "\u001b[2m\u001b[36m(PPO pid=35792)\u001b[0m         StateBufferConnector\n",
      "\u001b[2m\u001b[36m(PPO pid=35792)\u001b[0m         ViewRequirementAgentConnector\n",
      "\u001b[2m\u001b[36m(PPO pid=35792)\u001b[0m 2025-05-10 16:28:55,437\tINFO util.py:124 --     ActionConnectorPipeline\n",
      "\u001b[2m\u001b[36m(PPO pid=35792)\u001b[0m         ConvertToNumpyConnector\n",
      "\u001b[2m\u001b[36m(PPO pid=35792)\u001b[0m         NormalizeActionsConnector\n",
      "\u001b[2m\u001b[36m(PPO pid=35792)\u001b[0m         ImmutableActionsConnector\n",
      "\u001b[2m\u001b[36m(PPO pid=35792)\u001b[0m 2025-05-10 16:28:55,437\tINFO rollout_worker.py:2043 -- Built policy map: <PolicyMap lru-caching-capacity=100 policy-IDs=['default_policy']>\n",
      "\u001b[2m\u001b[36m(PPO pid=35792)\u001b[0m 2025-05-10 16:28:55,437\tINFO rollout_worker.py:2044 -- Built preprocessor map: {'default_policy': None}\n",
      "\u001b[2m\u001b[36m(PPO pid=35792)\u001b[0m 2025-05-10 16:28:55,437\tINFO rollout_worker.py:760 -- Built filter map: defaultdict(<class 'ray.rllib.utils.filter.NoFilter'>, {})\n",
      "\u001b[2m\u001b[36m(PPO pid=35792)\u001b[0m 2025-05-10 16:28:55,437\tWARNING util.py:67 -- Install gputil for GPU system monitoring.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=35923)\u001b[0m 2025-05-10 16:28:55,440\tINFO rollout_worker.py:908 -- Generating sample batch of size 5\n",
      "\u001b[2m\u001b[36m(PPO pid=35792)\u001b[0m 2025-05-10 16:28:56,390\tWARNING deprecation.py:50 -- DeprecationWarning: `_get_slice_indices` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=35792)\u001b[0m /home/vladimir/.pyenv/versions/3.9.21/lib/python3.9/site-packages/numpy/core/_methods.py:180: RuntimeWarning: overflow encountered in reduce\n",
      "\u001b[2m\u001b[36m(PPO pid=35792)\u001b[0m   ret = umr_sum(arr, axis, dtype, out, keepdims, where=where)\n",
      "\u001b[2m\u001b[36m(PPO pid=35792)\u001b[0m 2025-05-10 16:28:56,392\tINFO rollout_worker.py:1004 -- Training on concatenated sample batches:\n",
      "\u001b[2m\u001b[36m(PPO pid=35792)\u001b[0m \n",
      "\u001b[2m\u001b[36m(PPO pid=35792)\u001b[0m { 'count': 2,\n",
      "\u001b[2m\u001b[36m(PPO pid=35792)\u001b[0m   'policy_batches': { 'default_policy': { 'action_dist_inputs': np.ndarray((2, 5), dtype=float32, min=-3.4028234663852886e+38, max=-7.684, mean=-inf),\n",
      "\u001b[2m\u001b[36m(PPO pid=35792)\u001b[0m                                           'action_logp': np.ndarray((2,), dtype=float32, min=-0.811, max=-0.0, mean=-0.405),\n",
      "\u001b[2m\u001b[36m(PPO pid=35792)\u001b[0m                                           'actions': np.ndarray((2,), dtype=int32, min=0.0, max=1.0, mean=0.5),\n",
      "\u001b[2m\u001b[36m(PPO pid=35792)\u001b[0m                                           'advantages': np.ndarray((2,), dtype=float32, min=-0.285, max=1.789, mean=0.752),\n",
      "\u001b[2m\u001b[36m(PPO pid=35792)\u001b[0m                                           'agent_index': np.ndarray((2,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(PPO pid=35792)\u001b[0m                                           'eps_id': np.ndarray((2,), dtype=int64, min=2.6899442972628445e+17, max=9.60299532220445e+17, mean=6.146469809733647e+17),\n",
      "\u001b[2m\u001b[36m(PPO pid=35792)\u001b[0m                                           'new_obs': np.ndarray((2, 403), dtype=float32, min=0.0, max=4.0, mean=0.375),\n",
      "\u001b[2m\u001b[36m(PPO pid=35792)\u001b[0m                                           'obs': np.ndarray((2, 403), dtype=float32, min=0.0, max=4.0, mean=0.622),\n",
      "\u001b[2m\u001b[36m(PPO pid=35792)\u001b[0m                                           'rewards': np.ndarray((2,), dtype=float32, min=-0.531, max=-0.395, mean=-0.463),\n",
      "\u001b[2m\u001b[36m(PPO pid=35792)\u001b[0m                                           't': np.ndarray((2,), dtype=int64, min=2.0, max=4.0, mean=3.0),\n",
      "\u001b[2m\u001b[36m(PPO pid=35792)\u001b[0m                                           'terminateds': np.ndarray((2,), dtype=bool, min=0.0, max=1.0, mean=0.5),\n",
      "\u001b[2m\u001b[36m(PPO pid=35792)\u001b[0m                                           'truncateds': np.ndarray((2,), dtype=bool, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(PPO pid=35792)\u001b[0m                                           'unroll_id': np.ndarray((2,), dtype=int64, min=1.0, max=1.0, mean=1.0),\n",
      "\u001b[2m\u001b[36m(PPO pid=35792)\u001b[0m                                           'value_targets': np.ndarray((2,), dtype=float32, min=-1.324, max=-0.531, mean=-0.928),\n",
      "\u001b[2m\u001b[36m(PPO pid=35792)\u001b[0m                                           'vf_preds': np.ndarray((2,), dtype=float32, min=-14.631, max=-4.113, mean=-9.372)}},\n",
      "\u001b[2m\u001b[36m(PPO pid=35792)\u001b[0m   'type': 'MultiAgentBatch'}\n",
      "\u001b[2m\u001b[36m(PPO pid=35792)\u001b[0m \n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=35923)\u001b[0m /home/vladimir/.pyenv/versions/3.9.21/lib/python3.9/site-packages/numpy/core/_methods.py:180: RuntimeWarning: overflow encountered in reduce\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=35923)\u001b[0m   ret = umr_sum(arr, axis, dtype, out, keepdims, where=where)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=35923)\u001b[0m 2025-05-10 16:28:56,358\tINFO rollout_worker.py:949 -- Completed sample batch:\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=35923)\u001b[0m \n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=35923)\u001b[0m { 'count': 5,\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=35923)\u001b[0m   'policy_batches': { 'default_policy': { 'action_dist_inputs': np.ndarray((5, 5), dtype=float32, min=-3.4028234663852886e+38, max=-2.861, mean=-inf),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=35923)\u001b[0m                                           'action_logp': np.ndarray((5,), dtype=float32, min=-1.277, max=-0.0, mean=-0.265),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=35923)\u001b[0m                                           'actions': np.ndarray((5,), dtype=int32, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=35923)\u001b[0m                                           'advantages': np.ndarray((5,), dtype=float32, min=-1.844, max=14.1, mean=4.087),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=35923)\u001b[0m                                           'agent_index': np.ndarray((5,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=35923)\u001b[0m                                           'eps_id': np.ndarray((5,), dtype=int64, min=3.9676351968200794e+17, max=3.9676351968200794e+17, mean=3.9676351968200794e+17),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=35923)\u001b[0m                                           'new_obs': np.ndarray((5, 403), dtype=float32, min=0.0, max=4.0, mean=0.596),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=35923)\u001b[0m                                           'obs': np.ndarray((5, 403), dtype=float32, min=0.0, max=5.0, mean=0.788),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=35923)\u001b[0m                                           'rewards': np.ndarray((5,), dtype=float32, min=-0.732, max=-0.179, mean=-0.379),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=35923)\u001b[0m                                           't': np.ndarray((5,), dtype=int64, min=0.0, max=4.0, mean=2.0),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=35923)\u001b[0m                                           'terminateds': np.ndarray((5,), dtype=bool, min=0.0, max=1.0, mean=0.2),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=35923)\u001b[0m                                           'truncateds': np.ndarray((5,), dtype=bool, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=35923)\u001b[0m                                           'unroll_id': np.ndarray((5,), dtype=int64, min=1.0, max=1.0, mean=1.0),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=35923)\u001b[0m                                           'value_targets': np.ndarray((5,), dtype=float32, min=-1.844, max=-0.531, mean=-1.361),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=35923)\u001b[0m                                           'vf_preds': np.ndarray((5,), dtype=float32, min=-14.631, max=0.0, mean=-5.448)}},\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=35923)\u001b[0m   'type': 'MultiAgentBatch'}\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=35923)\u001b[0m \n",
      "\u001b[2m\u001b[36m(PPO pid=35792)\u001b[0m 2025-05-10 16:28:56,885\tWARNING tf_utils.py:576 -- KL divergence is non-finite, this will likely destabilize your model and the training process. Action(s) in a specific state have near-zero probability. This can happen naturally in deterministic environments where the optimal policy has zero mass for a specific action. To fix this issue, consider setting the coefficient for the KL loss term to zero or increasing policy entropy.\n",
      "\u001b[2m\u001b[36m(PPO pid=35792)\u001b[0m 2025-05-10 16:28:57,601\tINFO eager_tf_policy_v2.py:930 -- Optimizing variable node_embedding/embeddings:0\n",
      "\u001b[2m\u001b[36m(PPO pid=35792)\u001b[0m 2025-05-10 16:28:57,601\tINFO eager_tf_policy_v2.py:930 -- Optimizing variable edge_update/concat_dense/dense/kernel:0\n",
      "\u001b[2m\u001b[36m(PPO pid=35792)\u001b[0m 2025-05-10 16:28:57,601\tINFO eager_tf_policy_v2.py:930 -- Optimizing variable edge_update/concat_dense/dense/bias:0\n",
      "\u001b[2m\u001b[36m(PPO pid=35792)\u001b[0m 2025-05-10 16:28:57,601\tINFO eager_tf_policy_v2.py:930 -- Optimizing variable edge_update/concat_dense/dense_1/kernel:0\n",
      "\u001b[2m\u001b[36m(PPO pid=35792)\u001b[0m 2025-05-10 16:28:57,601\tINFO eager_tf_policy_v2.py:930 -- Optimizing variable edge_update/concat_dense/dense_1/bias:0\n",
      "\u001b[2m\u001b[36m(PPO pid=35792)\u001b[0m 2025-05-10 16:28:57,601\tINFO eager_tf_policy_v2.py:930 -- Optimizing variable node_update/concat_dense/dense_2/kernel:0\n",
      "\u001b[2m\u001b[36m(PPO pid=35792)\u001b[0m 2025-05-10 16:28:57,601\tINFO eager_tf_policy_v2.py:930 -- Optimizing variable node_update/concat_dense/dense_2/bias:0\n",
      "\u001b[2m\u001b[36m(PPO pid=35792)\u001b[0m 2025-05-10 16:28:57,601\tINFO eager_tf_policy_v2.py:930 -- Optimizing variable node_update/concat_dense/dense_3/kernel:0\n",
      "\u001b[2m\u001b[36m(PPO pid=35792)\u001b[0m 2025-05-10 16:28:57,602\tINFO eager_tf_policy_v2.py:930 -- Optimizing variable node_update/concat_dense/dense_3/bias:0\n",
      "\u001b[2m\u001b[36m(PPO pid=35792)\u001b[0m 2025-05-10 16:28:57,602\tINFO eager_tf_policy_v2.py:930 -- Optimizing variable node_update/dense/kernel:0\n",
      "\u001b[2m\u001b[36m(PPO pid=35792)\u001b[0m 2025-05-10 16:28:57,602\tINFO eager_tf_policy_v2.py:930 -- Optimizing variable node_update/dense/bias:0\n",
      "\u001b[2m\u001b[36m(PPO pid=35792)\u001b[0m 2025-05-10 16:28:57,602\tINFO eager_tf_policy_v2.py:930 -- Optimizing variable node_update/dense_1/kernel:0\n",
      "\u001b[2m\u001b[36m(PPO pid=35792)\u001b[0m 2025-05-10 16:28:57,602\tINFO eager_tf_policy_v2.py:930 -- Optimizing variable node_update/dense_1/bias:0\n",
      "\u001b[2m\u001b[36m(PPO pid=35792)\u001b[0m 2025-05-10 16:28:57,602\tINFO eager_tf_policy_v2.py:930 -- Optimizing variable edge_update_1/concat_dense/dense/kernel:0\n",
      "\u001b[2m\u001b[36m(PPO pid=35792)\u001b[0m 2025-05-10 16:28:57,602\tINFO eager_tf_policy_v2.py:930 -- Optimizing variable edge_update_1/concat_dense/dense/bias:0\n",
      "\u001b[2m\u001b[36m(PPO pid=35792)\u001b[0m 2025-05-10 16:28:57,602\tINFO eager_tf_policy_v2.py:930 -- Optimizing variable edge_update_1/concat_dense/dense_1/kernel:0\n",
      "\u001b[2m\u001b[36m(PPO pid=35792)\u001b[0m 2025-05-10 16:28:57,602\tINFO eager_tf_policy_v2.py:930 -- Optimizing variable edge_update_1/concat_dense/dense_1/bias:0\n",
      "\u001b[2m\u001b[36m(PPO pid=35792)\u001b[0m 2025-05-10 16:28:57,602\tINFO eager_tf_policy_v2.py:930 -- Optimizing variable node_update_1/concat_dense/dense_2/kernel:0\n",
      "\u001b[2m\u001b[36m(PPO pid=35792)\u001b[0m 2025-05-10 16:28:57,602\tINFO eager_tf_policy_v2.py:930 -- Optimizing variable node_update_1/concat_dense/dense_2/bias:0\n",
      "\u001b[2m\u001b[36m(PPO pid=35792)\u001b[0m 2025-05-10 16:28:57,602\tINFO eager_tf_policy_v2.py:930 -- Optimizing variable node_update_1/concat_dense/dense_3/kernel:0\n",
      "\u001b[2m\u001b[36m(PPO pid=35792)\u001b[0m 2025-05-10 16:28:57,602\tINFO eager_tf_policy_v2.py:930 -- Optimizing variable node_update_1/concat_dense/dense_3/bias:0\n",
      "\u001b[2m\u001b[36m(PPO pid=35792)\u001b[0m 2025-05-10 16:28:57,602\tINFO eager_tf_policy_v2.py:930 -- Optimizing variable node_update_1/dense/kernel:0\n",
      "\u001b[2m\u001b[36m(PPO pid=35792)\u001b[0m 2025-05-10 16:28:57,602\tINFO eager_tf_policy_v2.py:930 -- Optimizing variable node_update_1/dense/bias:0\n",
      "\u001b[2m\u001b[36m(PPO pid=35792)\u001b[0m 2025-05-10 16:28:57,602\tINFO eager_tf_policy_v2.py:930 -- Optimizing variable node_update_1/dense_1/kernel:0\n",
      "\u001b[2m\u001b[36m(PPO pid=35792)\u001b[0m 2025-05-10 16:28:57,602\tINFO eager_tf_policy_v2.py:930 -- Optimizing variable node_update_1/dense_1/bias:0\n",
      "\u001b[2m\u001b[36m(PPO pid=35792)\u001b[0m 2025-05-10 16:28:57,602\tINFO eager_tf_policy_v2.py:930 -- Optimizing variable edge_update_2/concat_dense/dense/kernel:0\n",
      "\u001b[2m\u001b[36m(PPO pid=35792)\u001b[0m 2025-05-10 16:28:57,602\tINFO eager_tf_policy_v2.py:930 -- Optimizing variable edge_update_2/concat_dense/dense/bias:0\n",
      "\u001b[2m\u001b[36m(PPO pid=35792)\u001b[0m 2025-05-10 16:28:57,602\tINFO eager_tf_policy_v2.py:930 -- Optimizing variable edge_update_2/concat_dense/dense_1/kernel:0\n",
      "\u001b[2m\u001b[36m(PPO pid=35792)\u001b[0m 2025-05-10 16:28:57,602\tINFO eager_tf_policy_v2.py:930 -- Optimizing variable edge_update_2/concat_dense/dense_1/bias:0\n",
      "\u001b[2m\u001b[36m(PPO pid=35792)\u001b[0m 2025-05-10 16:28:57,602\tINFO eager_tf_policy_v2.py:930 -- Optimizing variable node_update_2/concat_dense/dense_2/kernel:0\n",
      "\u001b[2m\u001b[36m(PPO pid=35792)\u001b[0m 2025-05-10 16:28:57,602\tINFO eager_tf_policy_v2.py:930 -- Optimizing variable node_update_2/concat_dense/dense_2/bias:0\n",
      "\u001b[2m\u001b[36m(PPO pid=35792)\u001b[0m 2025-05-10 16:28:57,602\tINFO eager_tf_policy_v2.py:930 -- Optimizing variable node_update_2/concat_dense/dense_3/kernel:0\n",
      "\u001b[2m\u001b[36m(PPO pid=35792)\u001b[0m 2025-05-10 16:28:57,602\tINFO eager_tf_policy_v2.py:930 -- Optimizing variable node_update_2/concat_dense/dense_3/bias:0\n",
      "\u001b[2m\u001b[36m(PPO pid=35792)\u001b[0m 2025-05-10 16:28:57,602\tINFO eager_tf_policy_v2.py:930 -- Optimizing variable node_update_2/dense/kernel:0\n",
      "\u001b[2m\u001b[36m(PPO pid=35792)\u001b[0m 2025-05-10 16:28:57,602\tINFO eager_tf_policy_v2.py:930 -- Optimizing variable node_update_2/dense/bias:0\n",
      "\u001b[2m\u001b[36m(PPO pid=35792)\u001b[0m 2025-05-10 16:28:57,602\tINFO eager_tf_policy_v2.py:930 -- Optimizing variable node_update_2/dense_1/kernel:0\n",
      "\u001b[2m\u001b[36m(PPO pid=35792)\u001b[0m 2025-05-10 16:28:57,602\tINFO eager_tf_policy_v2.py:930 -- Optimizing variable node_update_2/dense_1/bias:0\n",
      "\u001b[2m\u001b[36m(PPO pid=35792)\u001b[0m 2025-05-10 16:28:57,602\tINFO eager_tf_policy_v2.py:930 -- Optimizing variable distance_values/kernel:0\n",
      "\u001b[2m\u001b[36m(PPO pid=35792)\u001b[0m 2025-05-10 16:28:57,602\tINFO eager_tf_policy_v2.py:930 -- Optimizing variable distance_values/bias:0\n",
      "\u001b[2m\u001b[36m(PPO pid=35792)\u001b[0m 2025-05-10 16:28:57,602\tINFO eager_tf_policy_v2.py:930 -- Optimizing variable action_value_output/kernel:0\n",
      "\u001b[2m\u001b[36m(PPO pid=35792)\u001b[0m 2025-05-10 16:28:57,602\tINFO eager_tf_policy_v2.py:930 -- Optimizing variable action_value_output/bias:0\n",
      "\u001b[2m\u001b[36m(PPO pid=35792)\u001b[0m 2025-05-10 16:28:57,602\tINFO eager_tf_policy_v2.py:930 -- Optimizing variable distance__weights/kernel:0\n",
      "\u001b[2m\u001b[36m(PPO pid=35792)\u001b[0m 2025-05-10 16:28:57,602\tINFO eager_tf_policy_v2.py:930 -- Optimizing variable distance__weights/bias:0\n",
      "\u001b[2m\u001b[36m(PPO pid=35792)\u001b[0m 2025-05-10 16:28:57,602\tINFO eager_tf_policy_v2.py:930 -- Optimizing variable action_weight_output/kernel:0\n",
      "\u001b[2m\u001b[36m(PPO pid=35792)\u001b[0m 2025-05-10 16:28:57,602\tINFO eager_tf_policy_v2.py:930 -- Optimizing variable action_weight_output/bias:0\n",
      "\u001b[2m\u001b[36m(PPO pid=35792)\u001b[0m 2025-05-10 16:28:58,422\tWARNING tf_utils.py:576 -- KL divergence is non-finite, this will likely destabilize your model and the training process. Action(s) in a specific state have near-zero probability. This can happen naturally in deterministic environments where the optimal policy has zero mass for a specific action. To fix this issue, consider setting the coefficient for the KL loss term to zero or increasing policy entropy.\n",
      "\u001b[2m\u001b[36m(PPO pid=35792)\u001b[0m 2025-05-10 16:29:00,123\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=35792)\u001b[0m 2025-05-10 16:29:00,124\tINFO rollout_worker.py:908 -- Generating sample batch of size 1\n",
      "\u001b[2m\u001b[36m(PPO pid=35792)\u001b[0m 2025-05-10 16:29:00,991\tINFO rollout_worker.py:949 -- Completed sample batch:\n",
      "\u001b[2m\u001b[36m(PPO pid=35792)\u001b[0m \n",
      "\u001b[2m\u001b[36m(PPO pid=35792)\u001b[0m { 'count': 5,\n",
      "\u001b[2m\u001b[36m(PPO pid=35792)\u001b[0m   'policy_batches': { 'default_policy': { 'advantages': np.ndarray((5,), dtype=float32, min=-2.946, max=13.076, mean=3.409),\n",
      "\u001b[2m\u001b[36m(PPO pid=35792)\u001b[0m                                           'agent_index': np.ndarray((5,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(PPO pid=35792)\u001b[0m                                           'eps_id': np.ndarray((5,), dtype=int64, min=7.084019726863364e+17, max=7.084019726863364e+17, mean=7.084019726863364e+17),\n",
      "\u001b[2m\u001b[36m(PPO pid=35792)\u001b[0m                                           'new_obs': np.ndarray((5, 403), dtype=float32, min=0.0, max=4.0, mean=0.595),\n",
      "\u001b[2m\u001b[36m(PPO pid=35792)\u001b[0m                                           'obs': np.ndarray((5, 403), dtype=float32, min=0.0, max=5.0, mean=0.787),\n",
      "\u001b[2m\u001b[36m(PPO pid=35792)\u001b[0m                                           'rewards': np.ndarray((5,), dtype=float32, min=-0.732, max=-0.143, mean=-0.399),\n",
      "\u001b[2m\u001b[36m(PPO pid=35792)\u001b[0m                                           't': np.ndarray((5,), dtype=int64, min=0.0, max=4.0, mean=2.0),\n",
      "\u001b[2m\u001b[36m(PPO pid=35792)\u001b[0m                                           'terminateds': np.ndarray((5,), dtype=bool, min=0.0, max=1.0, mean=0.2),\n",
      "\u001b[2m\u001b[36m(PPO pid=35792)\u001b[0m                                           'truncateds': np.ndarray((5,), dtype=bool, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(PPO pid=35792)\u001b[0m                                           'unroll_id': np.ndarray((5,), dtype=int64, min=1.0, max=1.0, mean=1.0),\n",
      "\u001b[2m\u001b[36m(PPO pid=35792)\u001b[0m                                           'value_targets': np.ndarray((5,), dtype=float32, min=-1.943, max=-0.531, mean=-1.436),\n",
      "\u001b[2m\u001b[36m(PPO pid=35792)\u001b[0m                                           'vf_preds': np.ndarray((5,), dtype=float32, min=-13.608, max=1.003, mean=-4.845)}},\n",
      "\u001b[2m\u001b[36m(PPO pid=35792)\u001b[0m   'type': 'MultiAgentBatch'}\n",
      "\u001b[2m\u001b[36m(PPO pid=35792)\u001b[0m \n",
      "\u001b[2m\u001b[36m(PPO pid=35792)\u001b[0m 2025-05-10 16:29:02,212\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=35792)\u001b[0m 2025-05-10 16:29:03,404\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=35792)\u001b[0m 2025-05-10 16:29:04,596\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=35792)\u001b[0m 2025-05-10 16:29:05,801\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=35792)\u001b[0m 2025-05-10 16:29:07,007\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=35792)\u001b[0m 2025-05-10 16:29:08,202\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=35792)\u001b[0m 2025-05-10 16:29:09,388\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=35792)\u001b[0m 2025-05-10 16:29:10,569\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=35792)\u001b[0m 2025-05-10 16:29:11,777\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=35792)\u001b[0m 2025-05-10 16:29:12,989\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=35792)\u001b[0m 2025-05-10 16:29:14,191\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=35792)\u001b[0m 2025-05-10 16:29:15,381\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=35792)\u001b[0m 2025-05-10 16:29:16,565\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=35792)\u001b[0m 2025-05-10 16:29:17,766\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=35792)\u001b[0m 2025-05-10 16:29:18,963\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=35792)\u001b[0m 2025-05-10 16:29:20,177\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=35792)\u001b[0m 2025-05-10 16:29:21,375\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=35792)\u001b[0m 2025-05-10 16:29:22,581\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=35792)\u001b[0m 2025-05-10 16:29:23,803\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=35792)\u001b[0m 2025-05-10 16:29:25,025\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=35792)\u001b[0m 2025-05-10 16:29:26,234\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=35792)\u001b[0m 2025-05-10 16:29:27,418\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=35792)\u001b[0m 2025-05-10 16:29:28,614\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=35792)\u001b[0m 2025-05-10 16:29:29,791\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=35792)\u001b[0m 2025-05-10 16:29:30,972\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=35792)\u001b[0m 2025-05-10 16:29:32,152\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=35792)\u001b[0m 2025-05-10 16:29:33,332\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=35792)\u001b[0m 2025-05-10 16:29:34,536\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=35792)\u001b[0m 2025-05-10 16:29:35,728\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=35792)\u001b[0m 2025-05-10 16:29:36,948\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=35792)\u001b[0m 2025-05-10 16:29:38,141\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=35792)\u001b[0m 2025-05-10 16:29:39,319\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=35792)\u001b[0m 2025-05-10 16:29:40,517\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=35792)\u001b[0m 2025-05-10 16:29:41,698\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=35792)\u001b[0m 2025-05-10 16:29:42,882\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=35792)\u001b[0m 2025-05-10 16:29:44,074\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=35792)\u001b[0m 2025-05-10 16:29:45,253\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=35792)\u001b[0m 2025-05-10 16:29:46,434\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=35792)\u001b[0m 2025-05-10 16:29:47,630\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=35792)\u001b[0m 2025-05-10 16:29:48,858\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=35792)\u001b[0m 2025-05-10 16:29:50,067\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=35792)\u001b[0m 2025-05-10 16:29:51,273\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=35792)\u001b[0m 2025-05-10 16:29:52,472\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=35792)\u001b[0m 2025-05-10 16:29:53,677\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=35792)\u001b[0m 2025-05-10 16:29:54,893\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=35792)\u001b[0m 2025-05-10 16:29:56,095\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=35792)\u001b[0m 2025-05-10 16:29:57,302\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=35792)\u001b[0m 2025-05-10 16:29:58,487\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=35792)\u001b[0m 2025-05-10 16:29:59,671\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=35792)\u001b[0m 2025-05-10 16:30:00,894\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=35792)\u001b[0m 2025-05-10 16:30:02,099\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=35792)\u001b[0m 2025-05-10 16:30:03,310\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=35792)\u001b[0m 2025-05-10 16:30:04,514\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=35792)\u001b[0m 2025-05-10 16:30:05,701\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=35792)\u001b[0m 2025-05-10 16:30:06,893\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=35792)\u001b[0m 2025-05-10 16:30:08,092\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=35792)\u001b[0m 2025-05-10 16:30:09,296\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=35792)\u001b[0m 2025-05-10 16:30:10,472\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=35792)\u001b[0m 2025-05-10 16:30:11,643\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=35792)\u001b[0m 2025-05-10 16:30:12,847\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=35792)\u001b[0m 2025-05-10 16:30:14,031\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=35792)\u001b[0m 2025-05-10 16:30:15,210\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=35792)\u001b[0m 2025-05-10 16:30:16,397\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=35792)\u001b[0m 2025-05-10 16:30:17,580\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=35792)\u001b[0m 2025-05-10 16:30:18,762\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=35792)\u001b[0m 2025-05-10 16:30:19,968\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=35792)\u001b[0m 2025-05-10 16:30:21,150\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=35792)\u001b[0m 2025-05-10 16:30:22,338\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=35792)\u001b[0m 2025-05-10 16:30:23,532\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=35792)\u001b[0m 2025-05-10 16:30:24,733\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=35792)\u001b[0m 2025-05-10 16:30:25,917\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=35792)\u001b[0m 2025-05-10 16:30:27,120\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=35792)\u001b[0m 2025-05-10 16:30:28,304\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=35792)\u001b[0m 2025-05-10 16:30:29,486\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=35792)\u001b[0m 2025-05-10 16:30:30,685\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=35792)\u001b[0m 2025-05-10 16:30:31,875\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=35792)\u001b[0m 2025-05-10 16:30:33,070\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=35792)\u001b[0m 2025-05-10 16:30:34,278\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=35792)\u001b[0m 2025-05-10 16:30:35,481\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=35792)\u001b[0m 2025-05-10 16:30:36,686\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=35792)\u001b[0m 2025-05-10 16:30:37,881\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=35792)\u001b[0m 2025-05-10 16:30:39,085\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=35792)\u001b[0m 2025-05-10 16:30:40,290\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=35792)\u001b[0m 2025-05-10 16:30:41,501\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=35792)\u001b[0m 2025-05-10 16:30:42,692\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=35792)\u001b[0m 2025-05-10 16:30:43,905\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=35792)\u001b[0m 2025-05-10 16:30:45,089\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=35792)\u001b[0m 2025-05-10 16:30:46,299\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=35792)\u001b[0m 2025-05-10 16:30:47,499\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=35792)\u001b[0m 2025-05-10 16:30:48,735\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=35792)\u001b[0m 2025-05-10 16:30:49,964\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=35792)\u001b[0m 2025-05-10 16:30:51,166\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=35792)\u001b[0m 2025-05-10 16:30:52,361\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=35792)\u001b[0m 2025-05-10 16:30:53,572\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=35792)\u001b[0m 2025-05-10 16:30:54,751\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=35792)\u001b[0m 2025-05-10 16:30:55,942\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=35792)\u001b[0m 2025-05-10 16:30:57,114\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=35792)\u001b[0m 2025-05-10 16:30:58,299\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n",
      "\u001b[2m\u001b[36m(PPO pid=35792)\u001b[0m 2025-05-10 16:30:59,485\tINFO algorithm.py:925 -- Evaluating current state of PPO for 100 episodes.\n"
     ]
    }
   ],
   "source": [
    "print(f\"Running with following CLI options: {args}\")\n",
    "logging.basicConfig(level=args.log_level.upper())\n",
    "ray.init(local_mode=args.local_mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7ac9d4e5-2477-461b-a467-4f0f457826fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = args.N\n",
    "G = make_complete_planar_graph(N=N, seed=args.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a9a56466-059f-45a7-ac36-aa4a93e27ea4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Networkx heuristic reward: -1.700\n",
      "[0, 4, 1, 3, 2, 0]\n",
      "Networkx greedy reward: -1.996\n",
      "[0, 2, 1, 3, 4, 0]\n"
     ]
    }
   ],
   "source": [
    "tsp_approx = nx.approximation.traveling_salesman_problem\n",
    "path = tsp_approx(G, cycle=True)\n",
    "reward_baseline = -sum([G[path[i]][path[i + 1]][\"weight\"] for i in range(0, N)])\n",
    "print(f\"Networkx heuristic reward: {reward_baseline:1.3f}\")\n",
    "print(path)\n",
    "path = tsp_approx(G, cycle=True, method=greedy_tsp)\n",
    "reward_baseline = -sum([G[path[i]][path[i + 1]][\"weight\"] for i in range(0, N)])\n",
    "print(f\"Networkx greedy reward: {reward_baseline:1.3f}\")\n",
    "print(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c508cb95-5a0b-49c9-bc4a-1aa49ab413ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Algorithm-specific config, common ones are in the main config dict below\n",
    "if args.run == \"PPO\":\n",
    "    run_config = PPOConfig()\n",
    "    train_batch_size = args.rollouts_per_worker * N * args.num_workers\n",
    "    sgd_minibatch_size = 16 if train_batch_size > 16 else 2\n",
    "    run_config.training(entropy_coeff=args.entropy_coeff,\n",
    "                        sgd_minibatch_size=sgd_minibatch_size,\n",
    "                        num_sgd_iter=5,\n",
    "    )\n",
    "elif args.run in [\"DQN\"]:\n",
    "    run_config = DQNConfig()\n",
    "    # Update here with custom config\n",
    "    run_config.training(hiddens=False,\n",
    "                    dueling=False\n",
    "    )\n",
    "    run_config.exploration(exploration_config={\"epsilon_timesteps\": 250000})\n",
    "elif args.run == \"A3C\":\n",
    "    run_config = A3CConfig()\n",
    "elif args.run == \"MARWIL\":\n",
    "    run_config = MARWILConfig()\n",
    "else:\n",
    "    raise ValueError(f\"Import agent {args.run} and try again\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "27e446a3-976f-46f0-899a-da10ab18e3a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "use_gnn\n"
     ]
    }
   ],
   "source": [
    "# Define custom_model, config, and state based on GNN yes/no\n",
    "if args.use_gnn:\n",
    "    custom_model = \"TSPGNNModel\"\n",
    "    custom_model_config = {\"num_messages\": 3, \"embed_dim\": 32}\n",
    "    print('use_gnn')\n",
    "    ModelCatalog.register_custom_model(custom_model, TSPGNNModel)\n",
    "    _tag = \"gnn\"\n",
    "    state = TSPNFPState(\n",
    "        lambda: G,\n",
    "        max_num_neighbors=args.max_num_neighbors,\n",
    "    )\n",
    "else:\n",
    "    custom_model_config = {\"hidden_dim\": 256, \"embed_dim\": 256, \"num_nodes\": N}\n",
    "    custom_model = \"TSPModel\"\n",
    "    Model = TSPQModel if args.run in [\"DQN\", \"R2D2\"] else TSPModel\n",
    "    ModelCatalog.register_custom_model(custom_model, Model)\n",
    "    _tag = f\"basic{args.run}\"\n",
    "    state = TSPState(lambda: G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "505c95ca-d686-4c5a-890f-cbd9a64531de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register env name with hyperparams that will help tracking experiments\n",
    "# via tensorboard\n",
    "env_name = f\"mygraphenv-v0\" #_{N}_{_tag}_lr={args.lr}\n",
    "register_env(env_name, lambda config: GraphEnv(config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e0d687e6-fe6f-498e-880e-fee875f2fd0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "args.num_gpus = 0\n",
    "args.num_workers = 4\n",
    "run_config = (\n",
    "    run_config\n",
    "    .resources(num_gpus=args.num_gpus) \n",
    "    .framework(\"tf2\") #tf ?\n",
    "    .rollouts(num_rollout_workers=args.num_workers, \n",
    "              # a multiple of N (collect whole episodes)\n",
    "              rollout_fragment_length=N\n",
    "             )\n",
    "    .environment(env=env_name,\n",
    "                 env_config={\"state\": state, \n",
    "                             \"max_num_children\": G.number_of_nodes()}\n",
    "              )\n",
    "    .training(lr=args.lr,\n",
    "              train_batch_size=args.rollouts_per_worker * N * args.num_workers,\n",
    "              model={\"custom_model\": custom_model, \n",
    "                     \"custom_model_config\": custom_model_config}\n",
    "              )\n",
    "    .evaluation(evaluation_config={\"explore\": False},\n",
    "                evaluation_interval=1, \n",
    "                evaluation_duration=100,\n",
    "              )\n",
    "    .debugging(log_level=args.log_level)\n",
    "    .framework(eager_tracing=True)\n",
    ")\n",
    "\n",
    "stop = {\n",
    "    \"training_iteration\": args.stop_iters,\n",
    "    \"timesteps_total\": args.stop_timesteps,\n",
    "    \"episode_reward_mean\": args.stop_reward,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d29b8901-6bec-42a3-a0f6-97358ab62ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_path = Path(\"/home/vladimir/work/graph_test/scratch/ray_results\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e7557961-396a-49cf-839c-6251fc2f7d07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2025-05-10 16:31:00</td></tr>\n",
       "<tr><td>Running for: </td><td>00:02:16.92        </td></tr>\n",
       "<tr><td>Memory:      </td><td>5.0/125.7 GiB      </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      Using FIFO scheduling algorithm.<br>Resources requested: 0/20 CPUs, 0/2 GPUs, 0.0/77.91 GiB heap, 0.0/37.38 GiB objects (0.0/1.0 accelerator_type:G)\n",
       "    </div>\n",
       "    \n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name                   </th><th>status    </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_mygraphenv-v0_d6e5d_00000</td><td>TERMINATED</td><td>192.168.0.126:35792</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">         122.872</td><td style=\"text-align: right;\">2000</td><td style=\"text-align: right;\">-1.69171</td><td style=\"text-align: right;\">            -1.58714</td><td style=\"text-align: right;\">            -1.99563</td><td style=\"text-align: right;\">                 5</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-10 16:28:43,614\tINFO algorithm_config.py:2888 -- Executing eagerly (framework='tf2'), with eager_tracing=tf2. For production workloads, make sure to set eager_tracing=True  in order to match the speed of tf-static-graph (framework='tf'). For debugging purposes, `eager_tracing=False` is the best choice.\n",
      "2025-05-10 16:28:43,615\tINFO algorithm_config.py:2888 -- Executing eagerly (framework='tf2'), with eager_tracing=tf2. For production workloads, make sure to set eager_tracing=True  in order to match the speed of tf-static-graph (framework='tf'). For debugging purposes, `eager_tracing=False` is the best choice.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"trialProgress\">\n",
       "  <h3>Trial Progress</h3>\n",
       "  <table>\n",
       "<thead>\n",
       "<tr><th>Trial name                   </th><th style=\"text-align: right;\">  agent_timesteps_total</th><th>connector_metrics                                                                                                                                             </th><th>counters                                                                                                                        </th><th>custom_metrics  </th><th>date               </th><th>done  </th><th style=\"text-align: right;\">  episode_len_mean</th><th>episode_media  </th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_mean</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episodes_this_iter</th><th style=\"text-align: right;\">  episodes_total</th><th>evaluation                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      </th><th>experiment_id                   </th><th>hostname  </th><th>info                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 </th><th style=\"text-align: right;\">  iterations_since_restore</th><th>node_ip      </th><th style=\"text-align: right;\">  num_agent_steps_sampled</th><th style=\"text-align: right;\">  num_agent_steps_trained</th><th style=\"text-align: right;\">  num_env_steps_sampled</th><th style=\"text-align: right;\">  num_env_steps_sampled_this_iter</th><th style=\"text-align: right;\">  num_env_steps_trained</th><th style=\"text-align: right;\">  num_env_steps_trained_this_iter</th><th style=\"text-align: right;\">  num_faulty_episodes</th><th style=\"text-align: right;\">  num_healthy_workers</th><th style=\"text-align: right;\">  num_in_flight_async_reqs</th><th style=\"text-align: right;\">  num_remote_worker_restarts</th><th style=\"text-align: right;\">  num_steps_trained_this_iter</th><th>perf                                              </th><th style=\"text-align: right;\">  pid</th><th>policy_reward_max  </th><th>policy_reward_mean  </th><th>policy_reward_min  </th><th>sampler_perf                                                                                                                                                                                                     </th><th>sampler_results                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          </th><th style=\"text-align: right;\">  time_since_restore</th><th style=\"text-align: right;\">  time_this_iter_s</th><th style=\"text-align: right;\">  time_total_s</th><th>timers                                                                                                                        </th><th style=\"text-align: right;\">  timestamp</th><th style=\"text-align: right;\">  timesteps_since_restore</th><th style=\"text-align: right;\">  timesteps_total</th><th style=\"text-align: right;\">  training_iteration</th><th>trial_id   </th><th style=\"text-align: right;\">  warmup_time</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_mygraphenv-v0_d6e5d_00000</td><td style=\"text-align: right;\">                   2000</td><td>{&#x27;ObsPreprocessorConnector_ms&#x27;: 0.04930233955383301, &#x27;StateBufferConnector_ms&#x27;: 0.007714986801147461, &#x27;ViewRequirementAgentConnector_ms&#x27;: 0.10429954528808594}</td><td>{&#x27;num_env_steps_sampled&#x27;: 2000, &#x27;num_env_steps_trained&#x27;: 2000, &#x27;num_agent_steps_sampled&#x27;: 2000, &#x27;num_agent_steps_trained&#x27;: 2000}</td><td>{}              </td><td>2025-05-10_16-31-00</td><td>True  </td><td style=\"text-align: right;\">                 5</td><td>{}             </td><td style=\"text-align: right;\">            -1.58714</td><td style=\"text-align: right;\">             -1.69171</td><td style=\"text-align: right;\">            -1.99563</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">             400</td><td>{&#x27;episode_reward_max&#x27;: -1.5871424799869809, &#x27;episode_reward_min&#x27;: -1.5871424799869809, &#x27;episode_reward_mean&#x27;: -1.587142479986981, &#x27;episode_len_mean&#x27;: 5.0, &#x27;episode_media&#x27;: {}, &#x27;episodes_this_iter&#x27;: 100, &#x27;policy_reward_min&#x27;: {}, &#x27;policy_reward_max&#x27;: {}, &#x27;policy_reward_mean&#x27;: {}, &#x27;custom_metrics&#x27;: {}, &#x27;hist_stats&#x27;: {&#x27;episode_reward&#x27;: [-1.5871424799869809, -1.5871424799869809, -1.5871424799869809, -1.5871424799869809, -1.5871424799869809, -1.5871424799869809, -1.5871424799869809, -1.5871424799869809, -1.5871424799869809, -1.5871424799869809, -1.5871424799869809, -1.5871424799869809, -1.5871424799869809, -1.5871424799869809, -1.5871424799869809, -1.5871424799869809, -1.5871424799869809, -1.5871424799869809, -1.5871424799869809, -1.5871424799869809, -1.5871424799869809, -1.5871424799869809, -1.5871424799869809, -1.5871424799869809, -1.5871424799869809, -1.5871424799869809, -1.5871424799869809, -1.5871424799869809, -1.5871424799869809, -1.5871424799869809, -1.5871424799869809, -1.5871424799869809, -1.5871424799869809, -1.5871424799869809, -1.5871424799869809, -1.5871424799869809, -1.5871424799869809, -1.5871424799869809, -1.5871424799869809, -1.5871424799869809, -1.5871424799869809, -1.5871424799869809, -1.5871424799869809, -1.5871424799869809, -1.5871424799869809, -1.5871424799869809, -1.5871424799869809, -1.5871424799869809, -1.5871424799869809, -1.5871424799869809, -1.5871424799869809, -1.5871424799869809, -1.5871424799869809, -1.5871424799869809, -1.5871424799869809, -1.5871424799869809, -1.5871424799869809, -1.5871424799869809, -1.5871424799869809, -1.5871424799869809, -1.5871424799869809, -1.5871424799869809, -1.5871424799869809, -1.5871424799869809, -1.5871424799869809, -1.5871424799869809, -1.5871424799869809, -1.5871424799869809, -1.5871424799869809, -1.5871424799869809, -1.5871424799869809, -1.5871424799869809, -1.5871424799869809, -1.5871424799869809, -1.5871424799869809, -1.5871424799869809, -1.5871424799869809, -1.5871424799869809, -1.5871424799869809, -1.5871424799869809, -1.5871424799869809, -1.5871424799869809, -1.5871424799869809, -1.5871424799869809, -1.5871424799869809, -1.5871424799869809, -1.5871424799869809, -1.5871424799869809, -1.5871424799869809, -1.5871424799869809, -1.5871424799869809, -1.5871424799869809, -1.5871424799869809, -1.5871424799869809, -1.5871424799869809, -1.5871424799869809, -1.5871424799869809, -1.5871424799869809, -1.5871424799869809, -1.5871424799869809], &#x27;episode_lengths&#x27;: [5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5]}, &#x27;sampler_perf&#x27;: {&#x27;mean_raw_obs_processing_ms&#x27;: 0.4904832065788494, &#x27;mean_inference_ms&#x27;: 1.2785140396282646, &#x27;mean_action_processing_ms&#x27;: 0.08887698756644978, &#x27;mean_env_wait_ms&#x27;: 0.06635997136166893, &#x27;mean_env_render_ms&#x27;: 0.0}, &#x27;num_faulty_episodes&#x27;: 0, &#x27;connector_metrics&#x27;: {&#x27;ObsPreprocessorConnector_ms&#x27;: 0.039293766021728516, &#x27;StateBufferConnector_ms&#x27;: 0.0059163570404052734, &#x27;ViewRequirementAgentConnector_ms&#x27;: 0.0808725357055664}, &#x27;num_agent_steps_sampled_this_iter&#x27;: 500, &#x27;num_env_steps_sampled_this_iter&#x27;: 500, &#x27;timesteps_this_iter&#x27;: 500, &#x27;num_healthy_workers&#x27;: 0, &#x27;num_in_flight_async_reqs&#x27;: 0, &#x27;num_remote_worker_restarts&#x27;: 0}</td><td>4420fb27cf544c28bed11a398768ab77</td><td>srv5      </td><td>{&#x27;learner&#x27;: {&#x27;default_policy&#x27;: {&#x27;learner_stats&#x27;: {&#x27;cur_kl_coeff&#x27;: 1.413315605007752e-24, &#x27;cur_lr&#x27;: 9.999999747378752e-05, &#x27;total_loss&#x27;: 3.9262173, &#x27;policy_loss&#x27;: -0.075419456, &#x27;vf_loss&#x27;: 4.0016365, &#x27;vf_explained_var&#x27;: -0.34990507, &#x27;kl&#x27;: 0.052687388, &#x27;entropy&#x27;: 0.34859613, &#x27;entropy_coeff&#x27;: 0.0}, &#x27;custom_metrics&#x27;: {}, &#x27;num_agent_steps_trained&#x27;: 2.0, &#x27;num_grad_updates_lifetime&#x27;: 4975.5, &#x27;diff_num_grad_updates_vs_sampler_policy&#x27;: 24.5}}, &#x27;num_env_steps_sampled&#x27;: 2000, &#x27;num_env_steps_trained&#x27;: 2000, &#x27;num_agent_steps_sampled&#x27;: 2000, &#x27;num_agent_steps_trained&#x27;: 2000}</td><td style=\"text-align: right;\">                       100</td><td>192.168.0.126</td><td style=\"text-align: right;\">                     2000</td><td style=\"text-align: right;\">                     2000</td><td style=\"text-align: right;\">                   2000</td><td style=\"text-align: right;\">                               20</td><td style=\"text-align: right;\">                   2000</td><td style=\"text-align: right;\">                               20</td><td style=\"text-align: right;\">                    0</td><td style=\"text-align: right;\">                    4</td><td style=\"text-align: right;\">                         0</td><td style=\"text-align: right;\">                           0</td><td style=\"text-align: right;\">                           20</td><td>{&#x27;cpu_util_percent&#x27;: 6.8, &#x27;ram_util_percent&#x27;: 4.0}</td><td style=\"text-align: right;\">35792</td><td>{}                 </td><td>{}                  </td><td>{}                 </td><td>{&#x27;mean_raw_obs_processing_ms&#x27;: 0.6470959908709477, &#x27;mean_inference_ms&#x27;: 3.7316520334681895, &#x27;mean_action_processing_ms&#x27;: 0.10954066127132983, &#x27;mean_env_wait_ms&#x27;: 0.08781922826226754, &#x27;mean_env_render_ms&#x27;: 0.0}</td><td>{&#x27;episode_reward_max&#x27;: -1.5871424799869809, &#x27;episode_reward_min&#x27;: -1.9956256435123456, &#x27;episode_reward_mean&#x27;: -1.691708655250344, &#x27;episode_len_mean&#x27;: 5.0, &#x27;episode_media&#x27;: {}, &#x27;episodes_this_iter&#x27;: 4, &#x27;policy_reward_min&#x27;: {}, &#x27;policy_reward_max&#x27;: {}, &#x27;policy_reward_mean&#x27;: {}, &#x27;custom_metrics&#x27;: {}, &#x27;hist_stats&#x27;: {&#x27;episode_reward&#x27;: [-1.9956256435123456, -1.684290210383738, -1.9956256435123456, -1.5871424799869809, -1.684290210383738, -1.9956256435123456, -1.9956256435123456, -1.684290210383738, -1.7000894302075489, -1.9956256435123456, -1.5871424799869809, -1.893276848138505, -1.5871424799869809, -1.5871424799869809, -1.893276848138505, -1.5871424799869809, -1.893276848138505, -1.9956256435123456, -1.893276848138505, -1.5871424799869809, -1.9956256435123456, -1.684290210383738, -1.684290210383738, -1.9956256435123456, -1.7000894302075489, -1.6948883652304656, -1.5871424799869809, -1.5871424799869809, -1.7000894302075489, -1.6948883652304656, -1.5871424799869809, -1.7000894302075489, -1.7000894302075489, -1.6293851214904962, -1.6293851214904962, -1.7000894302075489, -1.5871424799869809, -1.5871424799869809, -1.5871424799869809, -1.5871424799869809, -1.7000894302075489, -1.6948883652304656, -1.6948883652304656, -1.6948883652304656, -1.9956256435123456, -1.5871424799869809, -1.5871424799869809, -1.5871424799869809, -1.684290210383738, -1.5871424799869809, -1.893276848138505, -1.7000894302075489, -1.7000894302075489, -1.5871424799869809, -1.6293851214904962, -1.7000894302075489, -1.7000894302075489, -1.7000894302075489, -1.7000894302075489, -1.6293851214904962, -1.7317339168643366, -1.6948883652304656, -1.5871424799869809, -1.7000894302075489, -1.5871424799869809, -1.6948883652304656, -1.5871424799869809, -1.7000894302075489, -1.6842902103837378, -1.6842902103837378, -1.5871424799869809, -1.7000894302075489, -1.5871424799869809, -1.5871424799869809, -1.5871424799869809, -1.5871424799869809, -1.5871424799869809, -1.5871424799869809, -1.6948883652304656, -1.5871424799869809, -1.6948883652304656, -1.6842902103837378, -1.5871424799869809, -1.5871424799869809, -1.9407205546191035, -1.893276848138505, -1.5871424799869809, -1.6293851214904962, -1.6842902103837378, -1.6842902103837378, -1.6842902103837378, -1.6842902103837378, -1.6842902103837378, -1.5871424799869809, -1.6842902103837378, -1.5871424799869809, -1.5871424799869809, -1.5871424799869809, -1.5871424799869809, -1.5871424799869809], &#x27;episode_lengths&#x27;: [5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5]}, &#x27;sampler_perf&#x27;: {&#x27;mean_raw_obs_processing_ms&#x27;: 0.6470959908709477, &#x27;mean_inference_ms&#x27;: 3.7316520334681895, &#x27;mean_action_processing_ms&#x27;: 0.10954066127132983, &#x27;mean_env_wait_ms&#x27;: 0.08781922826226754, &#x27;mean_env_render_ms&#x27;: 0.0}, &#x27;num_faulty_episodes&#x27;: 0, &#x27;connector_metrics&#x27;: {&#x27;ObsPreprocessorConnector_ms&#x27;: 0.04930233955383301, &#x27;StateBufferConnector_ms&#x27;: 0.007714986801147461, &#x27;ViewRequirementAgentConnector_ms&#x27;: 0.10429954528808594}}</td><td style=\"text-align: right;\">             122.872</td><td style=\"text-align: right;\">           1.15698</td><td style=\"text-align: right;\">       122.872</td><td>{&#x27;training_iteration_time_ms&#x27;: 184.586, &#x27;learn_time_ms&#x27;: 161.292, &#x27;learn_throughput&#x27;: 123.999, &#x27;synch_weights_time_ms&#x27;: 7.323}</td><td style=\"text-align: right;\"> 1746894660</td><td style=\"text-align: right;\">                        0</td><td style=\"text-align: right;\">             2000</td><td style=\"text-align: right;\">                 100</td><td>d6e5d_00000</td><td style=\"text-align: right;\">      8.63571</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "</div>\n",
       "<style>\n",
       ".trialProgress {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".trialProgress h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".trialProgress td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-10 16:31:00,706\tINFO tune.py:798 -- Total run time: 137.17 seconds (136.88 seconds for the tuning loop).\n"
     ]
    }
   ],
   "source": [
    "res = tune.run(\n",
    "    args.run,\n",
    "    config=run_config.to_dict(),\n",
    "    stop=stop,\n",
    "    local_dir=my_path,\n",
    "    checkpoint_freq = 10,\n",
    "    checkpoint_at_end=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4eae4fe9-c52d-4451-884b-7bed2662d361",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-10 16:31:00,735\tINFO experiment_analysis.py:789 -- No `self.trials`. Drawing logdirs from checkpoint file. This may result in some information that is out of sync, as checkpointing is periodic.\n"
     ]
    }
   ],
   "source": [
    "# Загружаем результаты\n",
    "analysis = ExperimentAnalysis(my_path)\n",
    "best_trial = analysis.get_best_trial(metric=\"episode_reward_mean\", mode=\"max\")\n",
    "best_checkpoint = analysis.get_best_checkpoint(best_trial, mode = \"max\")\n",
    "#best_checkpoint = analysis.get_last_checkpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f642ad15-8584-4449-a466-7bbbedec047e",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = GraphEnv({\"state\": state, \n",
    "          \"max_num_children\": G.number_of_nodes()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4f6df3ee-a3a5-42e8-a2c5-cc619b021e77",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-10 18:47:51,084\tINFO algorithm_config.py:2888 -- Executing eagerly (framework='tf2'), with eager_tracing=tf2. For production workloads, make sure to set eager_tracing=True  in order to match the speed of tf-static-graph (framework='tf'). For debugging purposes, `eager_tracing=False` is the best choice.\n",
      "2025-05-10 18:47:55,984\tINFO worker.py:1553 -- Started a local Ray instance.\n",
      "\u001b[2m\u001b[36m(pid=40253)\u001b[0m WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "\u001b[2m\u001b[36m(pid=40253)\u001b[0m I0000 00:00:1746902878.216445   40253 fork_posix.cc:75] Other threads are currently calling into gRPC, skipping fork() handlers\n",
      "\u001b[2m\u001b[36m(pid=40249)\u001b[0m WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "\u001b[2m\u001b[36m(pid=40249)\u001b[0m I0000 00:00:1746902878.236348   40249 fork_posix.cc:75] Other threads are currently calling into gRPC, skipping fork() handlers\n",
      "\u001b[2m\u001b[36m(pid=40252)\u001b[0m WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "\u001b[2m\u001b[36m(pid=40252)\u001b[0m I0000 00:00:1746902878.235735   40252 fork_posix.cc:75] Other threads are currently calling into gRPC, skipping fork() handlers\n",
      "\u001b[2m\u001b[36m(pid=40250)\u001b[0m WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "\u001b[2m\u001b[36m(pid=40250)\u001b[0m I0000 00:00:1746902878.250799   40250 fork_posix.cc:75] Other threads are currently calling into gRPC, skipping fork() handlers\n",
      "2025-05-10 18:47:59,997\tERROR actor_manager.py:496 -- Ray error, taking actor 1 out of service. The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=40249, ip=192.168.0.126, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7716cba743d0>)\n",
      "  File \"/home/vladimir/.pyenv/versions/3.9.21/lib/python3.9/site-packages/gymnasium/envs/registration.py\", line 569, in make\n",
      "    _check_version_exists(ns, name, version)\n",
      "  File \"/home/vladimir/.pyenv/versions/3.9.21/lib/python3.9/site-packages/gymnasium/envs/registration.py\", line 219, in _check_version_exists\n",
      "    _check_name_exists(ns, name)\n",
      "  File \"/home/vladimir/.pyenv/versions/3.9.21/lib/python3.9/site-packages/gymnasium/envs/registration.py\", line 197, in _check_name_exists\n",
      "    raise error.NameNotFound(\n",
      "gymnasium.error.NameNotFound: Environment mygraphenv doesn't exist. \n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "\u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=40249, ip=192.168.0.126, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7716cba743d0>)\n",
      "  File \"/home/vladimir/.pyenv/versions/3.9.21/lib/python3.9/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 607, in __init__\n",
      "    self.env = env_creator(copy.deepcopy(self.env_context))\n",
      "  File \"/home/vladimir/.pyenv/versions/3.9.21/lib/python3.9/site-packages/ray/rllib/env/utils.py\", line 178, in _gym_env_creator\n",
      "    raise EnvError(ERR_MSG_INVALID_ENV_DESCRIPTOR.format(env_descriptor))\n",
      "ray.rllib.utils.error.EnvError: The env string you provided ('mygraphenv-v0') is:\n",
      "a) Not a supported/installed environment.\n",
      "b) Not a tune-registered environment creator.\n",
      "c) Not a valid env class string.\n",
      "\n",
      "Try one of the following:\n",
      "a) For Atari support: `pip install gym[atari] autorom[accept-rom-license]`.\n",
      "   For VizDoom support: Install VizDoom\n",
      "   (https://github.com/mwydmuch/ViZDoom/blob/master/doc/Building.md) and\n",
      "   `pip install vizdoomgym`.\n",
      "   For PyBullet support: `pip install pybullet`.\n",
      "b) To register your custom env, do `from ray import tune;\n",
      "   tune.register('[name]', lambda cfg: [return env obj from here using cfg])`.\n",
      "   Then in your config, do `config['env'] = [name]`.\n",
      "c) Make sure you provide a fully qualified classpath, e.g.:\n",
      "   `ray.rllib.examples.env.repeat_after_me_env.RepeatAfterMeEnv`\n",
      "2025-05-10 18:47:59,998\tERROR actor_manager.py:496 -- Ray error, taking actor 2 out of service. The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=40250, ip=192.168.0.126, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7e0710c933d0>)\n",
      "  File \"/home/vladimir/.pyenv/versions/3.9.21/lib/python3.9/site-packages/gymnasium/envs/registration.py\", line 569, in make\n",
      "    _check_version_exists(ns, name, version)\n",
      "  File \"/home/vladimir/.pyenv/versions/3.9.21/lib/python3.9/site-packages/gymnasium/envs/registration.py\", line 219, in _check_version_exists\n",
      "    _check_name_exists(ns, name)\n",
      "  File \"/home/vladimir/.pyenv/versions/3.9.21/lib/python3.9/site-packages/gymnasium/envs/registration.py\", line 197, in _check_name_exists\n",
      "    raise error.NameNotFound(\n",
      "gymnasium.error.NameNotFound: Environment mygraphenv doesn't exist. \n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "\u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=40250, ip=192.168.0.126, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7e0710c933d0>)\n",
      "  File \"/home/vladimir/.pyenv/versions/3.9.21/lib/python3.9/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 607, in __init__\n",
      "    self.env = env_creator(copy.deepcopy(self.env_context))\n",
      "  File \"/home/vladimir/.pyenv/versions/3.9.21/lib/python3.9/site-packages/ray/rllib/env/utils.py\", line 178, in _gym_env_creator\n",
      "    raise EnvError(ERR_MSG_INVALID_ENV_DESCRIPTOR.format(env_descriptor))\n",
      "ray.rllib.utils.error.EnvError: The env string you provided ('mygraphenv-v0') is:\n",
      "a) Not a supported/installed environment.\n",
      "b) Not a tune-registered environment creator.\n",
      "c) Not a valid env class string.\n",
      "\n",
      "Try one of the following:\n",
      "a) For Atari support: `pip install gym[atari] autorom[accept-rom-license]`.\n",
      "   For VizDoom support: Install VizDoom\n",
      "   (https://github.com/mwydmuch/ViZDoom/blob/master/doc/Building.md) and\n",
      "   `pip install vizdoomgym`.\n",
      "   For PyBullet support: `pip install pybullet`.\n",
      "b) To register your custom env, do `from ray import tune;\n",
      "   tune.register('[name]', lambda cfg: [return env obj from here using cfg])`.\n",
      "   Then in your config, do `config['env'] = [name]`.\n",
      "c) Make sure you provide a fully qualified classpath, e.g.:\n",
      "   `ray.rllib.examples.env.repeat_after_me_env.RepeatAfterMeEnv`\n",
      "2025-05-10 18:47:59,998\tERROR actor_manager.py:496 -- Ray error, taking actor 3 out of service. The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=40252, ip=192.168.0.126, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x77cfeecd34c0>)\n",
      "  File \"/home/vladimir/.pyenv/versions/3.9.21/lib/python3.9/site-packages/gymnasium/envs/registration.py\", line 569, in make\n",
      "    _check_version_exists(ns, name, version)\n",
      "  File \"/home/vladimir/.pyenv/versions/3.9.21/lib/python3.9/site-packages/gymnasium/envs/registration.py\", line 219, in _check_version_exists\n",
      "    _check_name_exists(ns, name)\n",
      "  File \"/home/vladimir/.pyenv/versions/3.9.21/lib/python3.9/site-packages/gymnasium/envs/registration.py\", line 197, in _check_name_exists\n",
      "    raise error.NameNotFound(\n",
      "gymnasium.error.NameNotFound: Environment mygraphenv doesn't exist. \n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "\u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=40252, ip=192.168.0.126, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x77cfeecd34c0>)\n",
      "  File \"/home/vladimir/.pyenv/versions/3.9.21/lib/python3.9/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 607, in __init__\n",
      "    self.env = env_creator(copy.deepcopy(self.env_context))\n",
      "  File \"/home/vladimir/.pyenv/versions/3.9.21/lib/python3.9/site-packages/ray/rllib/env/utils.py\", line 178, in _gym_env_creator\n",
      "    raise EnvError(ERR_MSG_INVALID_ENV_DESCRIPTOR.format(env_descriptor))\n",
      "ray.rllib.utils.error.EnvError: The env string you provided ('mygraphenv-v0') is:\n",
      "a) Not a supported/installed environment.\n",
      "b) Not a tune-registered environment creator.\n",
      "c) Not a valid env class string.\n",
      "\n",
      "Try one of the following:\n",
      "a) For Atari support: `pip install gym[atari] autorom[accept-rom-license]`.\n",
      "   For VizDoom support: Install VizDoom\n",
      "   (https://github.com/mwydmuch/ViZDoom/blob/master/doc/Building.md) and\n",
      "   `pip install vizdoomgym`.\n",
      "   For PyBullet support: `pip install pybullet`.\n",
      "b) To register your custom env, do `from ray import tune;\n",
      "   tune.register('[name]', lambda cfg: [return env obj from here using cfg])`.\n",
      "   Then in your config, do `config['env'] = [name]`.\n",
      "c) Make sure you provide a fully qualified classpath, e.g.:\n",
      "   `ray.rllib.examples.env.repeat_after_me_env.RepeatAfterMeEnv`\n",
      "2025-05-10 18:47:59,999\tERROR actor_manager.py:496 -- Ray error, taking actor 4 out of service. The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=40253, ip=192.168.0.126, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x745583692430>)\n",
      "  File \"/home/vladimir/.pyenv/versions/3.9.21/lib/python3.9/site-packages/gymnasium/envs/registration.py\", line 569, in make\n",
      "    _check_version_exists(ns, name, version)\n",
      "  File \"/home/vladimir/.pyenv/versions/3.9.21/lib/python3.9/site-packages/gymnasium/envs/registration.py\", line 219, in _check_version_exists\n",
      "    _check_name_exists(ns, name)\n",
      "  File \"/home/vladimir/.pyenv/versions/3.9.21/lib/python3.9/site-packages/gymnasium/envs/registration.py\", line 197, in _check_name_exists\n",
      "    raise error.NameNotFound(\n",
      "gymnasium.error.NameNotFound: Environment mygraphenv doesn't exist. \n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "\u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=40253, ip=192.168.0.126, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x745583692430>)\n",
      "  File \"/home/vladimir/.pyenv/versions/3.9.21/lib/python3.9/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 607, in __init__\n",
      "    self.env = env_creator(copy.deepcopy(self.env_context))\n",
      "  File \"/home/vladimir/.pyenv/versions/3.9.21/lib/python3.9/site-packages/ray/rllib/env/utils.py\", line 178, in _gym_env_creator\n",
      "    raise EnvError(ERR_MSG_INVALID_ENV_DESCRIPTOR.format(env_descriptor))\n",
      "ray.rllib.utils.error.EnvError: The env string you provided ('mygraphenv-v0') is:\n",
      "a) Not a supported/installed environment.\n",
      "b) Not a tune-registered environment creator.\n",
      "c) Not a valid env class string.\n",
      "\n",
      "Try one of the following:\n",
      "a) For Atari support: `pip install gym[atari] autorom[accept-rom-license]`.\n",
      "   For VizDoom support: Install VizDoom\n",
      "   (https://github.com/mwydmuch/ViZDoom/blob/master/doc/Building.md) and\n",
      "   `pip install vizdoomgym`.\n",
      "   For PyBullet support: `pip install pybullet`.\n",
      "b) To register your custom env, do `from ray import tune;\n",
      "   tune.register('[name]', lambda cfg: [return env obj from here using cfg])`.\n",
      "   Then in your config, do `config['env'] = [name]`.\n",
      "c) Make sure you provide a fully qualified classpath, e.g.:\n",
      "   `ray.rllib.examples.env.repeat_after_me_env.RepeatAfterMeEnv`\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=40253)\u001b[0m 2025-05-10 18:47:59,969\tERROR worker.py:772 -- Exception raised in creation task: The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=40253, ip=192.168.0.126, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x745583692430>)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=40253)\u001b[0m   File \"/home/vladimir/.pyenv/versions/3.9.21/lib/python3.9/site-packages/gymnasium/envs/registration.py\", line 569, in make\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=40253)\u001b[0m     _check_version_exists(ns, name, version)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=40253)\u001b[0m   File \"/home/vladimir/.pyenv/versions/3.9.21/lib/python3.9/site-packages/gymnasium/envs/registration.py\", line 219, in _check_version_exists\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=40253)\u001b[0m     _check_name_exists(ns, name)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=40253)\u001b[0m   File \"/home/vladimir/.pyenv/versions/3.9.21/lib/python3.9/site-packages/gymnasium/envs/registration.py\", line 197, in _check_name_exists\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=40253)\u001b[0m     raise error.NameNotFound(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=40253)\u001b[0m gymnasium.error.NameNotFound: Environment mygraphenv doesn't exist. \n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=40253)\u001b[0m \n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=40253)\u001b[0m During handling of the above exception, another exception occurred:\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=40253)\u001b[0m \n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=40253)\u001b[0m \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=40253, ip=192.168.0.126, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x745583692430>)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=40253)\u001b[0m   File \"/home/vladimir/.pyenv/versions/3.9.21/lib/python3.9/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 607, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=40253)\u001b[0m     self.env = env_creator(copy.deepcopy(self.env_context))\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=40253)\u001b[0m   File \"/home/vladimir/.pyenv/versions/3.9.21/lib/python3.9/site-packages/ray/rllib/env/utils.py\", line 178, in _gym_env_creator\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=40253)\u001b[0m     raise EnvError(ERR_MSG_INVALID_ENV_DESCRIPTOR.format(env_descriptor))\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=40253)\u001b[0m ray.rllib.utils.error.EnvError: The env string you provided ('mygraphenv-v0') is:\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=40253)\u001b[0m a) Not a supported/installed environment.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=40253)\u001b[0m b) Not a tune-registered environment creator.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=40253)\u001b[0m c) Not a valid env class string.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=40253)\u001b[0m \n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=40253)\u001b[0m Try one of the following:\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=40253)\u001b[0m a) For Atari support: `pip install gym[atari] autorom[accept-rom-license]`.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=40253)\u001b[0m    For VizDoom support: Install VizDoom\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=40253)\u001b[0m    (https://github.com/mwydmuch/ViZDoom/blob/master/doc/Building.md) and\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=40253)\u001b[0m    `pip install vizdoomgym`.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=40253)\u001b[0m    For PyBullet support: `pip install pybullet`.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=40253)\u001b[0m b) To register your custom env, do `from ray import tune;\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=40253)\u001b[0m    tune.register('[name]', lambda cfg: [return env obj from here using cfg])`.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=40253)\u001b[0m    Then in your config, do `config['env'] = [name]`.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=40253)\u001b[0m c) Make sure you provide a fully qualified classpath, e.g.:\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=40253)\u001b[0m    `ray.rllib.examples.env.repeat_after_me_env.RepeatAfterMeEnv`\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=40249)\u001b[0m 2025-05-10 18:47:59,976\tERROR worker.py:772 -- Exception raised in creation task: The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=40249, ip=192.168.0.126, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7716cba743d0>)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=40249)\u001b[0m   File \"/home/vladimir/.pyenv/versions/3.9.21/lib/python3.9/site-packages/gymnasium/envs/registration.py\", line 569, in make\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=40249)\u001b[0m     _check_version_exists(ns, name, version)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=40249)\u001b[0m   File \"/home/vladimir/.pyenv/versions/3.9.21/lib/python3.9/site-packages/gymnasium/envs/registration.py\", line 219, in _check_version_exists\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=40249)\u001b[0m     _check_name_exists(ns, name)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=40249)\u001b[0m   File \"/home/vladimir/.pyenv/versions/3.9.21/lib/python3.9/site-packages/gymnasium/envs/registration.py\", line 197, in _check_name_exists\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=40249)\u001b[0m     raise error.NameNotFound(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=40249)\u001b[0m gymnasium.error.NameNotFound: Environment mygraphenv doesn't exist. \n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=40249)\u001b[0m \n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=40249)\u001b[0m During handling of the above exception, another exception occurred:\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=40249)\u001b[0m \n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=40249)\u001b[0m \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=40249, ip=192.168.0.126, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7716cba743d0>)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=40249)\u001b[0m   File \"/home/vladimir/.pyenv/versions/3.9.21/lib/python3.9/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 607, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=40249)\u001b[0m     self.env = env_creator(copy.deepcopy(self.env_context))\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=40249)\u001b[0m   File \"/home/vladimir/.pyenv/versions/3.9.21/lib/python3.9/site-packages/ray/rllib/env/utils.py\", line 178, in _gym_env_creator\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=40249)\u001b[0m     raise EnvError(ERR_MSG_INVALID_ENV_DESCRIPTOR.format(env_descriptor))\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=40249)\u001b[0m ray.rllib.utils.error.EnvError: The env string you provided ('mygraphenv-v0') is:\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=40249)\u001b[0m a) Not a supported/installed environment.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=40249)\u001b[0m b) Not a tune-registered environment creator.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=40249)\u001b[0m c) Not a valid env class string.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=40249)\u001b[0m \n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=40249)\u001b[0m Try one of the following:\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=40249)\u001b[0m a) For Atari support: `pip install gym[atari] autorom[accept-rom-license]`.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=40249)\u001b[0m    For VizDoom support: Install VizDoom\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=40249)\u001b[0m    (https://github.com/mwydmuch/ViZDoom/blob/master/doc/Building.md) and\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=40249)\u001b[0m    `pip install vizdoomgym`.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=40249)\u001b[0m    For PyBullet support: `pip install pybullet`.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=40249)\u001b[0m b) To register your custom env, do `from ray import tune;\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=40249)\u001b[0m    tune.register('[name]', lambda cfg: [return env obj from here using cfg])`.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=40249)\u001b[0m    Then in your config, do `config['env'] = [name]`.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=40249)\u001b[0m c) Make sure you provide a fully qualified classpath, e.g.:\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=40249)\u001b[0m    `ray.rllib.examples.env.repeat_after_me_env.RepeatAfterMeEnv`\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=40252)\u001b[0m 2025-05-10 18:47:59,994\tERROR worker.py:772 -- Exception raised in creation task: The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=40252, ip=192.168.0.126, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x77cfeecd34c0>)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=40252)\u001b[0m   File \"/home/vladimir/.pyenv/versions/3.9.21/lib/python3.9/site-packages/gymnasium/envs/registration.py\", line 569, in make\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=40252)\u001b[0m     _check_version_exists(ns, name, version)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=40252)\u001b[0m   File \"/home/vladimir/.pyenv/versions/3.9.21/lib/python3.9/site-packages/gymnasium/envs/registration.py\", line 219, in _check_version_exists\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=40252)\u001b[0m     _check_name_exists(ns, name)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=40252)\u001b[0m   File \"/home/vladimir/.pyenv/versions/3.9.21/lib/python3.9/site-packages/gymnasium/envs/registration.py\", line 197, in _check_name_exists\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=40252)\u001b[0m     raise error.NameNotFound(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=40252)\u001b[0m gymnasium.error.NameNotFound: Environment mygraphenv doesn't exist. \n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=40252)\u001b[0m \n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=40252)\u001b[0m During handling of the above exception, another exception occurred:\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=40252)\u001b[0m \n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=40252)\u001b[0m \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=40252, ip=192.168.0.126, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x77cfeecd34c0>)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=40252)\u001b[0m   File \"/home/vladimir/.pyenv/versions/3.9.21/lib/python3.9/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 607, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=40252)\u001b[0m     self.env = env_creator(copy.deepcopy(self.env_context))\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=40252)\u001b[0m   File \"/home/vladimir/.pyenv/versions/3.9.21/lib/python3.9/site-packages/ray/rllib/env/utils.py\", line 178, in _gym_env_creator\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=40252)\u001b[0m     raise EnvError(ERR_MSG_INVALID_ENV_DESCRIPTOR.format(env_descriptor))\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=40252)\u001b[0m ray.rllib.utils.error.EnvError: The env string you provided ('mygraphenv-v0') is:\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=40252)\u001b[0m a) Not a supported/installed environment.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=40252)\u001b[0m b) Not a tune-registered environment creator.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=40252)\u001b[0m c) Not a valid env class string.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=40252)\u001b[0m \n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=40252)\u001b[0m Try one of the following:\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=40252)\u001b[0m a) For Atari support: `pip install gym[atari] autorom[accept-rom-license]`.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=40252)\u001b[0m    For VizDoom support: Install VizDoom\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=40252)\u001b[0m    (https://github.com/mwydmuch/ViZDoom/blob/master/doc/Building.md) and\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=40252)\u001b[0m    `pip install vizdoomgym`.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=40252)\u001b[0m    For PyBullet support: `pip install pybullet`.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=40252)\u001b[0m b) To register your custom env, do `from ray import tune;\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=40252)\u001b[0m    tune.register('[name]', lambda cfg: [return env obj from here using cfg])`.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=40252)\u001b[0m    Then in your config, do `config['env'] = [name]`.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=40252)\u001b[0m c) Make sure you provide a fully qualified classpath, e.g.:\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=40252)\u001b[0m    `ray.rllib.examples.env.repeat_after_me_env.RepeatAfterMeEnv`\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=40250)\u001b[0m 2025-05-10 18:47:59,994\tERROR worker.py:772 -- Exception raised in creation task: The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=40250, ip=192.168.0.126, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7e0710c933d0>)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=40250)\u001b[0m   File \"/home/vladimir/.pyenv/versions/3.9.21/lib/python3.9/site-packages/gymnasium/envs/registration.py\", line 569, in make\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=40250)\u001b[0m     _check_version_exists(ns, name, version)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=40250)\u001b[0m   File \"/home/vladimir/.pyenv/versions/3.9.21/lib/python3.9/site-packages/gymnasium/envs/registration.py\", line 219, in _check_version_exists\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=40250)\u001b[0m     _check_name_exists(ns, name)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=40250)\u001b[0m   File \"/home/vladimir/.pyenv/versions/3.9.21/lib/python3.9/site-packages/gymnasium/envs/registration.py\", line 197, in _check_name_exists\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=40250)\u001b[0m     raise error.NameNotFound(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=40250)\u001b[0m gymnasium.error.NameNotFound: Environment mygraphenv doesn't exist. \n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=40250)\u001b[0m \n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=40250)\u001b[0m During handling of the above exception, another exception occurred:\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=40250)\u001b[0m \n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=40250)\u001b[0m \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=40250, ip=192.168.0.126, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7e0710c933d0>)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=40250)\u001b[0m   File \"/home/vladimir/.pyenv/versions/3.9.21/lib/python3.9/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 607, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=40250)\u001b[0m     self.env = env_creator(copy.deepcopy(self.env_context))\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=40250)\u001b[0m   File \"/home/vladimir/.pyenv/versions/3.9.21/lib/python3.9/site-packages/ray/rllib/env/utils.py\", line 178, in _gym_env_creator\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=40250)\u001b[0m     raise EnvError(ERR_MSG_INVALID_ENV_DESCRIPTOR.format(env_descriptor))\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=40250)\u001b[0m ray.rllib.utils.error.EnvError: The env string you provided ('mygraphenv-v0') is:\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=40250)\u001b[0m a) Not a supported/installed environment.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=40250)\u001b[0m b) Not a tune-registered environment creator.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=40250)\u001b[0m c) Not a valid env class string.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=40250)\u001b[0m \n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=40250)\u001b[0m Try one of the following:\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=40250)\u001b[0m a) For Atari support: `pip install gym[atari] autorom[accept-rom-license]`.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=40250)\u001b[0m    For VizDoom support: Install VizDoom\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=40250)\u001b[0m    (https://github.com/mwydmuch/ViZDoom/blob/master/doc/Building.md) and\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=40250)\u001b[0m    `pip install vizdoomgym`.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=40250)\u001b[0m    For PyBullet support: `pip install pybullet`.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=40250)\u001b[0m b) To register your custom env, do `from ray import tune;\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=40250)\u001b[0m    tune.register('[name]', lambda cfg: [return env obj from here using cfg])`.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=40250)\u001b[0m    Then in your config, do `config['env'] = [name]`.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=40250)\u001b[0m c) Make sure you provide a fully qualified classpath, e.g.:\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=40250)\u001b[0m    `ray.rllib.examples.env.repeat_after_me_env.RepeatAfterMeEnv`\n"
     ]
    },
    {
     "ename": "EnvError",
     "evalue": "The env string you provided ('mygraphenv-v0') is:\na) Not a supported/installed environment.\nb) Not a tune-registered environment creator.\nc) Not a valid env class string.\n\nTry one of the following:\na) For Atari support: `pip install gym[atari] autorom[accept-rom-license]`.\n   For VizDoom support: Install VizDoom\n   (https://github.com/mwydmuch/ViZDoom/blob/master/doc/Building.md) and\n   `pip install vizdoomgym`.\n   For PyBullet support: `pip install pybullet`.\nb) To register your custom env, do `from ray import tune;\n   tune.register('[name]', lambda cfg: [return env obj from here using cfg])`.\n   Then in your config, do `config['env'] = [name]`.\nc) Make sure you provide a fully qualified classpath, e.g.:\n   `ray.rllib.examples.env.repeat_after_me_env.RepeatAfterMeEnv`\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRayActorError\u001b[0m                             Traceback (most recent call last)",
      "File \u001b[0;32m~/.pyenv/versions/3.9.21/lib/python3.9/site-packages/ray/rllib/evaluation/worker_set.py:170\u001b[0m, in \u001b[0;36mWorkerSet.__init__\u001b[0;34m(self, env_creator, validate_env, default_policy_class, config, num_workers, local_worker, logdir, _setup, policy_class, trainer_config)\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 170\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_setup\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalidate_env\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidate_env\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_workers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_workers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    174\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_worker\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_worker\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;66;03m# WorkerSet creation possibly fails, if some (remote) workers cannot\u001b[39;00m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;66;03m# be initialized properly (due to some errors in the RolloutWorker's\u001b[39;00m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;66;03m# constructor).\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.21/lib/python3.9/site-packages/ray/rllib/evaluation/worker_set.py:240\u001b[0m, in \u001b[0;36mWorkerSet._setup\u001b[0;34m(self, validate_env, config, num_workers, local_worker)\u001b[0m\n\u001b[1;32m    239\u001b[0m \u001b[38;5;66;03m# Create a number of @ray.remote workers.\u001b[39;00m\n\u001b[0;32m--> 240\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_workers\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    241\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_workers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    242\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidate_workers_after_construction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    243\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    245\u001b[0m \u001b[38;5;66;03m# If num_workers > 0 and we don't have an env on the local worker,\u001b[39;00m\n\u001b[1;32m    246\u001b[0m \u001b[38;5;66;03m# get the observation- and action spaces for each policy from\u001b[39;00m\n\u001b[1;32m    247\u001b[0m \u001b[38;5;66;03m# the first remote worker (which does have an env).\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.21/lib/python3.9/site-packages/ray/rllib/evaluation/worker_set.py:614\u001b[0m, in \u001b[0;36mWorkerSet.add_workers\u001b[0;34m(self, num_workers, validate)\u001b[0m\n\u001b[1;32m    613\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m result\u001b[38;5;241m.\u001b[39mok:\n\u001b[0;32m--> 614\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m result\u001b[38;5;241m.\u001b[39mget()\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.21/lib/python3.9/site-packages/ray/rllib/utils/actor_manager.py:477\u001b[0m, in \u001b[0;36mFaultTolerantActorManager.__fetch_result\u001b[0;34m(self, remote_actor_ids, remote_calls, timeout_seconds, return_obj_refs, mark_healthy)\u001b[0m\n\u001b[1;32m    476\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 477\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mray\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    478\u001b[0m     remote_results\u001b[38;5;241m.\u001b[39madd_result(actor_id, ResultOrError(result\u001b[38;5;241m=\u001b[39mresult))\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.21/lib/python3.9/site-packages/ray/_private/client_mode_hook.py:105\u001b[0m, in \u001b[0;36mclient_mode_hook.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    104\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(ray, func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 105\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.21/lib/python3.9/site-packages/ray/_private/worker.py:2382\u001b[0m, in \u001b[0;36mget\u001b[0;34m(object_refs, timeout)\u001b[0m\n\u001b[1;32m   2381\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2382\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m value\n\u001b[1;32m   2384\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_individual_id:\n",
      "\u001b[0;31mRayActorError\u001b[0m: The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=40249, ip=192.168.0.126, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7716cba743d0>)\n  File \"/home/vladimir/.pyenv/versions/3.9.21/lib/python3.9/site-packages/gymnasium/envs/registration.py\", line 569, in make\n    _check_version_exists(ns, name, version)\n  File \"/home/vladimir/.pyenv/versions/3.9.21/lib/python3.9/site-packages/gymnasium/envs/registration.py\", line 219, in _check_version_exists\n    _check_name_exists(ns, name)\n  File \"/home/vladimir/.pyenv/versions/3.9.21/lib/python3.9/site-packages/gymnasium/envs/registration.py\", line 197, in _check_name_exists\n    raise error.NameNotFound(\ngymnasium.error.NameNotFound: Environment mygraphenv doesn't exist. \n\nDuring handling of the above exception, another exception occurred:\n\n\u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=40249, ip=192.168.0.126, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7716cba743d0>)\n  File \"/home/vladimir/.pyenv/versions/3.9.21/lib/python3.9/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 607, in __init__\n    self.env = env_creator(copy.deepcopy(self.env_context))\n  File \"/home/vladimir/.pyenv/versions/3.9.21/lib/python3.9/site-packages/ray/rllib/env/utils.py\", line 178, in _gym_env_creator\n    raise EnvError(ERR_MSG_INVALID_ENV_DESCRIPTOR.format(env_descriptor))\nray.rllib.utils.error.EnvError: The env string you provided ('mygraphenv-v0') is:\na) Not a supported/installed environment.\nb) Not a tune-registered environment creator.\nc) Not a valid env class string.\n\nTry one of the following:\na) For Atari support: `pip install gym[atari] autorom[accept-rom-license]`.\n   For VizDoom support: Install VizDoom\n   (https://github.com/mwydmuch/ViZDoom/blob/master/doc/Building.md) and\n   `pip install vizdoomgym`.\n   For PyBullet support: `pip install pybullet`.\nb) To register your custom env, do `from ray import tune;\n   tune.register('[name]', lambda cfg: [return env obj from here using cfg])`.\n   Then in your config, do `config['env'] = [name]`.\nc) Make sure you provide a fully qualified classpath, e.g.:\n   `ray.rllib.examples.env.repeat_after_me_env.RepeatAfterMeEnv`",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mEnvError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m algo \u001b[38;5;241m=\u001b[39m \u001b[43mAlgorithm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_checkpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbest_checkpoint\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.21/lib/python3.9/site-packages/ray/rllib/algorithms/algorithm.py:281\u001b[0m, in \u001b[0;36mAlgorithm.from_checkpoint\u001b[0;34m(checkpoint, policy_ids, policy_mapping_fn, policies_to_train)\u001b[0m\n\u001b[1;32m    268\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    269\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`checkpoint_info[\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcheckpoint_version\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m]` in `Algorithm.from_checkpoint\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    270\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m()` must be 1.0 or later! You are using a checkpoint with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    271\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mversion v\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcheckpoint_info[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcheckpoint_version\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    272\u001b[0m     )\n\u001b[1;32m    274\u001b[0m state \u001b[38;5;241m=\u001b[39m Algorithm\u001b[38;5;241m.\u001b[39m_checkpoint_info_to_algorithm_state(\n\u001b[1;32m    275\u001b[0m     checkpoint_info\u001b[38;5;241m=\u001b[39mcheckpoint_info,\n\u001b[1;32m    276\u001b[0m     policy_ids\u001b[38;5;241m=\u001b[39mpolicy_ids,\n\u001b[1;32m    277\u001b[0m     policy_mapping_fn\u001b[38;5;241m=\u001b[39mpolicy_mapping_fn,\n\u001b[1;32m    278\u001b[0m     policies_to_train\u001b[38;5;241m=\u001b[39mpolicies_to_train,\n\u001b[1;32m    279\u001b[0m )\n\u001b[0;32m--> 281\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mAlgorithm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_state\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.21/lib/python3.9/site-packages/ray/rllib/algorithms/algorithm.py:309\u001b[0m, in \u001b[0;36mAlgorithm.from_state\u001b[0;34m(state)\u001b[0m\n\u001b[1;32m    307\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m config:\n\u001b[1;32m    308\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo `config` found in given Algorithm state!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 309\u001b[0m new_algo \u001b[38;5;241m=\u001b[39m \u001b[43malgorithm_class\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    310\u001b[0m \u001b[38;5;66;03m# Set the new algo's state.\u001b[39;00m\n\u001b[1;32m    311\u001b[0m new_algo\u001b[38;5;241m.\u001b[39m__setstate__(state)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.21/lib/python3.9/site-packages/ray/util/tracing/tracing_helper.py:466\u001b[0m, in \u001b[0;36m_inject_tracing_into_class.<locals>.span_wrapper.<locals>._resume_span\u001b[0;34m(self, _ray_trace_ctx, *_args, **_kwargs)\u001b[0m\n\u001b[1;32m    464\u001b[0m \u001b[38;5;66;03m# If tracing feature flag is not on, perform a no-op\u001b[39;00m\n\u001b[1;32m    465\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _is_tracing_enabled() \u001b[38;5;129;01mor\u001b[39;00m _ray_trace_ctx \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 466\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    468\u001b[0m tracer: _opentelemetry\u001b[38;5;241m.\u001b[39mtrace\u001b[38;5;241m.\u001b[39mTracer \u001b[38;5;241m=\u001b[39m _opentelemetry\u001b[38;5;241m.\u001b[39mtrace\u001b[38;5;241m.\u001b[39mget_tracer(\n\u001b[1;32m    469\u001b[0m     \u001b[38;5;18m__name__\u001b[39m\n\u001b[1;32m    470\u001b[0m )\n\u001b[1;32m    472\u001b[0m \u001b[38;5;66;03m# Retrieves the context from the _ray_trace_ctx dictionary we\u001b[39;00m\n\u001b[1;32m    473\u001b[0m \u001b[38;5;66;03m# injected.\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.21/lib/python3.9/site-packages/ray/rllib/algorithms/algorithm.py:445\u001b[0m, in \u001b[0;36mAlgorithm.__init__\u001b[0;34m(self, config, env, logger_creator, **kwargs)\u001b[0m\n\u001b[1;32m    433\u001b[0m \u001b[38;5;66;03m# Initialize common evaluation_metrics to nan, before they become\u001b[39;00m\n\u001b[1;32m    434\u001b[0m \u001b[38;5;66;03m# available. We want to make sure the metrics are always present\u001b[39;00m\n\u001b[1;32m    435\u001b[0m \u001b[38;5;66;03m# (although their values may be nan), so that Tune does not complain\u001b[39;00m\n\u001b[1;32m    436\u001b[0m \u001b[38;5;66;03m# when we use these as stopping criteria.\u001b[39;00m\n\u001b[1;32m    437\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluation_metrics \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    438\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mevaluation\u001b[39m\u001b[38;5;124m\"\u001b[39m: {\n\u001b[1;32m    439\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepisode_reward_max\u001b[39m\u001b[38;5;124m\"\u001b[39m: np\u001b[38;5;241m.\u001b[39mnan,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    442\u001b[0m     }\n\u001b[1;32m    443\u001b[0m }\n\u001b[0;32m--> 445\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    446\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    447\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogger_creator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogger_creator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    448\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    449\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    451\u001b[0m \u001b[38;5;66;03m# Check, whether `training_iteration` is still a tune.Trainable property\u001b[39;00m\n\u001b[1;32m    452\u001b[0m \u001b[38;5;66;03m# and has not been overridden by the user in the attempt to implement the\u001b[39;00m\n\u001b[1;32m    453\u001b[0m \u001b[38;5;66;03m# algos logic (this should be done now inside `training_step`).\u001b[39;00m\n\u001b[1;32m    454\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.21/lib/python3.9/site-packages/ray/tune/trainable/trainable.py:169\u001b[0m, in \u001b[0;36mTrainable.__init__\u001b[0;34m(self, config, logger_creator, remote_checkpoint_dir, custom_syncer, sync_timeout)\u001b[0m\n\u001b[1;32m    167\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_local_ip \u001b[38;5;241m=\u001b[39m ray\u001b[38;5;241m.\u001b[39mutil\u001b[38;5;241m.\u001b[39mget_node_ip_address()\n\u001b[0;32m--> 169\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msetup\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdeepcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    170\u001b[0m setup_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m setup_time \u001b[38;5;241m>\u001b[39m SETUP_TIME_THRESHOLD:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.21/lib/python3.9/site-packages/ray/util/tracing/tracing_helper.py:466\u001b[0m, in \u001b[0;36m_inject_tracing_into_class.<locals>.span_wrapper.<locals>._resume_span\u001b[0;34m(self, _ray_trace_ctx, *_args, **_kwargs)\u001b[0m\n\u001b[1;32m    464\u001b[0m \u001b[38;5;66;03m# If tracing feature flag is not on, perform a no-op\u001b[39;00m\n\u001b[1;32m    465\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _is_tracing_enabled() \u001b[38;5;129;01mor\u001b[39;00m _ray_trace_ctx \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 466\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    468\u001b[0m tracer: _opentelemetry\u001b[38;5;241m.\u001b[39mtrace\u001b[38;5;241m.\u001b[39mTracer \u001b[38;5;241m=\u001b[39m _opentelemetry\u001b[38;5;241m.\u001b[39mtrace\u001b[38;5;241m.\u001b[39mget_tracer(\n\u001b[1;32m    469\u001b[0m     \u001b[38;5;18m__name__\u001b[39m\n\u001b[1;32m    470\u001b[0m )\n\u001b[1;32m    472\u001b[0m \u001b[38;5;66;03m# Retrieves the context from the _ray_trace_ctx dictionary we\u001b[39;00m\n\u001b[1;32m    473\u001b[0m \u001b[38;5;66;03m# injected.\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.21/lib/python3.9/site-packages/ray/rllib/algorithms/algorithm.py:571\u001b[0m, in \u001b[0;36mAlgorithm.setup\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m    564\u001b[0m \u001b[38;5;66;03m# Only if user did not override `_init()`:\u001b[39;00m\n\u001b[1;32m    565\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _init \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[1;32m    566\u001b[0m     \u001b[38;5;66;03m# - Create rollout workers here automatically.\u001b[39;00m\n\u001b[1;32m    567\u001b[0m     \u001b[38;5;66;03m# - Run the execution plan to create the local iterator to `next()`\u001b[39;00m\n\u001b[1;32m    568\u001b[0m     \u001b[38;5;66;03m#   in each training iteration.\u001b[39;00m\n\u001b[1;32m    569\u001b[0m     \u001b[38;5;66;03m# This matches the behavior of using `build_trainer()`, which\u001b[39;00m\n\u001b[1;32m    570\u001b[0m     \u001b[38;5;66;03m# has been deprecated.\u001b[39;00m\n\u001b[0;32m--> 571\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mworkers \u001b[38;5;241m=\u001b[39m \u001b[43mWorkerSet\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    572\u001b[0m \u001b[43m        \u001b[49m\u001b[43menv_creator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv_creator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    573\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalidate_env\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidate_env\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    574\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdefault_policy_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_default_policy_class\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    575\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    576\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_workers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_rollout_workers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    577\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_worker\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    578\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogdir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlogdir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    579\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    581\u001b[0m     \u001b[38;5;66;03m# TODO (avnishn): Remove the execution plan API by q1 2023\u001b[39;00m\n\u001b[1;32m    582\u001b[0m     \u001b[38;5;66;03m# Function defining one single training iteration's behavior.\u001b[39;00m\n\u001b[1;32m    583\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39m_disable_execution_plan_api:\n\u001b[1;32m    584\u001b[0m         \u001b[38;5;66;03m# Ensure remote workers are initially in sync with the local worker.\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.21/lib/python3.9/site-packages/ray/rllib/evaluation/worker_set.py:192\u001b[0m, in \u001b[0;36mWorkerSet.__init__\u001b[0;34m(self, env_creator, validate_env, default_policy_class, config, num_workers, local_worker, logdir, _setup, policy_class, trainer_config)\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m RayActorError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;66;03m# In case of an actor (remote worker) init failure, the remote worker\u001b[39;00m\n\u001b[1;32m    181\u001b[0m     \u001b[38;5;66;03m# may still exist and will be accessible, however, e.g. calling\u001b[39;00m\n\u001b[1;32m    182\u001b[0m     \u001b[38;5;66;03m# its `sample.remote()` would result in strange \"property not found\"\u001b[39;00m\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# errors.\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m e\u001b[38;5;241m.\u001b[39mactor_init_failed:\n\u001b[1;32m    185\u001b[0m         \u001b[38;5;66;03m# Raise the original error here that the RolloutWorker raised\u001b[39;00m\n\u001b[1;32m    186\u001b[0m         \u001b[38;5;66;03m# during its construction process. This is to enforce transparency\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[38;5;66;03m# - e.args[0].args[2]: The original Exception (e.g. a ValueError due\u001b[39;00m\n\u001b[1;32m    191\u001b[0m         \u001b[38;5;66;03m# to a config mismatch) thrown inside the actor.\u001b[39;00m\n\u001b[0;32m--> 192\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;241m2\u001b[39m]\n\u001b[1;32m    193\u001b[0m     \u001b[38;5;66;03m# In any other case, raise the RayActorError as-is.\u001b[39;00m\n\u001b[1;32m    194\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    195\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "\u001b[0;31mEnvError\u001b[0m: The env string you provided ('mygraphenv-v0') is:\na) Not a supported/installed environment.\nb) Not a tune-registered environment creator.\nc) Not a valid env class string.\n\nTry one of the following:\na) For Atari support: `pip install gym[atari] autorom[accept-rom-license]`.\n   For VizDoom support: Install VizDoom\n   (https://github.com/mwydmuch/ViZDoom/blob/master/doc/Building.md) and\n   `pip install vizdoomgym`.\n   For PyBullet support: `pip install pybullet`.\nb) To register your custom env, do `from ray import tune;\n   tune.register('[name]', lambda cfg: [return env obj from here using cfg])`.\n   Then in your config, do `config['env'] = [name]`.\nc) Make sure you provide a fully qualified classpath, e.g.:\n   `ray.rllib.examples.env.repeat_after_me_env.RepeatAfterMeEnv`\n"
     ]
    }
   ],
   "source": [
    "algo = Algorithm.from_checkpoint(best_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dd17c22a-08e2-46d0-b34b-731bf9245e93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0, 1, 2, 3, 4}\n",
      "0 2 -0.20869371845180915 0\n",
      "1 1 -0.24627330250392146 3\n",
      "2 0 -0.20562852490057235 2\n",
      "3 0 -0.3953628417186544 1\n",
      "4 0 -0.5311840924120236 4\n",
      "-1.5871424799869809 [0, 3, 2, 1, 4]\n"
     ]
    }
   ],
   "source": [
    "episode_reward = 0\n",
    "terminated = truncated = False\n",
    "obs, info = env.reset(G=G)\n",
    "i = 0\n",
    "path = [] #[obs[0]['node_idx'][0]]\n",
    "nn = {ob['current_node'] for ob in obs}\n",
    "print(nn)\n",
    "while not terminated and not truncated and i < 20: \n",
    "    action = algo.compute_single_action(obs, explore = False)\n",
    "    obs, reward, terminated, truncated, info = env.step(action)\n",
    "    nn_new = {ob['current_node'] for ob in obs}\n",
    "    cc = list(nn - nn_new)[0]\n",
    "    print(i, action, reward, cc)\n",
    "    episode_reward += reward\n",
    "    path.append(cc)\n",
    " #   path.append(obs[0]['node_idx'][0])\n",
    "    i += 1\n",
    "    nn = nn_new\n",
    "print( episode_reward, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b83d782c-3653-4a66-89e3-b21920905ce9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 3, 2, 1, 4, 0]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.state.tour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bdbb10bf-5944-4e51-932e-1977cb5b792a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from python_tsp.exact import solve_tsp_dynamic_programming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "15730e9f-c600-4415-9d7b-4c98e0be896c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0, 3, 2, 1, 4], 1.5871424799869809)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "solve_tsp_dynamic_programming(nx.to_numpy_array(G))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fa3f9e74-c93a-4410-a623-1f6f40f394d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0359244b-4de2-4607-be46-5a3a6fad9fe5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
